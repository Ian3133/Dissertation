{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363a962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b475d",
   "metadata": {},
   "source": [
    "Urd Here frist example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f772b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gradient norms: ‖∇W1‖=0.007561, ‖∇W2‖=0.069418\n",
      "Epoch 1/5 — avg loss=0.1765\n",
      "             ‖W1‖=0.532, ‖W2‖=0.650\n",
      "Epoch 2/5 — avg loss=0.1710\n",
      "             ‖W1‖=0.543, ‖W2‖=0.769\n",
      "Epoch 3/5 — avg loss=0.1656\n",
      "             ‖W1‖=0.563, ‖W2‖=0.943\n",
      "Epoch 4/5 — avg loss=0.1611\n",
      "             ‖W1‖=0.590, ‖W2‖=1.147\n",
      "Epoch 5/5 — avg loss=0.1561\n",
      "             ‖W1‖=0.620, ‖W2‖=1.370\n",
      "\n",
      "Trained W1:\n",
      " [[ 0.20517172 -0.07842559]\n",
      " [ 0.33105983  0.42822562]\n",
      " [-0.20687538 -0.00718386]]\n",
      "\n",
      "Trained W2:\n",
      " [[ 0.73970015  0.10936166]\n",
      " [ 0.50994731 -0.44686367]\n",
      " [-0.54062519 -0.75242338]]\n",
      "\n",
      "Hidden times for x0: [1.616 1.442]\n",
      "Hidden times for x1: [1.682 1.551]\n",
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.9 0.4] -> outputs: [2.689 2.69 ] -> pred: 1, true: 1\n",
      "Input: [0.8 0.7] -> outputs: [2.765 2.656] -> pred: 0, true: 0\n",
      "Input: [0.9 0.4] -> outputs: [2.689 2.69 ] -> pred: 1, true: 1\n",
      "Input: [0.8 0.7] -> outputs: [2.765 2.656] -> pred: 0, true: 0\n",
      "Input: [0.9 0.4] -> outputs: [2.689 2.69 ] -> pred: 1, true: 1\n",
      "Input: [0.8 0.7] -> outputs: [2.765 2.656] -> pred: 0, true: 0\n",
      "Input: [0.9 0.4] -> outputs: [2.689 2.69 ] -> pred: 1, true: 1\n",
      "Input: [0.8 0.7] -> outputs: [2.765 2.656] -> pred: 0, true: 0\n"
     ]
    }
   ],
   "source": [
    "# mini example 2x2 - ask from audience: \n",
    "\n",
    "from brian2 import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Optional: compile device (uncomment if you want runtime compilation)\n",
    "# set_device('runtime')\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative (numpy implementations)\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, time=1, result=1)\n",
    "def spike_timing(w, time):\n",
    "    # time is treated in ms (we pass raw numbers like 0.9, 1.2 ...)\n",
    "    x = (time % 1.0)\n",
    "    z = 10.0 * (x - 0.5)\n",
    "    sigmoid_val = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sigmoid_val\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, time=1, result=1)\n",
    "def d_spike_timing_dw(w, time):\n",
    "    x = (time % 1.0)\n",
    "    z = 5.0 * (x - 0.5)\n",
    "    sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sig * (1.0 - sig) * z\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass for an (n_in)->(n_hidden)->(n_out) layer using Brian2\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) shape (n_in,)\n",
    "    W: weight matrix with bias row: shape (n_in+1, n_out)\n",
    "    layer_idx: integer used inside synapse to tag layer (affects scheduled_time formula)\n",
    "    returns: output spike times array shape (n_out,)\n",
    "    \"\"\"\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    # create a single postsynaptic neuron per output unit (run serially for clarity)\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "            spiked : boolean\n",
    "        ''', threshold='v>1', reset='''\n",
    "            v = 0\n",
    "            spiked = True\n",
    "        ''', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock)\n",
    "            scheduled_time = (sum/sr + layer + 0.004)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = (1.0 - spiked) * int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts) > 0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop (2 -> 2 -> 2)\n",
    "\n",
    "def train_snn_backprop(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=50.0, w_min=-50.0, w_max=50.0,\n",
    "    non_target_time=2.0,\n",
    "    lam=0.5\n",
    "):\n",
    "    # W1 shape: (n_in+1, n_hidden), W2 shape: (n_hidden+1, n_out)\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * lam * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # output layer gradients\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = lam * (o_times[j] - non_target_time)\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "\n",
    "            # backprop to hidden (fixed rule as in your code)\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "                    delta_h[k] += delta_o[j] * W2[k, j] * dt_dw_output\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(W1[i, k], aug_xi[i])\n",
    "\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        if ep % 5 == 0:\n",
    "            print(f\"  Gradient norms: ‖∇W1‖={np.linalg.norm(acc_dW1):.6f}, ‖∇W2‖={np.linalg.norm(acc_dW2):.6f}\")\n",
    "\n",
    "        lr1 = 2.0 * lr\n",
    "        lr2 = lr\n",
    "\n",
    "        g1_norm = np.linalg.norm(acc_dW1)\n",
    "        g2_norm = np.linalg.norm(acc_dW2)\n",
    "        if g1_norm > max_grad:\n",
    "            acc_dW1 = acc_dW1 * (max_grad / g1_norm)\n",
    "        if g2_norm > max_grad:\n",
    "            acc_dW2 = acc_dW2 * (max_grad / g2_norm)\n",
    "\n",
    "        vW1 = beta * vW1 + (1 - beta) * acc_dW1\n",
    "        vW2 = beta * vW2 + (1 - beta) * acc_dW2\n",
    "\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr2 * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Example run\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Two input patterns (2 features)\n",
    "    x0 = np.array([0.9, 0.4])\n",
    "    x1 = np.array([0.8, 0.7])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "\n",
    "    # Two target classes (2 outputs) -> target spike times (ms)\n",
    "    y0 = np.array([2.0, 3.0])  # class 0: output0 early, output1 late\n",
    "    y1 = np.array([3.0, 2.0])  # class 1: output1 early, output0 late\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "\n",
    "    # initialize small weights (including bias rows)\n",
    "    W1_0 = np.random.randn(3, 2) * 0.3  # (2 inputs + bias, 2 hidden)\n",
    "    W2_0 = np.random.randn(3, 2) * 0.3  # (2 hidden + bias, 2 outputs)\n",
    "\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0, epochs=5, lr=20)\n",
    "\n",
    "    # print('\\nTrained W1:\\n', W1_tr)\n",
    "    # print('\\nTrained W2:\\n', W2_tr)\n",
    "\n",
    "    # # quick inspection on the two prototype inputs\n",
    "    # print('\\nHidden times for x0:', layer_forward(x0, W1_tr, 1))\n",
    "    # print('Hidden times for x1:', layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    print('\\n=== Test predictions ===')\n",
    "    for xi, yi in zip(X, Y):\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "        pred_class = np.argmax(o_times)\n",
    "        true_class = np.argmax(yi)\n",
    "        print(f\"Input: {xi} -> outputs: {o_times} -> pred: {pred_class}, true: {true_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b64a12",
   "metadata": {},
   "source": [
    "Next 2nd example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bb18e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (untrained) — train_acc=0.330 test_acc=0.342\n",
      "Epoch   1 loss=1.2434 acc_test=0.579 acc_train=0.579\n",
      "Epoch  20 loss=1.4685 acc_test=0.974 acc_train=0.974\n",
      "Epoch  40 loss=1.3563 acc_test=0.947 acc_train=0.947\n",
      "Epoch  60 loss=1.2737 acc_test=0.921 acc_train=0.921\n",
      "Epoch  80 loss=1.2014 acc_test=0.895 acc_train=0.895\n",
      "Epoch 100 loss=1.1423 acc_test=0.895 acc_train=0.895\n",
      "\n",
      "Final accuracy on full Iris dataset (timing SNN + Adam + margin): 0.927\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[12  1  0]\n",
      " [ 1 11  0]\n",
      " [ 0  2 11]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Timing-based SNN-like MLP (NumPy) + backprop for classification (Iris).\n",
    "\n",
    "This version:\n",
    "- Avoids data leakage (scales only train set).\n",
    "- Reports untrained accuracy before training.\n",
    "- Adds training stabilizers: LR warmup, margin ramp, gradient clipping.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------------- Spike timing function + derivatives -------------------------\n",
    "alpha = 10.0   # shape factor in z = alpha*(x-0.5)\n",
    "\n",
    "def spike_timing(w, t):\n",
    "    x = np.mod(t, 1.0)\n",
    "    z = alpha * (x - 0.5)\n",
    "    sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sig\n",
    "\n",
    "def d_spike_timing_dw(w, t):\n",
    "    x = np.mod(t, 1.0)\n",
    "    z = alpha * (x - 0.5)\n",
    "    sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sig * (1.0 - sig) * z\n",
    "\n",
    "def d_spike_timing_dt(w, t):\n",
    "    x = np.mod(t, 1.0)\n",
    "    z = alpha * (x - 0.5)\n",
    "    sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sig * (1.0 - sig) * w * alpha\n",
    "\n",
    "# ------------------------- Utilities -------------------------\n",
    "\n",
    "def softmax(logits, temp=1.0):\n",
    "    l = (logits - np.max(logits)) / temp\n",
    "    ex = np.exp(l)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "def one_hot(y, n_classes):\n",
    "    v = np.zeros(n_classes)\n",
    "    v[y] = 1.0\n",
    "    return v\n",
    "\n",
    "def features_to_spiketimes_minmax(x_raw, mins, maxs, tmin=0.05, tmax=1.0):\n",
    "    denom = (maxs - mins).copy()\n",
    "    denom[denom == 0] = 1.0\n",
    "    norm = (x_raw - mins) / denom  # in [0,1]\n",
    "    times = tmin + (1.0 - norm) * (tmax - tmin)\n",
    "    return times\n",
    "\n",
    "# ------------------------- Forward pass (timing simulation style) -------------------------\n",
    "\n",
    "def forward_timing(sample_times, W1, W2, layer_offsets=(0.06, 0.14)):\n",
    "    n_in = sample_times.shape[0]\n",
    "    n_hidden = W1.shape[1]\n",
    "    n_out = W2.shape[1]\n",
    "\n",
    "    # input -> hidden\n",
    "    aug_in_times = np.concatenate([sample_times, np.array([0.0])])  # bias spike at 0\n",
    "    sr_hidden = float(aug_in_times.size)\n",
    "    hidden_times = np.zeros(n_hidden)\n",
    "\n",
    "    for k in range(n_hidden):\n",
    "        s = 0.0\n",
    "        for i, t_i in enumerate(aug_in_times):\n",
    "            st = spike_timing(W1[i, k], t_i)\n",
    "            s += st\n",
    "        hidden_times[k] = s / sr_hidden + layer_offsets[0]\n",
    "\n",
    "    # hidden -> output\n",
    "    aug_hidden = np.concatenate([hidden_times, np.array([0.0])])\n",
    "    sr_out = float(aug_hidden.size)\n",
    "    out_times = np.zeros(n_out)\n",
    "\n",
    "    for j in range(n_out):\n",
    "        s = 0.0\n",
    "        for k, t_k in enumerate(aug_hidden):\n",
    "            st = spike_timing(W2[k, j], t_k)\n",
    "            s += st\n",
    "        out_times[j] = s / sr_out + layer_offsets[1]\n",
    "\n",
    "    info = {\n",
    "        'aug_in_times': aug_in_times, 'sr_hidden': sr_hidden,\n",
    "        'hidden_times': hidden_times,\n",
    "        'aug_hidden': aug_hidden, 'sr_out': sr_out,\n",
    "        'out_times': out_times\n",
    "    }\n",
    "    return out_times, info\n",
    "\n",
    "# ------------------------- Backpropagation through timing -------------------------\n",
    "\n",
    "def grads_timing_given_dL_dlogits(info, W1, W2, dL_dlogits):\n",
    "    aug_in = info['aug_in_times']\n",
    "    sr_h = info['sr_hidden']\n",
    "    hidden_times = info['hidden_times']\n",
    "    aug_h = info['aug_hidden']\n",
    "    sr_o = info['sr_out']\n",
    "    out_times = info['out_times']\n",
    "\n",
    "    n_in_plus_bias, n_hidden = W1.shape\n",
    "    n_hidden_plus_bias, n_out = W2.shape\n",
    "\n",
    "    dW2 = np.zeros_like(W2)\n",
    "    dW1 = np.zeros_like(W1)\n",
    "\n",
    "    # W2 grads\n",
    "    for j in range(n_out):\n",
    "        for k in range(n_hidden_plus_bias):\n",
    "            t_k = aug_h[k]\n",
    "            w = W2[k, j]\n",
    "            d_st_dw = d_spike_timing_dw(w, t_k)\n",
    "            dL_dw = dL_dlogits[j] * (1.0 / sr_o) * d_st_dw * (-1.0)  # logit = -t\n",
    "            dW2[k, j] = dL_dw\n",
    "\n",
    "    # W1 grads: chain through hidden -> outputs\n",
    "    for i in range(n_in_plus_bias):\n",
    "        t_i = aug_in[i]\n",
    "        for k in range(n_hidden):\n",
    "            chain = 0.0\n",
    "            t_hidden_k = hidden_times[k]\n",
    "            for j in range(n_out):\n",
    "                w2_kj = W2[k, j]\n",
    "                d_st_dt = d_spike_timing_dt(w2_kj, t_hidden_k)\n",
    "                dtout_dhk = (1.0 / sr_o) * d_st_dt\n",
    "                chain += dL_dlogits[j] * dtout_dhk * (-1.0)\n",
    "            d_sum_dw1 = d_spike_timing_dw(W1[i, k], t_i)\n",
    "            dW1[i, k] = chain * (1.0 / sr_h) * d_sum_dw1\n",
    "\n",
    "    return dW1, dW2\n",
    "\n",
    "# ------------------------- Metrics & Helpers -------------------------\n",
    "\n",
    "def predict_times_to_class(out_times, temp=0.5):\n",
    "    probs = softmax(-out_times, temp=temp)\n",
    "    return int(np.argmax(probs)), probs\n",
    "\n",
    "def confusion_matrix(preds, truths, n_classes=3):\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for p, t in zip(preds, truths):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "# ------------------------- Stabilizers -------------------------\n",
    "\n",
    "def clip_grad_(g, clip=1.0):\n",
    "    np.clip(g, -clip, clip, out=g)\n",
    "\n",
    "def linear_warmup(epoch, warmup_epochs, base_lr):\n",
    "    if warmup_epochs <= 0:\n",
    "        return base_lr\n",
    "    return base_lr * min(1.0, epoch / warmup_epochs)\n",
    "\n",
    "def ramp_lambda(epoch, ramp_epochs, target_lambda):\n",
    "    if ramp_epochs <= 0:\n",
    "        return target_lambda\n",
    "    return target_lambda * min(1.0, epoch / ramp_epochs)\n",
    "\n",
    "# ------------------------- Training with Adam + margin loss -------------------------\n",
    "\n",
    "def train_iris_timing_adam(hidden_size=12, lr=0.02, epochs=100, temp=0.60, print_every=5,\n",
    "                            lam_margin=0.6, beta_margin=4.0, weight_decay=1e-5,\n",
    "                            warmup_epochs=0, lambda_ramp_epochs=8, grad_clip=1.0):\n",
    "    iris = load_iris()\n",
    "    X_raw = iris.data.copy()\n",
    "    y = iris.target.copy()\n",
    "    n_classes = 3\n",
    "\n",
    "    # Split, then scale train/test separately\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        X_raw, y, test_size=0.25, random_state=14, stratify=y)\n",
    "\n",
    "    mm = MinMaxScaler()\n",
    "    X_train_mm = mm.fit_transform(X_train_raw)\n",
    "    X_test_mm  = mm.transform(X_test_raw)\n",
    "\n",
    "    mins = np.zeros(X_raw.shape[1]); maxs = np.ones(X_raw.shape[1])\n",
    "    X_train_times = np.array([features_to_spiketimes_minmax(s, mins, maxs, 0.05, 1.0) for s in X_train_mm])\n",
    "    X_test_times  = np.array([features_to_spiketimes_minmax(s, mins, maxs, 0.05, 1.0) for s in X_test_mm])\n",
    "    X_full_mm     = mm.transform(X_raw)\n",
    "    X_full_times  = np.array([features_to_spiketimes_minmax(s, mins, maxs, 0.05, 1.0) for s in X_full_mm])\n",
    "\n",
    "    n_in = X_train_times.shape[1]\n",
    "    n_hidden = hidden_size\n",
    "    n_out = n_classes\n",
    "\n",
    "    rng = np.random.RandomState(14)\n",
    "    W1 = rng.normal(scale=0.12, size=(n_in + 1, n_hidden))\n",
    "    W2 = rng.normal(scale=0.12, size=(n_hidden + 1, n_out))\n",
    "\n",
    "    # Adam state\n",
    "    mW1 = np.zeros_like(W1); vW1 = np.zeros_like(W1)\n",
    "    mW2 = np.zeros_like(W2); vW2 = np.zeros_like(W2)\n",
    "    beta1, beta2 = 0.9, 0.999\n",
    "    eps = 1e-8\n",
    "    tstep = 0\n",
    "\n",
    "    layer_offsets = (0.06, 0.14)\n",
    "\n",
    "    def predict_set_times(Xset_times, temp_local=temp):\n",
    "        preds = []\n",
    "        for x_t in Xset_times:\n",
    "            out_times, _ = forward_timing(x_t, W1, W2, layer_offsets)\n",
    "            pred, _ = predict_times_to_class(out_times, temp=temp_local)\n",
    "            preds.append(pred)\n",
    "        return np.array(preds)\n",
    "\n",
    "    # --- initial (untrained) accuracy ---\n",
    "    print(f\"Initial (untrained) — train_acc={np.mean(predict_set_times(X_train_times)==y_train):.3f} \"\n",
    "          f\"test_acc={np.mean(predict_set_times(X_test_times)==y_test):.3f}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        lr_now = linear_warmup(epoch, warmup_epochs, lr)\n",
    "        lam_now = ramp_lambda(epoch, lambda_ramp_epochs, lam_margin)\n",
    "\n",
    "        idx = np.arange(len(X_train_times))\n",
    "        rng.shuffle(idx)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for ii in idx:\n",
    "            x_t = X_train_times[ii]\n",
    "            y_true = int(y_train[ii])\n",
    "\n",
    "            out_times, info = forward_timing(x_t, W1, W2, layer_offsets)\n",
    "            logits = -out_times\n",
    "            probs = softmax(logits, temp=temp)\n",
    "\n",
    "            target = one_hot(y_true, n_out)\n",
    "\n",
    "            # CE loss\n",
    "            loss_ce = -np.sum(target * np.log(probs + 1e-12))\n",
    "\n",
    "            # Margin loss (with ramp)\n",
    "            mask = np.ones_like(out_times, dtype=bool); mask[y_true] = False\n",
    "            exp_terms = np.exp(-beta_margin * (out_times[mask] - out_times[y_true]))\n",
    "            loss_margin = lam_now * np.sum(exp_terms)\n",
    "\n",
    "            # dL/dt for margin\n",
    "            dLdt = np.zeros_like(out_times)\n",
    "            dLdt[mask] = -lam_now * beta_margin * np.exp(-beta_margin * (out_times[mask] - out_times[y_true]))\n",
    "            dLdt[y_true] = lam_now * beta_margin * np.sum(np.exp(-beta_margin * (out_times[mask] - out_times[y_true])))\n",
    "\n",
    "            # combine grads (logit = -t)\n",
    "            dL_dlogits = (probs - target) + (-dLdt)\n",
    "\n",
    "            dW1, dW2 = grads_timing_given_dL_dlogits(info, W1, W2, dL_dlogits)\n",
    "\n",
    "            # weight decay\n",
    "            if weight_decay:\n",
    "                dW1 += weight_decay * W1\n",
    "                dW2 += weight_decay * W2\n",
    "\n",
    "            # clip\n",
    "            clip_grad_(dW1, grad_clip)\n",
    "            clip_grad_(dW2, grad_clip)\n",
    "\n",
    "            # Adam update\n",
    "            tstep += 1\n",
    "            mW1 = beta1 * mW1 + (1 - beta1) * dW1\n",
    "            vW1 = beta2 * vW1 + (1 - beta2) * (dW1 * dW1)\n",
    "            mW1_hat = mW1 / (1 - beta1 ** tstep)\n",
    "            vW1_hat = vW1 / (1 - beta2 ** tstep)\n",
    "            W1 -= lr_now * mW1_hat / (np.sqrt(vW1_hat) + eps)\n",
    "\n",
    "            mW2 = beta1 * mW2 + (1 - beta1) * dW2\n",
    "            vW2 = beta2 * vW2 + (1 - beta2) * (dW2 * dW2)\n",
    "            mW2_hat = mW2 / (1 - beta1 ** tstep)\n",
    "            vW2_hat = vW2 / (1 - beta2 ** tstep)\n",
    "            W2 -= lr_now * mW2_hat / (np.sqrt(vW2_hat) + eps)\n",
    "\n",
    "            total_loss += (loss_ce + loss_margin)\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            train_acc = np.mean(predict_set_times(X_train_times)==y_train)\n",
    "            test_acc  = np.mean(predict_set_times(X_test_times)==y_test)\n",
    "            print(f\"Epoch {epoch:3d}\"\n",
    "                  f\" loss={total_loss/len(X_train_times):.4f} acc_test={test_acc:.3f} acc_train={test_acc:.3f}\")\n",
    "\n",
    "    # final evaluation\n",
    "    preds_all = predict_set_times(X_full_times)\n",
    "    full_acc = np.mean(preds_all == y)\n",
    "    print(f\"\\nFinal accuracy on full Iris dataset (timing SNN + Adam + margin): {full_acc:.3f}\")\n",
    "\n",
    "    preds_test = predict_set_times(X_test_times)\n",
    "    cm = confusion_matrix(preds_test, y_test, n_classes=n_out)\n",
    "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "if __name__ == '__main__':\n",
    "    W1, W2 = train_iris_timing_adam(hidden_size=50, lr=0.001, epochs=100, print_every=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8b343",
   "metadata": {},
   "source": [
    "Demo 3 Ymir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64b09525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v5 Ymir gets X-Y-Z so that 3 layers: \n",
    "\n",
    "# v4 : Ymir with varying inputs and outputs x-(x*y)-y network\n",
    "\n",
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging, warnings\n",
    "\n",
    "start_scope()\n",
    "\n",
    "defaultclock.dt = 0.01*ms  \n",
    "\n",
    "#prefs.codegen.target = 'cython'\n",
    "set_device('runtime')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)  # check error later\n",
    "np.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "decay_rate = 5*ms\n",
    "\n",
    "def run_Ymir(inputs, y, z, taus1, taus2):\n",
    "\n",
    "    x = len(inputs)\n",
    "    xy_ind = x*y\n",
    "    yz_ind = y*z\n",
    "\n",
    "\n",
    "    if len(taus1) != xy_ind:\n",
    "        print(\"not the right size for taus1 buddy\")\n",
    "        raise ValueError(f\"Length of taus1 wrong .\")\n",
    "    \n",
    "    if len(taus2) != yz_ind:\n",
    "        print(\"not the right size for taus2 buddy\")\n",
    "        raise ValueError(f\"Length of taus2 is wrong\")\n",
    "\n",
    "    input_neurons = NeuronGroup(x,'''\n",
    "    dv/dt = -v/ decay_rate : volt                 \n",
    "        ''',\n",
    "    threshold='v > 1.0 * volt',\n",
    "    reset='v = 0 * volt',\n",
    "    method='exact')\n",
    "    input_neurons.v = 0 * volt\n",
    "\n",
    "    indices_input = []\n",
    "    for i in range(0, x):\n",
    "        indices_input.append(i)\n",
    "\n",
    "    stim_input = SpikeGeneratorGroup(x, indices=indices_input, times= inputs * ms)\n",
    "\n",
    "    syn_input = Synapses(stim_input, input_neurons[0:x], '''\n",
    "    ''', on_pre='''\n",
    "        v += 1.2 * volt\n",
    "    ''')\n",
    "    syn_input.connect(j='i') \n",
    "\n",
    "    ind_input_neurons = NeuronGroup(xy_ind,\n",
    "    '''dv/dt = -v/ decay_rate : volt \n",
    "         \n",
    "        ''',\n",
    "    threshold='v > 1.0 * volt',\n",
    "    reset='v = 0 * volt',\n",
    "    method='exact')\n",
    "    ind_input_neurons.v = 0 * volt\n",
    "\n",
    "    input_range = []\n",
    "    for i in range(0, x):\n",
    "        for j in range(0, y):\n",
    "            input_range.append(i)\n",
    "\n",
    "\n",
    "    ind_input = Synapses(input_neurons[0:x], ind_input_neurons[0:xy_ind], '''\n",
    "    ''', on_pre='''\n",
    "        v += 0.55 * volt\n",
    "    ''')\n",
    "    ind_input.connect(i=input_range, j=[k for k in range(0, xy_ind)]) # look into what j is later\n",
    "\n",
    "\n",
    "    stim_tau_hidden = SpikeGeneratorGroup(xy_ind, indices=[k for k in range(0, xy_ind)], times = taus1 * ms)\n",
    "\n",
    "    syn_tau_hidden = Synapses(stim_tau_hidden, ind_input_neurons[0:xy_ind], '''\n",
    "    ''', on_pre='''\n",
    "        v += 0.55 * volt\n",
    "    ''')\n",
    "    syn_tau_hidden.connect(j='i')\n",
    "\n",
    "    hidden_neurons = NeuronGroup(y,\n",
    "    '''dv/dt = -v/ decay_rate : volt \n",
    "         \n",
    "        ''',\n",
    "    threshold='v > 1.0 * volt',\n",
    "    reset='v = 0 * volt',\n",
    "    method='exact')\n",
    "    hidden_neurons.v = 0 * volt\n",
    "\n",
    "    output_range = []\n",
    "    for i in range(0, x):\n",
    "        for j in range(0, y):\n",
    "            output_range.append(j)\n",
    "\n",
    "    syn_ind_hidden = Synapses(ind_input_neurons[0:xy_ind], hidden_neurons[0:y], '''\n",
    "    ''', on_pre='''\n",
    "        v += 1.2 * volt\n",
    "    ''')\n",
    "\n",
    "    syn_ind_hidden.connect(i=[k for k in range(0, xy_ind)], j=output_range)\n",
    "\n",
    "    # last layer copy of above below but changed names same logic -- can functionalize later\n",
    "\n",
    "    # stim_input = SpikeGeneratorGroup(x, indices=indices_input, times= inputs * ms)\n",
    "\n",
    "    # syn_input = Synapses(stim_input, input_neurons[0:x], '''\n",
    "    # ''', on_pre='''\n",
    "    #     v += 1.2 * volt\n",
    "    # ''')\n",
    "    # syn_input.connect(j='i') \n",
    "\n",
    "    ind_hidden_neurons = NeuronGroup(yz_ind,\n",
    "    '''dv/dt = -v/ decay_rate : volt \n",
    "         \n",
    "        ''',\n",
    "    threshold='v > 1.0 * volt',\n",
    "    reset='v = 0 * volt',\n",
    "    method='exact')\n",
    "    ind_hidden_neurons.v = 0 * volt\n",
    "\n",
    "    hidden_range = []\n",
    "    for i in range(0, y):\n",
    "        for j in range(0, z):\n",
    "            hidden_range.append(i)\n",
    "\n",
    "\n",
    "    syn_hidden_output = Synapses(hidden_neurons[0:y], ind_hidden_neurons[0:yz_ind], '''\n",
    "    ''', on_pre='''\n",
    "        v += 0.55 * volt\n",
    "    ''')\n",
    "    syn_hidden_output.connect(i=hidden_range, j=[k for k in range(0, yz_ind)])\n",
    "\n",
    "\n",
    "    stim_tau_output = SpikeGeneratorGroup(yz_ind, indices=[k for k in range(0, yz_ind)], times = taus2 * ms)\n",
    "\n",
    "    syn_tau_output = Synapses(stim_tau_output, ind_hidden_neurons[0:yz_ind], '''\n",
    "    ''', on_pre='''\n",
    "        v += 0.55 * volt\n",
    "    ''')\n",
    "    syn_tau_output.connect(j='i')\n",
    "\n",
    "    output_neurons = NeuronGroup(z,\n",
    "    '''dv/dt = -v/ decay_rate : volt \n",
    "         \n",
    "        ''',\n",
    "    threshold='v > 1.0 * volt',\n",
    "    reset='v = 0 * volt',\n",
    "    method='exact')\n",
    "    output_neurons.v = 0 * volt\n",
    "\n",
    "    output_out_range = []\n",
    "    for i in range(0, y):\n",
    "        for j in range(0, z):\n",
    "            output_out_range.append(j)\n",
    "\n",
    "    syn_output = Synapses(ind_hidden_neurons[0:yz_ind], output_neurons[0:z], '''\n",
    "    ''', on_pre='''\n",
    "        v += 1.2 * volt\n",
    "    ''')\n",
    "\n",
    "    syn_output.connect(i=[k for k in range(0, yz_ind)], j=output_out_range)\n",
    "\n",
    "    \n",
    "\n",
    "    mon = StateMonitor(input_neurons, 'v', record=True, dt=0.01*ms)\n",
    "    M1 = StateMonitor(ind_input_neurons, 'v', record=True, dt=0.01*ms)\n",
    "\n",
    "    M2 = StateMonitor(hidden_neurons, 'v', record=True, dt=0.01*ms)\n",
    "\n",
    "    M3 = StateMonitor(ind_hidden_neurons, 'v', record=True, dt=0.01*ms)\n",
    "    M4 = StateMonitor(output_neurons, 'v', record=True, dt=0.01*ms)\n",
    "\n",
    "    spikemon = SpikeMonitor(input_neurons)\n",
    "    spikemon_1 = SpikeMonitor(ind_input_neurons)\n",
    "    spikemon_2 = SpikeMonitor(hidden_neurons)\n",
    "    spikemon_3 = SpikeMonitor(ind_hidden_neurons)\n",
    "    spikemon_4 = SpikeMonitor(output_neurons)\n",
    "\n",
    "    run(10*ms)\n",
    "\n",
    "    # # Plot v\n",
    "    # figure(figsize=(10, 6))\n",
    "    # for i in range(0, yz_ind): \n",
    "    #     #plot(mon.t/ms, mon.v[i], label=f'Neuron {i}')\n",
    "    #     plot(M2.t/ms, M2.v[i], label=f'Neuron {i}')\n",
    "    #     plot(M4.t/ms, M4.v[i], label=f'output neruon')\n",
    "    # # for i in range(0, z): \n",
    "    # #     plot(M4.t/ms, M2.v[i], label=f'Neuron output')\n",
    "    # xlabel('Time (ms)')\n",
    "    # ylabel('Membrane potential')\n",
    "    # legend()\n",
    "    # title('SNN Spike Propagation Across All Layers')\n",
    "    # show()\n",
    "\n",
    "    results = []\n",
    "    results.append(spikemon.spike_trains())\n",
    "    results.append(spikemon_1.spike_trains())\n",
    "    results.append(spikemon_2.spike_trains())\n",
    "    results.append(spikemon_3.spike_trains())\n",
    "    results.append(spikemon_4.spike_trains())\n",
    "    \n",
    "\n",
    "\n",
    "    #print(results)\n",
    "    return results   # check the dictionalry apect of this as the odder might be changing so harder wirinign i-j such relationship\n",
    "\n",
    "\n",
    "\n",
    "inputs = [1.0]\n",
    "y = 2\n",
    "z = 2\n",
    "\n",
    "h_neurons_sumed_movements = [0] * y \n",
    "\n",
    "taus1 = [1.1, 1.1] \n",
    "taus2 = [1.2, 1.3, 1.35, 1.4] \n",
    "\n",
    "\n",
    "# funcition for training taus\n",
    "def see_system_run():\n",
    "    raw_results = run_Ymir(inputs, y, z, taus1, taus2)\n",
    "\n",
    "    recorded_spikes = [\n",
    "        {neuron_idx: (spike_times / ms).tolist()  # convert to float ms\n",
    "        for neuron_idx, spike_times in group.items()}\n",
    "        for group in raw_results\n",
    "    ]\n",
    "\n",
    "    return(recorded_spikes)\n",
    "\n",
    "    # for i in range(len(recorded_spikes)):\n",
    "    #     print(f\"Group {i} spikes:\")\n",
    "    #     for neuron_idx, spike_times in recorded_spikes[i].items():\n",
    "    #         print(f\"  Neuron {neuron_idx}: {spike_times}\")\n",
    "    #     print()\n",
    "\n",
    "\n",
    "\n",
    "def tau_shifer(spike, tau, i, strength, closer=True):\n",
    "    global h_neurons_sumed_movements\n",
    "    # i is equal to what syn we are on 0 and 2 are n0 and 1 & 3 --> n1\n",
    "    delta = spike - tau\n",
    "    j = int(i > 1)\n",
    "\n",
    "    if closer:\n",
    "        # Original behavior: bigger delta → bigger step toward spike\n",
    "        step = delta * strength\n",
    "        if tau + step < 0:\n",
    "            h_neurons_sumed_movements[j] += 0.02\n",
    "            return 0.02\n",
    "        h_neurons_sumed_movements[j] += step\n",
    "        return tau + step\n",
    "    else:\n",
    "        # Inverse scaling: bigger delta → smaller step\n",
    "        if delta == 0:\n",
    "            h_neurons_sumed_movements[j] += 0.02\n",
    "            return tau + 0.02  # struck will always fire if - is in range? - make sure 0 is always out of range for possilbe inputs? \n",
    "        step = (strength / abs(delta)) * delta  # sign from delta\n",
    "        if tau - step < 0:\n",
    "            h_neurons_sumed_movements[j] += 0.02\n",
    "            return 0.02\n",
    "        h_neurons_sumed_movements[j] -= step\n",
    "        return tau - step\n",
    "        \n",
    "def directions(output_spikes, desired):\n",
    "     # just firinng for a single spike or not change change later\n",
    "    mods = [-1] * len(desired)  # assuming we want to modify all neurons\n",
    "    \n",
    "    for i in range(len(desired)): \n",
    "        if desired[i] == True:    # if we wanted a spike\n",
    "            if output_spikes[i] == []: # if no spike saw\n",
    "                mods[i] = 2 \n",
    "            else:                   # if spike saw\n",
    "                mods[i] = 0\n",
    "        else:\n",
    "            if output_spikes[i] == []: # if we didnt want a spike and it didnt spike\n",
    "                mods[i] = 3\n",
    "            else:   # if we didnt want it to spike but it did\n",
    "                mods[i] = 1\n",
    "    return mods     \n",
    "\n",
    "\n",
    "def train_taus_last_layer(inputs, taus1, taus2, strength, desired):\n",
    "\n",
    "    # run it and gets data with current values\n",
    "    raw_results = run_Ymir(inputs, y, z, taus1, taus2)\n",
    "\n",
    "    recorded_spikes = [\n",
    "        {neuron_idx: (spike_times / ms).tolist()  # convert to float ms\n",
    "        for neuron_idx, spike_times in group.items()}\n",
    "        for group in raw_results\n",
    "    ]\n",
    "     # here we have the data of all the spikes and taus and desired we send into to get new taus\n",
    "    \n",
    "    d = directions(recorded_spikes[2], desired)  # get the directions for the taus\n",
    "    \n",
    "    #print(f\"directions: {d}\")\n",
    "\n",
    "    # have function for sorting and making bools for such\n",
    "\n",
    "\n",
    "    # basied on direction get proper assocaltion for each direction and strength for updating and send each though tau shifter\n",
    "    move = False\n",
    "    count = 0\n",
    "    index = 0\n",
    "    for i in range((len(taus2))):\n",
    "        # if spike as well\n",
    "            #if i % 2 == 0: # even index taus        # will need to change to extend to multi later\n",
    "        if d[index] % 2 == 0:  # 0 and 2 here\n",
    "            move = True\n",
    "        else:\n",
    "            move = False\n",
    "        if recorded_spikes[2][count] != []:  # if we saw a spike\n",
    "            # print(\"count:\", count)\n",
    "            # print(i)\n",
    "            # print(\"taus \", taus2[i])\n",
    "            # if recorded_spikes[3][count][0] != []:\n",
    "            #     print(f\"{i}\")\n",
    "            #     print(\"spike at: \", recorded_spikes[3][count][0])\n",
    "            #     print(\"taus2: \", taus2[i])\n",
    "            #     print(\"strength: \", strength)\n",
    "            #     print(\"move: \", move)\n",
    "            new_tau = tau_shifer(recorded_spikes[2][count][0], taus2[i], i, strength, closer=move)\n",
    "            #print(\"new tay and old tau: \", new_tau, taus2[i])\n",
    "            taus2[i] = new_tau\n",
    "    \n",
    "        else: \n",
    "            #new_tau = tau_shifer(0, taus2[i], strength, closer=move)  # if no spike saw then just use 0\n",
    "            print(\"condition of change on condition of hidden not having spike \")# DID YOU EVER GOT HEREEEE\")\n",
    "            #print(\"no spike saw so no update?\" )#new tay and old tau: \", new_tau, taus2[i])\n",
    "            #taus2[i] = new_tau\n",
    "\n",
    "        if (i+1) % 2 == 0:\n",
    "            count += 1\n",
    "        if index == 0:\n",
    "            index += 1\n",
    "        elif index == 1:\n",
    "            index = 0\n",
    "    \n",
    "\n",
    "    return #recorded_spikes[3]  # return the last layer spikes for now\n",
    "\n",
    "    # example 2 varying inputs to train specifics 1.0 goes to T/F and 1.5 goes to F/T \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ab1e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs 1 output_spikes {0: [1.1400000000000001], 1: []}  desired:  [True, False]\n",
      "inputs 2 output_spikes {0: [], 1: [2.31]}  desired:  [False, True]\n"
     ]
    }
   ],
   "source": [
    "y = 2\n",
    "z = 2\n",
    "taus1 = [1.1, 1.1] \n",
    "taus2 = [1.2, 1.3, 1.35, 1.4] \n",
    "\n",
    "see_system_run()\n",
    "\n",
    "desired_1 = [True, False]\n",
    "desired_2 = [False, True]\n",
    "for i in range(20):\n",
    "    train_taus_last_layer([1.0], taus1, taus2, .1, desired_1)\n",
    "    train_taus_last_layer([1.5], taus1, taus2, .1, desired_2)\n",
    "\n",
    "inputs = [1.0]\n",
    "print(\"inputs 1 output_spikes\" , see_system_run()[4], \" desired: \", desired_1)\n",
    "inputs = [1.5]\n",
    "print(\"inputs 2 output_spikes\" , see_system_run()[4],  \" desired: \", desired_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e826a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with larger model here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
