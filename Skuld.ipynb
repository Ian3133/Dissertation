{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "99d3ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math Section for Skuld model v2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "a = 1 # the main part of what i will be changing for weights changes the size of the sigmoid without changing near zero\n",
    "b = 0  # bias will add to updating with ^ later\n",
    "m = 0.5 # the rate of change of the sigmoid \n",
    "c = 1 # a constant I used, same as but but shift the sigmod to a place where i 0 is close to 0 (actuyll 0.04 but whatever)\n",
    "layer_bias = -0.5\n",
    "\n",
    "\n",
    "def ians_sigmoid(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "    # width = max(width, 0.01)  # prevent division by 0 or instability\n",
    "    # layer_shift = layer * 0.1  # optional small bias per layer\n",
    "    # return scale * np.exp(-((x - center - layer_shift) ** 2) / (2 * width ** 2))\n",
    "    return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "    #return 1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias)))\n",
    "#print(ians_sigmoid(0.8, a, b, m, c))\n",
    "\n",
    "# def ians_sigmoid_new(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "#     return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "\n",
    "def calc_loss(output, desired):\n",
    "    # can change but not a requirement yet\n",
    "    return (0.5 * ((output - desired) ** 2))\n",
    "\n",
    "\n",
    "def d_loss_d_weight_2(n1, n2, desired):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1) # removedall layer_bias can check layer if correct call\n",
    "\n",
    "def d_loss_d_weight_1(n1, n2, desired, w2, input):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1 * w2 * (1- n1) * input)\n",
    "\n",
    "\n",
    "def d_loss_d_bias_2(n2, desired):\n",
    "    return (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "\n",
    "def d_loss_d_bias_1(n1, n2, delta2, desired, a2):\n",
    "    #delta2 = (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "    return delta2 * a2 * (n1 - layer_bias) * (1 - (n1 - layer_bias))\n",
    "\n",
    "\n",
    "def update_param(lr, grad, w):\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(input, a1, a2, b1, b2):\n",
    "    sig_r1 = ians_sigmoid(input, a1, b1, m, c)\n",
    "    delay_1 = sig_r1 + input\n",
    "    sig_r2 = ians_sigmoid(delay_1, a2, b2, m, c, layer=1)\n",
    "    delay_2 = sig_r2 + delay_1\n",
    "\n",
    "    return ([delay_1, delay_2], [sig_r1, sig_r2])\n",
    "\n",
    "\n",
    "def run_basic(input, a1, a2, b1, b2, desired, lr):   # will need to check work on bias/b and make sure everything is correct in this\n",
    "\n",
    "    result = forward_pass(input, a1, a2, b1, b2)\n",
    "    output = result[0][1]\n",
    "    n1 = result[0][0]\n",
    "    n2 = result[0][1]\n",
    "    print(n1, n2)\n",
    "    #loss = calc_loss(output, desired)\n",
    "\n",
    "    delta_a2 = - d_loss_d_weight_2(n1, n2, desired)\n",
    "\n",
    "    delta_a1 = - d_loss_d_weight_1(n1, n2, desired, a2, input) # trying to just input a as a the weight directly\n",
    "\n",
    "    new_a1 = update_param(lr, delta_a1, a1)\n",
    "    new_a2 = update_param(lr, delta_a2, a2)\n",
    "\n",
    "\n",
    "    delta_b2 = d_loss_d_bias_2(n2, desired)\n",
    "    delta_b1 = d_loss_d_bias_1(n1, n2, delta_b2, desired, b2) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "    new_b1 = update_param(lr, delta_b1, b1)\n",
    "    new_b2 = update_param(lr, delta_b2, b2)\n",
    "\n",
    "    return new_a1, new_a2, new_b1, new_b2, result\n",
    "\n",
    "\n",
    "# run_basic(0.8, 1, 1, 0, 0, 1.5, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34adcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron: \n",
    "    def __init__(self, layer, number):\n",
    "\n",
    "        self.layer = layer\n",
    "        self.number = number\n",
    "    \n",
    "\n",
    "        self.current_layer = 0\n",
    "        self.next_layer = 0\n",
    "        self.past_layer = 1\n",
    "        self.input_syn = 0\n",
    "        self.output_syn = 0\n",
    "        \n",
    "        \n",
    "        self.spikes = 0\n",
    "        #self.firetime = -1\n",
    "        self.fired = False\n",
    "\n",
    "        self.set_outputs(layer)\n",
    "\n",
    "        if self.layer == 0:\n",
    "            self.threshold = 1.0\n",
    "        else: \n",
    "            self.threshold = 1.0  #2/3 * (self.past_layer)   #+ (self.current_layer + 1) # previous layer # of n /2 seem like a good fit  # will change to fun that reflect # of inputs and layer -- number of inputs\n",
    "        self.voltage = 0\n",
    "\n",
    "        #self.collected = 0\n",
    "        self.scheduled_spike = -10 # change to just runtime or somthng else\n",
    "        self.spike_time = 1000\n",
    "        \n",
    "\n",
    "    def set_outputs(self, layer):\n",
    "        '''Called with the creation of neuron create all the correct assocations for later here'''\n",
    "        global input_layer, output_layer, syn_layer_1, syn_layer_2, hidden_layer, syn_outputs, past_layer\n",
    "        if layer == 0:\n",
    "            self.next_layer = hidden_layer\n",
    "            #self.input_syn = 0 # remains nothing\n",
    "            self.output_syn = syn_layer_1\n",
    "        elif layer == 1:\n",
    "            self.current_layer = hidden_layer\n",
    "            self.next_layer = output_layer\n",
    "            self.past_layer = input_layer\n",
    "            self.input_syn = syn_layer_1\n",
    "            self.output_syn = syn_layer_2\n",
    "        elif layer == 2:\n",
    "            self.current_layer = output_layer\n",
    "            self.next_layer = output_layer # same amount of outputs\n",
    "            self.past_layer = hidden_layer\n",
    "            self.input_syn = syn_layer_2\n",
    "            self.output_syn = syn_outputs\n",
    "  \n",
    "\n",
    "    def send_to_syn(self, time, delay):\n",
    "        '''Sends out the times to the synapses of when they should arive at their next neuron'''\n",
    "        global l1_a, l2_a, l1_b, l2_b, m, c\n",
    "        self.fired = True\n",
    "        #print(self.layer, self.number, time, delay)\n",
    "        start = int(self.number * self.next_layer)\n",
    "        for syn in self.output_syn[start:(start+(self.next_layer))]:\n",
    "            #print('input: ', syn.input_neuron, \"output \", syn.output_neuron, \"on layer \", syn.layer)\n",
    "            # if self.layer == 1:\n",
    "            #     a = l1_a[syn.input_neuron][syn.output_neuron]\n",
    "            #     b = l1_b[syn.input_neuron][syn.output_neuron]\n",
    "            # if self.layer == 2:\n",
    "            #     a = l2_a[syn.input_neuron][syn.output_neuron]\n",
    "            #     b = l2_b[syn.input_neuron][syn.output_neuron]\n",
    "            syn.spike_time = ians_sigmoid(time, syn.a, syn.b, m, c, layer=syn.layer) + delay\n",
    "            #print(ians_sigmoid(time, 1, 0, m, c, layer=syn.layer) + delay)\n",
    "            #print(ians_sigmoid(time, 2, 0, m, c, layer=syn.layer) + delay)\n",
    "            #print(syn.spike_time, syn.layer)\n",
    "            #print(\"syn a and b\", a, b)\n",
    "            #print(\"syna and b\", syn.a, syn.b)\n",
    "            #print(\"syn spike time & delay \", syn.spike_time, delay)\n",
    "            #print(\"spike time send to syn \",syn.input_neuron,syn.output_neuron, \"at time \", syn.spike_time)\n",
    "            pass\n",
    "        # for all sync going out from this one: \n",
    "            # set call ian_sig and set that synapaces time to reach at calculated time\n",
    "\n",
    "\n",
    "    def check_arival(self):\n",
    "        '''Check first if this neuron has alreay sent or not. If it hasn't then it goes to check the synapses to see if one has arrived'''\n",
    "        if self.fired:\n",
    "            return\n",
    "        else:\n",
    "            self.check_syn_arival()\n",
    "\n",
    "\n",
    "    def check_syn_arival(self):\n",
    "        '''used in check_arival it will see if neruon is ready to act now basied on arriving spikes'''\n",
    "        global current_time\n",
    "        #print(self.layer)\n",
    "        if self.layer == 0: # only for input layer\n",
    "            if self.scheduled_spike <= current_time: # if fired already checked\n",
    "                self.voltage += 1\n",
    "                #print(\"input voltage set at time \", current_time)\n",
    "        else:\n",
    "\n",
    "            start = self.number * (self.current_layer -1)\n",
    "            for syn in self.input_syn[start: : self.current_layer]: # goes over all incoming synapaces\n",
    "                if (syn.message_complete) == False: # if syn is has not yet given this neuron its message\n",
    "                    #print(self.layer, syn.layer)\n",
    "                    #print(syn.check_spike(), current_time, syn.layer)\n",
    "                    if syn.check_spike() <= current_time: # will check if current step has been reached\n",
    "                        #print(syn.check_spike(), current_time)\n",
    "                        #print(self.layer, syn.layer)\n",
    "\n",
    "                        #print(syn.check_spike(), current_time, self.layer)\n",
    "                        # print(\"layer: \", self.layer)\n",
    "                        # print(\"recived spike at: \",current_time)\n",
    "                        syn.message_complete = True\n",
    "                        #update neruons voltage\n",
    "                        self.voltage += 1.2\n",
    "\n",
    "        self.check_threshold_voltage()\n",
    "        \n",
    "                    \n",
    "                    \n",
    "    def check_threshold_voltage(self):\n",
    "        global time_step\n",
    "        if self.voltage >= self.threshold:\n",
    "            #print(self.layer, self.voltage, self.threshold)\n",
    "            #print(\"self.voltage: \", self.voltage)\n",
    "            #print(\"threshold: \", self.threshold)\n",
    "            self.spike_time = current_time\n",
    "            #print(self.layer, self.spike_time)\n",
    "            #print(\"spike time: \", current_time)\n",
    "            self.send_to_syn(current_time, current_time)  # CHANGE IF LAYER NOT == 0?\n",
    "        # else:\n",
    "        #     if current_time >= (self.layer):\n",
    "        #         self.voltage += (time_step*self.past_layer) * 0.5 # will change such and whatnot just playing to start# \n",
    "\n",
    "\n",
    "class synapse:\n",
    "    def __init__(self, input_neuron, output_neuron, layer, a, b):\n",
    "        global run_time\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.input_neuron = input_neuron\n",
    "        self.output_neuron = output_neuron\n",
    "        self.layer = layer\n",
    "        self.spike_time = run_time\n",
    "        self.message_complete = False\n",
    "        self.grad_a = 0\n",
    "        self.grad_b = 0\n",
    "\n",
    "    def set_spiketime(self, time):\n",
    "        self.spike_time = time\n",
    "\n",
    "\n",
    "    def check_spike(self):\n",
    "        return self.spike_time\n",
    "\n",
    "\n",
    "\n",
    "def genesis():\n",
    "    create_new_neurons_network()\n",
    "    create_synapses()\n",
    "    return\n",
    "\n",
    "\n",
    "def create_new_neurons_network():\n",
    "    count = 0\n",
    "    for i in range(len(inputs)):\n",
    "        n = Neuron(layer=0, number=i)  # will add later to add a and b and such\n",
    "        #n.set_firetime(val)\n",
    "        neurons_input.append(n)\n",
    "        n.scheduled_spike = inputs[i] \n",
    "        #print('Neuron Input neuron ' + str(neurons_input[i].number), \"scheduled time: \", n.scheduled_spike)\n",
    "\n",
    "    count = 0\n",
    "    for i in range (hidden_layer):\n",
    "        n = Neuron(layer=1, number=i)\n",
    "        neurons_hidden.append(n)\n",
    "        #print(\"Neuron at layer 1: \" + str(neurons_hidden[i].number))\n",
    "\n",
    "    count = 0\n",
    "    for i in range (output_layer):\n",
    "        n = Neuron(layer=2, number=i)\n",
    "        neurons_output.append(n)\n",
    "        #print(\"Neuron at output layer: \" + str(neurons_output[i].number))\n",
    "\n",
    "\n",
    "def create_synapses():\n",
    "    for input_n in neurons_input:\n",
    "        for hidden_n in neurons_hidden:\n",
    "            #print(input_n.number)\n",
    "            #print(hidden_n.number)\n",
    "            # create new object of synapics \n",
    "            #give the object to both neurons so one can output to and the other can edit it\n",
    "            syn = synapse(input_neuron=input_n.number, output_neuron=hidden_n.number, layer=0, a= 1 + random.uniform(-0.1, 0.1), b= random.uniform(-0.1, 0.1)) \n",
    "            syn_layer_1.append(syn)\n",
    "\n",
    "    for hidden_n in neurons_hidden:\n",
    "        for output_n in neurons_output:\n",
    "            #print(\"h\", hidden_n.number)\n",
    "            #print(\"0\" , output_n.number)\n",
    "            # create new object of synapics \n",
    "            #give the object to both neurons so one can output to and the other can edit it\n",
    "            syn = synapse(input_neuron=hidden_n.number, output_neuron=output_n.number, layer=1, a= 1 + random.uniform(-0.1, 0.1), b= random.uniform(-0.1, 0.1)) \n",
    "            syn_layer_2.append(syn)\n",
    "\n",
    "    \n",
    "    for count in range(output_layer): # will check layer \n",
    "            syn = synapse(input_neuron=count, output_neuron=count, layer=2, a=a, b=b) \n",
    "            syn_outputs.append(syn)\n",
    "\n",
    "\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f373296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7929999999999133 1.7219999999999211 1.6349999999999307\n",
      "1.7929999999999133 1.7219999999999211 1.6349999999999307\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "input_layer = 2\n",
    "hidden_layer = 10\n",
    "output_layer = 3\n",
    "m = 5 \n",
    "c = 3   # play with values and such\n",
    "\n",
    "run_time = 10\n",
    "\n",
    "neurons_input = []\n",
    "neurons_hidden = []\n",
    "neurons_output = []\n",
    "\n",
    "syn_layer_1 = []\n",
    "syn_layer_2 = []\n",
    "syn_outputs = []\n",
    "\n",
    "current_time = 0\n",
    "time_step = 1\n",
    "\n",
    "def run_simulation(runtime, dx):\n",
    "    global current_time, time_step\n",
    "    time_step = 1.0/dx\n",
    "    \n",
    "    current_time = 0\n",
    "\n",
    "    for i in range(runtime*dx):\n",
    "       \n",
    "        for n1 in neurons_input:\n",
    "            n1.check_arival()\n",
    "            pass \n",
    "\n",
    "        for n2 in neurons_hidden:\n",
    "            n2.check_arival()\n",
    "            pass \n",
    "\n",
    "        for n3 in neurons_output:\n",
    "            n3.check_arival()\n",
    "            pass\n",
    "\n",
    "        #print(\"TIME\", current_time)\n",
    "        current_time += time_step\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "def reset_network():\n",
    "    global current_time\n",
    "    current_time = 0\n",
    "    for n in neurons_input: \n",
    "        n.voltage = 0\n",
    "        n.fired = False\n",
    "        n.spike_time = run_time\n",
    "    for n in neurons_hidden:\n",
    "        n.voltage = 0\n",
    "        n.fired = False\n",
    "        n.spike_time = run_time\n",
    "    for n in neurons_output:\n",
    "        n.voltage = 0\n",
    "        n.fired = False\n",
    "        n.spike_time = run_time\n",
    "    for s in syn_layer_1:\n",
    "        s.spike_time = run_time\n",
    "        s.message_complete = False\n",
    "    for s in syn_layer_2:\n",
    "        s.spike_time = run_time\n",
    "        s.message_complete = False\n",
    "\n",
    "\n",
    "inputs = [0.5, 0.8] # need to match input #\n",
    "\n",
    "#random.seed(0)\n",
    "\n",
    "genesis()\n",
    "\n",
    "run_simulation(run_time, 1000)\n",
    "# run_simulation(run_time, 1000)\n",
    "\n",
    "# print(neurons_output[0].spike_time, neurons_output[1].spike_time, neurons_output[2].spike_time)\n",
    "\n",
    "# run_simulation(run_time, 1000)\n",
    "\n",
    "print(neurons_output[0].spike_time, neurons_output[1].spike_time, neurons_output[2].spike_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4ef1b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01817688888890783, 1.909333333333234)\n",
      "(0.01479200000001748, 1.9279999999998985)\n",
      "(0.011858000000015922, 1.9459999999998967)\n",
      "(0.009476055555570066, 1.962333333333228)\n",
      "(0.007605555555568723, 1.97666666666656)\n",
      "(0.006013388888900787, 1.990333333333225)\n",
      "(0.004834722222232991, 2.0016666666665572)\n",
      "(0.0038427222222319262, 2.0123333333332227)\n",
      "(0.0030160555555642384, 2.0223333333332216)\n",
      "(0.0023805000000077796, 2.0309999999998873)\n"
     ]
    }
   ],
   "source": [
    "def Skuld_learn(inputs, x,y,z, desired, lr):\n",
    "    \"\"\"description here\n",
    "    \"\"\"\n",
    "    global m, c, layer_bias\n",
    "    \n",
    "    # genesis()\n",
    "\n",
    "    run_simulation(run_time, 1000)\n",
    "\n",
    "\n",
    "    avg_input  = sum(n.spike_time for n in neurons_input ) / input_layer\n",
    "    \n",
    "    avg_output = 0\n",
    "    for i in range(output_layer): # goes overall outputs to get the average output\n",
    "        avg_output += neurons_output[i].spike_time\n",
    "    avg_output = avg_output/output_layer\n",
    "\n",
    "    avg_hidden = 0\n",
    "    for i in range(hidden_layer): # goes overall outputs to get the average output\n",
    "        avg_hidden += neurons_hidden[i].spike_time\n",
    "    avg_hidden= avg_hidden/hidden_layer\n",
    "\n",
    "\n",
    "    # calcs loss\n",
    "    loss = calc_loss(avg_output, desired)\n",
    "\n",
    "    # === Backward pass ===\n",
    "    N = input_layer + hidden_layer + output_layer\n",
    "    grads_a = [0.0] * N\n",
    "    grads_b = [0.0] * N\n",
    "\n",
    "    # dL/dO and delta for each output neuron\n",
    "    dL_dO = avg_output - desired\n",
    "\n",
    "    for syn in syn_layer_2:\n",
    "\n",
    "        # chain through average\n",
    "        dL_dδ = dL_dO * (1 / output_layer)\n",
    "\n",
    "        # derivative of f wrt a[idx], b[idx]\n",
    "        E = -m * avg_hidden + syn.b + c + 2 * layer_bias # need to look into having the avg_hidden instead of pure neuron\n",
    "        expE = np.exp(E)\n",
    "        u = (1 / syn.a) + expE\n",
    "\n",
    "        # ∂f/∂a and ∂f/∂b for f = a/u\n",
    "        df_da = 1/u + 1/(u * u * syn.a)\n",
    "        df_db = - syn.a * expE / (u * u)\n",
    "\n",
    "        # print(syn.grad_a)\n",
    "        syn.grad_a = dL_dδ * df_da\n",
    "        syn.grad_b = dL_dδ * df_db\n",
    "\n",
    "        syn.a -= lr * syn.grad_a\n",
    "        syn.b -= lr * syn.grad_b\n",
    "        #print(syn.a)\n",
    "        # print(syn.b)\n",
    "\n",
    "    # backprop into avg_hidden\n",
    "    dL_dh = dL_dO  # because each output δ adds directly to avg_hidden with weight 1/z, summed z times → cancels\n",
    "\n",
    "    # hidden layer\n",
    "    for syn in syn_layer_1:\n",
    "\n",
    "        # each hidden δ contributes equally to avg_hidden\n",
    "        dL_dδ = dL_dh * (1 / y)\n",
    "\n",
    "        E = -m * avg_input + syn.b + c + 1 * layer_bias\n",
    "        expE = np.exp(E)\n",
    "        u = (1 / syn.a) + expE\n",
    "\n",
    "        df_da = 1/u + 1/(u * u * syn.a)\n",
    "        df_db = -syn.a * expE / (u * u)\n",
    "\n",
    "        syn.grad_a = dL_dδ * df_da\n",
    "        syn.grad_b = dL_dδ * df_db\n",
    "\n",
    "        syn.a -= lr * syn.grad_a\n",
    "        syn.b -= lr * syn.grad_b\n",
    "\n",
    "\n",
    "    return loss, avg_output\n",
    "\n",
    "for i in range(10):\n",
    "    reset_network()\n",
    "    print(Skuld_learn(inputs, input_layer, hidden_layer, output_layer, 2.1, 0.1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 and a2:  0.8927805697950354 0.5248898561163828\n",
      "b1 and b2:  -0.12049517582171908 1.1520251367974752\n",
      "2.41\n",
      "a1 and a2:  0.8926605766728039 0.5219351860366003\n",
      "b1 and b2:  -0.14663693623469287 1.200745787899999\n",
      "1.059\n",
      "a1 and a2:  0.8925398525675887 0.5188501482669003\n",
      "b1 and b2:  -0.1746132989319488 1.2498779480999986\n",
      "1.061\n",
      "a1 and a2:  0.8924178961407159 0.5155585934592004\n",
      "b1 and b2:  -0.20473390984683737 1.2996302143999987\n",
      "1.064\n",
      "a1 and a2:  0.892296461346564 0.5120618931031005\n",
      "b1 and b2:  -0.23712345886139952 1.3500073262999988\n",
      "1.067\n",
      "a1 and a2:  0.892177673375852 0.5084128062179004\n",
      "b1 and b2:  -0.2717629028679992 1.4008081008999984\n",
      "1.069\n",
      "a1 and a2:  0.8920560669653943 0.5044782915108005\n",
      "b1 and b2:  -0.3091903147230132 1.4524561516999985\n",
      "1.073\n",
      "a1 and a2:  0.8919353068016324 0.5002548420861006\n",
      "b1 and b2:  -0.349583218119257 1.5049629032999985\n",
      "1.077\n",
      "a1 and a2:  0.891816420011961 0.4957172209569005\n",
      "b1 and b2:  -0.39326305256278726 1.5583283940999983\n",
      "1.081\n",
      "a1 and a2:  0.8916961671544278 0.4907239197300006\n",
      "b1 and b2:  -0.44098001678667786 1.6130131002999981\n",
      "1.087\n",
      "a1 and a2:  0.8915792905667198 0.4853137508608007\n",
      "b1 and b2:  -0.4928430242435328 1.668803868799998\n",
      "1.092\n",
      "a1 and a2:  0.8914637696957571 0.47939649305520077\n",
      "b1 and b2:  -0.5495395813807787 1.7259451191999977\n",
      "1.098\n",
      "a1 and a2:  0.8913504642277792 0.4728718533750009\n",
      "b1 and b2:  -0.6118874203873657 1.7846970124999977\n",
      "1.105\n",
      "a1 and a2:  0.8912497819060865 0.465433655237501\n",
      "b1 and b2:  -0.6815780061694373 1.845783337499997\n",
      "1.115\n",
      "a1 and a2:  0.8911657620362222 0.4572032025148011\n",
      "b1 and b2:  -0.7587770610355511 1.9087733366999968\n",
      "1.123\n",
      "a1 and a2:  0.8911085985376215 0.4479380060973013\n",
      "b1 and b2:  -0.8453128056849251 1.9742025136999966\n",
      "1.133\n",
      "a1 and a2:  0.8910795122946243 0.4374689185792015\n",
      "b1 and b2:  -0.9427875545196551 2.042382598399996\n",
      "1.144\n",
      "a1 and a2:  0.8910957567227181 0.4254640546642018\n",
      "b1 and b2:  -1.0541079718748132 2.1139042392999956\n",
      "1.157\n",
      "a1 and a2:  0.8911762473886651 0.4116744884514021\n",
      "b1 and b2:  -1.1817674622733345 2.189135271099995\n",
      "1.171\n",
      "a1 and a2:  0.8913354899751019 0.39593227570000245\n",
      "b1 and b2:  -1.328597058899906 2.2682044124999945\n",
      "1.185\n",
      "a1 and a2:  0.8915985668108076 0.37778939077280294\n",
      "b1 and b2:  -1.499078755418719 2.351787310099994\n",
      "1.201\n",
      "a1 and a2:  0.8919944846059139 0.35669524900590355\n",
      "b1 and b2:  -1.699253914900074 2.440655595899993\n",
      "1.219\n",
      "a1 and a2:  0.8925463879510254 0.33232780562160424\n",
      "b1 and b2:  -1.9346769205728966 2.535008455299992\n",
      "1.237\n",
      "a1 and a2:  0.893270508538117 0.304711510201105\n",
      "b1 and b2:  -2.208988599816822 2.6344066776999915\n",
      "1.253\n",
      "a1 and a2:  0.8941577471339922 0.27355046551170575\n",
      "b1 and b2:  -2.5283905785103653 2.7390217608999903\n",
      "1.269\n",
      "a1 and a2:  0.8951813366192688 0.23897927966080657\n",
      "b1 and b2:  -2.896265085923184 2.8483337686999897\n",
      "1.283\n",
      "a1 and a2:  0.8962536425571025 0.20169697740630718\n",
      "b1 and b2:  -3.3105881390960388 2.961082625699989\n",
      "1.293\n",
      "a1 and a2:  0.8973007788187376 0.16208317575370773\n",
      "b1 and b2:  -3.770852652728504 3.0766323400999886\n",
      "1.301\n",
      "a1 and a2:  0.8982216826321003 0.12131590845440805\n",
      "b1 and b2:  -4.2680570831982525 3.1932434463999884\n",
      "1.304\n",
      "a1 and a2:  0.8989501765287677 0.0799550528625082\n",
      "b1 and b2:  -4.797202441834179 3.310208512499988\n",
      "1.305\n",
      "a1 and a2:  0.8994501759139805 0.03811835058160834\n",
      "b1 and b2:  -5.355440471060427 3.4275342615999884\n",
      "1.306\n",
      "a1 and a2:  0.8996933737402453 -0.0038827027263916153\n",
      "b1 and b2:  -5.938699305313953 3.5448542615999883\n",
      "1.306\n",
      "a1 and a2:  0.8996646612308864 -0.0464246944383915\n",
      "b1 and b2:  -6.548869784634448 3.662887811199988\n",
      "1.308\n",
      "a1 and a2:  0.8993508156266661 -0.08967055441479134\n",
      "b1 and b2:  -7.186243234859382 3.782003273099988\n",
      "1.311\n",
      "a1 and a2:  0.8987339120305092 -0.13428437750039102\n",
      "b1 and b2:  -7.8563257028478475 3.9032827512999875\n",
      "1.317\n",
      "a1 and a2:  0.8977495731262994 -0.18169144080119037\n",
      "b1 and b2:  -8.573071019007582 4.028976378899987\n",
      "1.329\n",
      "a1 and a2:  0.8963127043422603 -0.23339394224378937\n",
      "b1 and b2:  -9.350178421505467 4.161485442299985\n",
      "1.347\n",
      "a1 and a2:  0.8941848197336077 -0.2932959476455874\n",
      "b1 and b2:  -10.22646619034132 4.306669243899983\n",
      "1.379\n",
      "a1 and a2:  0.8907184641645707 -0.3719802099199829\n",
      "b1 and b2:  -11.300816408558067 4.4799068383999785\n",
      "1.444\n",
      "a1 and a2:  0.8798798723624875 -0.5702228218749515\n",
      "b1 and b2:  -13.43148132650347 4.813799062499945\n",
      "1.725\n",
      "a1 and a2:  0.8771635612953036 -0.6048661328124932\n",
      "b1 and b2:  -14.137455882370794 4.92041093749999\n",
      "1.275\n",
      "a1 and a2:  0.8744218467900279 -0.6384021909999936\n",
      "b1 and b2:  -14.841212581125962 5.025353299999991\n",
      "1.27\n",
      "a1 and a2:  0.871675917799699 -0.6708405613124938\n",
      "b1 and b2:  -15.5420841450092 5.128642212499991\n",
      "1.265\n",
      "a1 and a2:  0.8689441622798216 -0.702210975999994\n",
      "b1 and b2:  -16.23945813654887 5.230297599999991\n",
      "1.26\n",
      "a1 and a2:  0.8661957857251423 -0.7327410007295942\n",
      "b1 and b2:  -16.9363127360119 5.3306617215999905\n",
      "1.256\n",
      "a1 and a2:  0.8634871742202862 -0.7622404253500944\n",
      "b1 and b2:  -17.6285761343163 5.429416575099991\n",
      "1.251\n",
      "a1 and a2:  0.8608187971701883 -0.7907311526255948\n",
      "b1 and b2:  -18.3157408362563 5.526587693599992\n",
      "1.246\n",
      "a1 and a2:  0.8581698158678229 -0.8184288962095949\n",
      "b1 and b2:  -19.00085782050706 5.6224982487999915\n",
      "1.242\n",
      "a1 and a2:  0.8555778798757805 -0.8451573089060951\n",
      "b1 and b2:  -19.679956613241046 5.716848455299992\n",
      "1.237\n",
      "a1 and a2:  0.8530234699101013 -0.8711248638020953\n",
      "b1 and b2:  -20.356145489103255 5.809962183699993\n",
      "1.233\n",
      "a1 and a2:  0.8505343166912446 -0.8961501216255955\n",
      "b1 and b2:  -21.02550229475124 5.901541235199993\n",
      "1.228\n",
      "a1 and a2:  0.8480913574233575 -0.9204468190975956\n",
      "b1 and b2:  -21.691164776000168 5.991907942399992\n",
      "1.224\n",
      "a1 and a2:  0.8457029158681287 -0.9440262559999958\n",
      "b1 and b2:  -22.352796818461407 6.081074799999993\n",
      "1.22\n",
      "a1 and a2:  0.8433684567573815 -0.966898302233596\n",
      "b1 and b2:  -23.010084871837364 6.1690417695999935\n",
      "1.216\n",
      "a1 and a2:  0.8410974757841801 -0.9890728283135961\n",
      "b1 and b2:  -23.66270707615054 6.255828812799993\n",
      "1.212\n",
      "a1 and a2:  0.8388894984194499 -1.0105597053695963\n",
      "b1 and b2:  -24.31038993691418 6.341445891199993\n",
      "1.208\n",
      "a1 and a2:  0.836744057523751 -1.0313788051455963\n",
      "b1 and b2:  -24.952849463695372 6.425902966399994\n",
      "1.204\n",
      "a1 and a2:  0.8346333865266965 -1.0517035743300964\n",
      "b1 and b2:  -25.593316832200145 6.509487310099995\n",
      "1.201\n",
      "a1 and a2:  0.8325919473015034 -1.0713737570980968\n",
      "b1 and b2:  -26.22807916927141 6.591931787299995\n",
      "1.197\n",
      "a1 and a2:  0.8305952840078529 -1.0905641928495966\n",
      "b1 and b2:  -26.860372179493798 6.6735191383999934\n",
      "1.194\n",
      "a1 and a2:  0.8286765992361733 -1.109125070999997\n",
      "b1 and b2:  -27.486516053721555 6.753980899999994\n",
      "1.19\n",
      "a1 and a2:  0.8268028721864294 -1.127230883846097\n",
      "b1 and b2:  -28.109766928122642 6.833601170299994\n",
      "1.187\n",
      "a1 and a2:  0.8249703092379019 -1.144873198873597\n",
      "b1 and b2:  -28.729927547515285 6.912386950399994\n",
      "1.184\n",
      "a1 and a2:  0.823188728019238 -1.1620619635620972\n",
      "b1 and b2:  -29.34681611598805 6.990348224099995\n",
      "1.181\n",
      "a1 and a2:  0.8214579563365383 -1.1788071255855974\n",
      "b1 and b2:  -29.96025975193623 7.067484975199995\n",
      "1.178\n",
      "a1 and a2:  0.8197778317985014 -1.1951186328124972\n",
      "b1 and b2:  -30.570085428205758 7.143797187499994\n",
      "1.175\n",
      "a1 and a2:  0.8181239066498855 -1.2111398037540972\n",
      "b1 and b2:  -31.179579687789374 7.2195750216999945\n",
      "1.173\n",
      "a1 and a2:  0.8165148755100405 -1.2267317709999976\n",
      "b1 and b2:  -31.78515687645225 7.294546299999995\n",
      "1.17\n",
      "a1 and a2:  0.8149561128372687 -1.2418999453820976\n",
      "b1 and b2:  -32.386676031931046 7.368712996299996\n",
      "1.167\n",
      "a1 and a2:  0.8134341985760134 -1.2567921513124976\n",
      "b1 and b2:  -32.98741768488165 7.442340462499995\n",
      "1.165\n",
      "a1 and a2:  0.8119558794672894 -1.2712705567535978\n",
      "b1 and b2:  -33.58383082019733 7.515176152799995\n",
      "1.162\n",
      "a1 and a2:  0.8105148674535428 -1.2854795359999975\n",
      "b1 and b2:  -34.17920057919794 7.587489599999995\n",
      "1.16\n",
      "a1 and a2:  0.809116629149726 -1.2992880305700978\n",
      "b1 and b2:  -34.76999832753892 7.659014239299996\n",
      "1.157\n",
      "a1 and a2:  0.8077562185936178 -1.3128336888124978\n",
      "b1 and b2:  -35.359511840938566 7.730013637499996\n",
      "1.155\n",
      "a1 and a2:  0.8064260696488892 -1.3261119923780977\n",
      "b1 and b2:  -35.94762913081736 7.800495407699996\n",
      "1.153\n",
      "a1 and a2:  0.8051261276229142 -1.339132926310098\n",
      "b1 and b2:  -36.53425363060406 7.870469545099996\n",
      "1.151\n",
      "a1 and a2:  0.8038563501909796 -1.351896475690098\n",
      "b1 and b2:  -37.11927721493859 7.939926044899996\n",
      "1.149\n",
      "a1 and a2:  0.8026166873328895 -1.3644126256380982\n",
      "b1 and b2:  -37.70261322587467 8.008874902299997\n",
      "1.147\n",
      "a1 and a2:  0.8014071095108422 -1.376671361312498\n",
      "b1 and b2:  -38.28415348234342 8.077306112499995\n",
      "1.145\n",
      "a1 and a2:  0.8002275702134768 -1.388682667910098\n",
      "b1 and b2:  -38.86381195427929 8.145239670699997\n",
      "1.143\n",
      "a1 and a2:  0.7990780338207196 -1.400446530666098\n",
      "b1 and b2:  -39.44150038067157 8.212665572099995\n",
      "1.141\n",
      "a1 and a2:  0.7979584580589089 -1.4119729348540981\n",
      "b1 and b2:  -40.01713190088524 8.279593811899996\n",
      "1.139\n",
      "a1 and a2:  0.7968688185616714 -1.4232518657860982\n",
      "b1 and b2:  -40.59061820106405 8.346014385299997\n",
      "1.137\n",
      "a1 and a2:  0.7957897039404952 -1.4344122742015981\n",
      "b1 and b2:  -41.165287777621934 8.412185545599996\n",
      "1.136\n",
      "a1 and a2:  0.7947400508354615 -1.4453349677935983\n",
      "b1 and b2:  -41.737665388424 8.477869610399997\n",
      "1.134\n",
      "a1 and a2:  0.7937202474125432 -1.4560201515775983\n",
      "b1 and b2:  -42.30767451598262 8.543055996799996\n",
      "1.132\n",
      "a1 and a2:  0.792730259171806 -1.4664778109999985\n",
      "b1 and b2:  -42.875228083713694 8.607754699999997\n",
      "1.13\n",
      "a1 and a2:  0.7917517179191731 -1.4768225645380983\n",
      "b1 and b2:  -43.44366884715132 8.672199918899995\n",
      "1.129\n",
      "a1 and a2:  0.7908015370199686 -1.4869339102140986\n",
      "b1 and b2:  -44.00952741786917 8.736162088299997\n",
      "1.127\n",
      "a1 and a2:  0.7898631873799625 -1.4969304987375986\n",
      "b1 and b2:  -44.57614055645119 8.799879037599997\n",
      "1.126\n",
      "a1 and a2:  0.788952739341519 -1.5067054981375987\n",
      "b1 and b2:  -45.14004061569076 8.863114662399997\n",
      "1.124\n",
      "a1 and a2:  0.7880545167700218 -1.5163739054140986\n",
      "b1 and b2:  -45.704570380912465 8.926103336699997\n",
      "1.123\n",
      "a1 and a2:  0.7871663689912012 -1.5259229153455987\n",
      "b1 and b2:  -46.26968209404961 8.988852584799996\n",
      "1.122\n",
      "a1 and a2:  0.786315539488973 -1.5352527359999986\n",
      "b1 and b2:  -46.83190380605639 9.051122799999996\n",
      "1.12\n",
      "a1 and a2:  0.7854774535457701 -1.5444735431420986\n",
      "b1 and b2:  -47.39458840681572 9.113153765899996\n",
      "1.119\n",
      "a1 and a2:  0.78466620931389 -1.5534769421220986\n",
      "b1 and b2:  -47.95427549356499 9.174707411299996\n",
      "1.117\n",
      "a1 and a2:  0.7838681359364009 -1.5623795303935988\n",
      "b1 and b2:  -48.5143176119396 9.236030089599996\n",
      "1.116\n",
      "a1 and a2:  0.7830800907373179 -1.571172708812499\n",
      "b1 and b2:  -49.0746653025825 9.297113337499997\n",
      "1.115\n",
      "a1 and a2:  0.7823020679973551 -1.5798564756015987\n",
      "b1 and b2:  -49.6352791006442 9.357957154399998\n",
      "1.114\n",
      "a1 and a2:  0.7815501305687252 -1.588335767193599\n",
      "b1 and b2:  -50.19267815373645 9.418336492799998\n",
      "1.112\n",
      "a1 and a2:  0.7815501305687252 -1.588335767193599\n",
      "b1 and b2:  -50.19267815373645 9.418336492799998\n"
     ]
    }
   ],
   "source": [
    "def run_new_111(desired, lr):   # will need to check work on bias/b and make sure everything is correct in this\n",
    "\n",
    "    #desired = desired -.032\n",
    "    run_simulation(run_time, 1000)\n",
    "    \n",
    "    n1 = neurons_hidden[0].spike_time\n",
    "    n2 = neurons_output[0].spike_time\n",
    "    #print(\"n1, n2 : \", n1, n2)\n",
    "    #loss = calc_loss(output, desired)\n",
    "\n",
    "    delta_a2 = - d_loss_d_weight_2(n1, n2, desired)\n",
    "\n",
    "    syn_a1 = syn_layer_1[0].a\n",
    "    syn_a2 = syn_layer_2[0].a\n",
    "\n",
    "    delta_a1 = - d_loss_d_weight_1(n1, n2, desired, syn_a2, inputs[0]) # trying to just input a as a the weight directly\n",
    "\n",
    "    new_a1 = update_param(lr, delta_a1, syn_a1)\n",
    "    new_a2 = update_param(lr, delta_a2, syn_a2)\n",
    "\n",
    "    syn_b1 = syn_layer_1[0].b\n",
    "    syn_b2 = syn_layer_2[0].b\n",
    "\n",
    "    delta_b2 = d_loss_d_bias_2(n2, desired) # b is not - but left as + here\n",
    "    delta_b1 = d_loss_d_bias_1(n1, n2, delta_b2, desired, syn_b2) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "    new_b1 = update_param(lr, delta_b1, syn_b1)\n",
    "    new_b2 = update_param(lr, delta_b2, syn_b2)\n",
    "\n",
    "    return new_a1, new_a2, new_b1, new_b2, n1, n2\n",
    "\n",
    "\n",
    "#run_new_111(input, 1.5, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "genesis()\n",
    "\n",
    "for i in range(100):\n",
    "   \n",
    "    reset_network() # needs to add reset for all that isn't the genesis call\n",
    "   \n",
    "    sa1, sa2, sb1, sb2, n1, n2 = run_new_111(desired=0.5, lr=.1) # basic range should work just need to find the range and such but can do later then \n",
    "                                # desired has to be between 1.532 and 3.335\n",
    "                                # also need to fully reset when values overeach to get that 1.532 and 3.335\n",
    "                                # limited ranged based on limit else it crashes and resets to at near start spike time\n",
    "                                # need roundeding else it falls off for speed\n",
    "    # l1_a[0][0] = sa1\n",
    "    # l2_a[0][0] = sa2\n",
    "    # l1_b[0][0] = sb1\n",
    "    # l2_b[0][0] = sb2\n",
    "    syn_layer_1[0].a = np.round(sa1, 5)\n",
    "    syn_layer_2[0].a = np.round(sa2, 5)\n",
    "    syn_layer_1[0].b = np.round(sb1, 5)\n",
    "    syn_layer_2[0].b = np.round(sb2, 5)\n",
    "\n",
    "    print(\"a1 and a2: \", sa1, sa2)\n",
    "    print(\"b1 and b2: \", sb1, sb2)\n",
    "    print(np.round(n2, 3))\n",
    "\n",
    "print(\"a1 and a2: \", sa1, sa2)\n",
    "print(\"b1 and b2: \", sb1, sb2)\n",
    "\n",
    "# print(l1_a)\n",
    "# print(l1_b)\n",
    "# print(l2_a)\n",
    "# print(l2_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "291febc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_synapses()      # create synapses objects once\n",
    "# for epoch in range(10):\n",
    "#    # clear out last run’s flags & times\n",
    "#     sa1, sa2, sb1, sb2, n1, n2 = run_new_111(1.5, 0.1)\n",
    "#     # write back into synapse objects:\n",
    "#     syn_layer_1[0].a = sa1\n",
    "#     syn_layer_2[0].a = sa2\n",
    "#     syn_layer_1[0].b = sb1\n",
    "#     syn_layer_2[0].b = sb2\n",
    "#     print(f\"Epoch {epoch}: a1={sa1:.4f}, a2={sa2:.4f}, n2={n2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6c1d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_spike_raster(inputs,\n",
    "#                       syn_layer_1,\n",
    "#                       syn_layer_2,\n",
    "#                       syn_outputs):\n",
    "#     \"\"\"\n",
    "#     inputs: list of input‐neuron spike times\n",
    "#     syn_layer_1: list of synapse objects for layer 1 (must have .spike_time)\n",
    "#     syn_layer_2: list of synapse objects for layer 2\n",
    "#     syn_outputs: list of synapse objects for output layer\n",
    "#     \"\"\"\n",
    "#     # collect times & y‐positions\n",
    "#     times = []\n",
    "#     rows  = []\n",
    "#     labels = []\n",
    "\n",
    "#     # row 0 = input spikes\n",
    "#     for t in inputs:\n",
    "#         times.append(t)\n",
    "#         rows.append(0)\n",
    "#         labels.append(\"input\")\n",
    "\n",
    "#     # row 1 = layer‐1 synapses\n",
    "#     for idx, s in enumerate(syn_layer_1):\n",
    "#         times.append(s.spike_time)\n",
    "#         rows.append(1)\n",
    "#         labels.append(f\"syn1_{idx}\")\n",
    "\n",
    "#     # row 2 = layer‐2 synapses\n",
    "#     for idx, s in enumerate(syn_layer_2):\n",
    "#         times.append(s.spike_time)\n",
    "#         rows.append(2)\n",
    "#         labels.append(f\"syn2_{idx}\")\n",
    "\n",
    "#     # row 3 = output synapses\n",
    "#     for idx, s in enumerate(syn_outputs):\n",
    "#         times.append(s.spike_time)\n",
    "#         rows.append(3)\n",
    "#         labels.append(f\"out_{idx}\")\n",
    "\n",
    "#     # now plot\n",
    "#     plt.figure(figsize=(8, 3))\n",
    "#     plt.scatter(times, rows, marker='|', s=200)\n",
    "#     plt.yticks([0,1,2,3], ['input','syn1','syn2','output'])\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.title(\"Spike / Synapse firing times\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # (assuming you kept your syn_layer_1, syn_layer_2, syn_outputs in scope)\n",
    "# final_time = run_simulation(inputs, run_time, 10000)\n",
    "# print(\"final spike at\", final_time)\n",
    "\n",
    "# plot_spike_raster(\n",
    "#     inputs,\n",
    "#     syn_layer_1,\n",
    "#     syn_layer_2,\n",
    "#     syn_outputs\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7860c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train\n",
    " \n",
    "# input = 0.5  # all inputs have to be larger then 0.5?\n",
    "# lr = 0.1\n",
    "# a1 = 1.1\n",
    "# a2 = 0.9\n",
    "# b1 = -0.1\n",
    "# b2 = 0.0\n",
    "# desired = 1.0\n",
    "# # weird thing with moving the lr too much need to keep low nery steep start with changing lower m maybe?\n",
    "# for i in range(1000):\n",
    "#     a1, a2, b1, b2, r = run_basic(input, a1, a2, b1, b2, desired, lr)\n",
    "#     # print(\"a1 and a2: \", a1, a2)\n",
    "#     # print(\"b1 and b2: \", b1, b2)\n",
    "#     print(np.round(r[0][1], 3))\n",
    "# print(\"a1 and a2: \", a1, a2)\n",
    "# print(\"b1 and b2: \", b1, b2)\n",
    "# print(np.round(r[0][1], 3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2449905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Brian Simulation: \n",
    "# from brian2 import *\n",
    "# import numpy as np\n",
    "# import logging\n",
    "# import warnings\n",
    "# from brian2 import prefs, set_device\n",
    "# # Tell Brian2 to use the Cython code generator:\n",
    "# prefs.codegen.target = 'cython'\n",
    "\n",
    "# # Optionally compile but keep Python interface:\n",
    "# set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "# # suppress overflow warnings\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "# numpy.seterr(over='ignore', under='ignore')\n",
    "# logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Spike timing and derivative\n",
    "# layer_bias = 0.5\n",
    "# start_scope()\n",
    "# defaultclock.dt = 0.001*ms\n",
    "\n",
    "# @implementation('numpy', discard_units=True)\n",
    "# @check_units(time=1, a=1, b=1, m=1, c=1, layer=1, result=1)\n",
    "# def ians_sigmoid(time, a, b, m, c, layer):\n",
    "#     print(time)\n",
    "#     global layer_bias\n",
    "#     print(\"result of ians_sig: \", 1/ ((1/a) + np.exp(((-time*m) + b + c + layer * layer_bias))))\n",
    "#     return 1/ ((1/a) + np.exp(((-time*m) + b + c + layer * layer_bias)))\n",
    "\n",
    "\n",
    "\n",
    "# def mini1x1x1(inputs):\n",
    "#     \"\"\"\n",
    "#     Mini 1x1x1 network with a single input, hidden, and output neuron.\n",
    "#     This is a minimal example to demonstrate the basic structure of a spiking neural network.\n",
    "#     \"\"\"\n",
    "#     # Define network sizes\n",
    "#     n_input = 1\n",
    "#     n_hidden = 1\n",
    "#     n_output = 1\n",
    "#     n_total = n_input + n_hidden + n_output\n",
    "\n",
    "#     # Full neuron group\n",
    "#     neurons = NeuronGroup(n_total, '''\n",
    "#         v : 1\n",
    "#         sum : 1\n",
    "#         spikes_received : 1\n",
    "#         scheduled_time : second\n",
    "#         global_clock : 1\n",
    "#         spiked : boolean\n",
    "#     ''', threshold='v > 0.5', reset='''\n",
    "#     v = 0\n",
    "#     ''', method='exact')\n",
    "#     neurons.v = 0\n",
    "#     neurons.scheduled_time = 1e9 * second\n",
    "#     neurons.global_clock = 0.0\n",
    "#     neurons.sum = 0.0\n",
    "#     neurons.spikes_received = 0.0\n",
    "\n",
    "#     # Spike inputs (one per input neuron)     \n",
    "#     stim = SpikeGeneratorGroup(n_input, indices=range(n_input), times=(inputs) * ms)\n",
    "\n",
    "#     # Input → Hidden connections\n",
    "#     syn_input = Synapses(stim, neurons[0:n_input], '''\n",
    "#         layer : 1\n",
    "#         a : 1\n",
    "#         b : 1\n",
    "#     ''', on_pre='''\n",
    "#         spikes_received += 1\n",
    "#         scheduled_time = (ians_sigmoid(t/ms, a, b, 3, 5, 0) + t/ms) * ms \n",
    "#     ''') \n",
    "#     syn_input.connect(j='i')  # connect stim[i] to neurons[i]\n",
    "#     syn_input.layer = 0\n",
    "#     syn_input.a = 5.0\n",
    "#     syn_input.b = 0.0\n",
    "\n",
    "\n",
    "#     # Hidden layer: input → hidden\n",
    "#     syn_hidden = Synapses(neurons[0:n_input], neurons[n_input:n_input + n_hidden], '''\n",
    "#         w : 1\n",
    "#         layer : 1\n",
    "#     ''', on_pre='''\n",
    "#         spikes_received += 1\n",
    "#         scheduled_time = (ians_sigmoid(t/ms, a, b, 3, 5, 0) + t/ms) * ms \n",
    "#     ''')\n",
    "#     syn_hidden.connect()\n",
    "#     #syn_hidden.w = w1 \n",
    "#     syn_hidden.layer = 1\n",
    "\n",
    "#     # Output layer: hidden → output\n",
    "#     # syn_output = Synapses(neurons[n_input:n_input + n_hidden], neurons[n_input + n_hidden:n_total], '''\n",
    "#     #     w : 1\n",
    "#     #     layer : 1\n",
    "#     # ''', on_pre='''\n",
    "#     #     spikes_received += 1\n",
    "#     #     v += 1\n",
    "    \n",
    "#     # ''')\n",
    "#     # syn_output.connect()\n",
    "#     # #syn_output.w = w2 \n",
    "#     # syn_output.layer = 2\n",
    "\n",
    "#     window = 0.02*ms\n",
    "\n",
    "#     neurons.run_regularly('''\n",
    "                          \n",
    "#         v = int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "#         in_window = (scheduled_time >= (t - 0.02*ms)) * (scheduled_time <= (t + 0.02*ms))\n",
    "   \n",
    "#         fire_flag = int(in_window * (1 - spiked))\n",
    "        \n",
    "#         v      = fire_flag * 2\n",
    "#         spiked = spiked or (fire_flag > 0)\n",
    "  \n",
    "#     ''', dt=0.001*ms)\n",
    "\n",
    "#         #v = int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "\n",
    "#     # Monitors\n",
    "#     mon = StateMonitor(neurons, 'v', record=True, dt=0.0001*ms)\n",
    "#     mon_sum = StateMonitor(neurons, 'sum', record=True)\n",
    "#     sp_mon = StateMonitor(neurons, 'spikes_received', record=True)\n",
    "#     sch_time = StateMonitor(neurons, 'scheduled_time', record=True)\n",
    "\n",
    "\n",
    "#     spikemon = SpikeMonitor(neurons)\n",
    "\n",
    "\n",
    "#     run(5*ms)\n",
    "\n",
    "#     # Plot voltages\n",
    "#     figure(figsize=(10, 6))\n",
    "#     for i in range(n_total):  # All neurons\n",
    "#         plot(mon.t/ms, mon.v[i], label=f'Neuron {i}')\n",
    "#     xlabel('Time (ms)')\n",
    "#     ylabel('Membrane potential')\n",
    "#     legend()\n",
    "#     title('SNN Spike Propagation Across All Layers')\n",
    "#     show()\n",
    "\n",
    "#     # plot(mon_sum.t/ms, mon_sum.sum[3])  # or any neuron index\n",
    "#     # print(mon_sum.sum[1])\n",
    "#     # print(sp_mon.spikes_received[1])\n",
    "#     # print(sch_time.scheduled_time[1])\n",
    "\n",
    "#     for i in range(n_total):\n",
    "#         times = spikemon.spike_trains()[i]\n",
    "#         if len(times) > 0:\n",
    "#             formatted_times = [f\"{t/ms:.3f} ms\" for t in times]\n",
    "#             print(f\"Neuron {i} spike times: {formatted_times}\")\n",
    "\n",
    "# input = [0.5]\n",
    "\n",
    "# mini1x1x1(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
