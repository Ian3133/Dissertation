{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "99d3ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310585786300049\n"
     ]
    }
   ],
   "source": [
    "# Math Section for Skuld model v2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "a = 1 # the main part of what i will be changing for weights changes the size of the sigmoid without changing near zero\n",
    "b = 0  # bias will add to updating with ^ later\n",
    "m = 5 # the rate of change of the sigmoid \n",
    "c = 3 # a constant I used, same as but but shift the sigmod to a place where i 0 is close to 0 (actuyll 0.04 but whatever)\n",
    "layer_bias = 0.5\n",
    "\n",
    "\n",
    "def ians_sigmoid(x, a, b, m, c, layer=0):\n",
    "    return 1/ ((1/a) + np.exp(((-x*m) + b + c + layer * layer_bias)))\n",
    "print(ians_sigmoid(0.8, a, b, m, c))\n",
    "\n",
    "def calc_loss(output, desired):\n",
    "    # can change but not a requirement yet\n",
    "    return (0.5 * ((output - desired) ** 2))\n",
    "\n",
    "\n",
    "def d_loss_d_weight_2(n1, n2, desired):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1) # removedall layer_bias can check layer if correct call\n",
    "\n",
    "def d_loss_d_weight_1(n1, n2, desired, w2, input):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1 * w2 * (1- n1) * input)\n",
    "\n",
    "def new_weight(lr, grad, w):\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7860c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13019603174325\n",
      "1.1426795189907628\n",
      "1.155483485629346\n",
      "1.1685172415594818\n",
      "1.1816940394519566\n",
      "1.1949334857828209\n",
      "1.208162961817237\n",
      "1.2213181437142775\n",
      "1.2343428150956925\n",
      "1.2471882101884901\n",
      "1.25981211936192\n",
      "1.2721779483358295\n",
      "1.2842538656368863\n",
      "1.2960121147802162\n",
      "1.3074285177593068\n",
      "1.3184821593891438\n",
      "1.3291552187591518\n",
      "1.3394329030384469\n",
      "1.3493034374739432\n",
      "1.3587580706268567\n",
      "1.3677910628869325\n",
      "1.3763996367455429\n",
      "1.3845838774567936\n",
      "1.3923465814348428\n",
      "1.3996930564322665\n",
      "1.406630882058247\n",
      "1.413169641675675\n",
      "1.4193206374991965\n",
      "1.4250966002218728\n",
      "1.43051140315119\n",
      "1.4355797890135362\n",
      "1.4403171155935437\n",
      "1.4447391244320018\n",
      "1.4488617350570605\n",
      "1.452700865746575\n",
      "1.456272280643208\n",
      "1.4595914621620412\n",
      "1.4626735070149435\n",
      "1.465533043787561\n",
      "1.4681841698004647\n",
      "1.47064040492389\n",
      "1.4729146600579972\n",
      "1.4750192181056083\n",
      "1.4769657254257746\n",
      "1.478765191944067\n",
      "1.4804279982936848\n",
      "1.4819639085594645\n",
      "1.4833820873870471\n",
      "1.4846911203970166\n",
      "1.485899037006043\n",
      "1.4870133349026944\n",
      "1.4880410055544435\n",
      "1.4889885602350896\n",
      "1.4898620561592533\n",
      "1.4906671223941492\n",
      "1.4914089852898105\n",
      "1.4920924932287247\n",
      "1.4927221405458808\n",
      "1.493302090511692\n",
      "1.4938361973044176\n",
      "1.4943280269265613\n",
      "1.4947808770422406\n",
      "1.4951977957305074\n",
      "1.4955815991638188\n",
      "1.4959348882318966\n",
      "1.496260064139599\n",
      "1.4965593430136919\n",
      "1.4968347695578503\n",
      "1.4970882297982058\n",
      "1.497321462963622\n",
      "1.4975360725457385\n",
      "1.4977335365839584\n",
      "1.4979152172201478\n",
      "1.4980823695668741\n",
      "1.4982361499317984\n",
      "1.4983776234393438\n",
      "1.4985077710890853\n",
      "1.4986274962885338\n",
      "1.4987376308961213\n",
      "1.498838940808315\n",
      "1.4989321311229036\n",
      "1.499017850908619\n",
      "1.499096697609449\n",
      "1.4991692211102192\n",
      "1.4992359274883098\n",
      "1.4992972824747524\n",
      "1.4993537146463667\n",
      "1.4994056183691487\n",
      "1.4994533565116832\n",
      "1.499497262946056\n",
      "1.4995376448524822\n",
      "1.4995747848426957\n",
      "1.4996089429160513\n",
      "1.4996403582612703\n",
      "1.4996692509157827\n",
      "1.499695823293755\n",
      "1.499720261593024\n",
      "1.4997427370904255\n",
      "1.4997634073342372\n",
      "1.4997824172418366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_pass(input, a1, a2):\n",
    "    sig_r1 = ians_sigmoid(input, a1, b, m, c)\n",
    "    delay_1 = sig_r1 + input\n",
    "    sig_r2 = ians_sigmoid(delay_1, a2, b, m, c, layer=1)\n",
    "    delay_2 = sig_r2 + delay_1\n",
    "\n",
    "    return ([delay_1, delay_2], [sig_r1, sig_r2])\n",
    "\n",
    "# forward_pass(0.4)[0] # here shows the delays calculated\n",
    "# forward_pass(0.4)[1] # here is hte sig values without added time delays calcuated\n",
    "# input = 0.4\n",
    "# result = forward_pass(input)\n",
    "# output = result[0][1]\n",
    "# n1 = result[0][0]\n",
    "# n2 = result[0][1]\n",
    "\n",
    "# loss = calc_loss(output, 1.5)\n",
    "\n",
    "# delta_w2 = d_loss_d_weight_2(n1, n2, 1.5)\n",
    "\n",
    "# delta_w1 = d_loss_d_weight_1(n1, n2, 1.5, a, input) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "# print(n1, n2)\n",
    "# print(delta_w2, delta_w1)\n",
    "\n",
    "\n",
    "# # just an attempt here: \n",
    "# lr = 0.1\n",
    "\n",
    "# new_a1 = new_weight(lr, delta_w1, a)\n",
    "# new_a2 = new_weight(lr, delta_w2, a)\n",
    "\n",
    "# print(new_a1, new_a2)\n",
    "\n",
    "\n",
    "def run_basic(input, a1, a2, desired, lr):\n",
    "\n",
    "    result = forward_pass(input, a1, a2)\n",
    "    output = result[0][1]\n",
    "    n1 = result[0][0]\n",
    "    n2 = result[0][1]\n",
    "\n",
    "    #loss = calc_loss(output, desired)\n",
    "\n",
    "    delta_w2 = - d_loss_d_weight_2(n1, n2, desired)\n",
    "\n",
    "    delta_w1 = - d_loss_d_weight_1(n1, n2, desired, a2, input) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    new_a1 = new_weight(lr, delta_w1, a1)\n",
    "    new_a2 = new_weight(lr, delta_w2, a2)\n",
    "\n",
    "    return new_a1, new_a2, result\n",
    "\n",
    "\n",
    "input = 0.4\n",
    "lr = 1.5\n",
    "a1 = 1.0\n",
    "a2 = 1.0\n",
    "desired = 1.5\n",
    "\n",
    "for i in range(100):\n",
    "    a1, a2, r = run_basic(input, a1, a2, desired, lr)\n",
    "    print(r[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2739fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23182316571450962\n",
      "-0.0015151549496330169 -0.08691101187823634\n",
      "0.373269111739985\n",
      "old w's 0.373269111739985 0.23182316571450962\n",
      "new W's 0.3734206272349483 0.24051426690233324\n"
     ]
    }
   ],
   "source": [
    "# Math section for Skuld model v1\n",
    "import math\n",
    "import numpy as np\n",
    "# inputs for Skuld model\n",
    "input = 0.3\n",
    "layer_bias = 0.5\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "b = 0.05 \n",
    "m = 1\n",
    "\n",
    "# weights_function: \\\n",
    "\n",
    "def apply_weight(x, a, b, layers=0):\n",
    "    return math.exp(-((a - x + layers*layer_bias) ** 2) / b)\n",
    "\n",
    "# Foward Pass\n",
    "\n",
    "def forward_pass(input, a1, a2, b):\n",
    "\n",
    "    n1 = apply_weight(input, a1, b, layers=0) + input\n",
    "    n2 = apply_weight(n1, a2, b, layers=1) + n1\n",
    "\n",
    "    return n2, n1\n",
    "\n",
    "# Loss calcualtion\n",
    "\n",
    "def loss_function(output, desired):\n",
    "    # can change but not a requirement yet\n",
    "    return (0.5 * ((output - desired) ** 2))\n",
    "\n",
    "\n",
    "# Grads calculated\n",
    "\n",
    "def d_loss_d_weight_2(n2, n1, desired):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2-layer_bias) * (1-(n2-layer_bias)) * n1)\n",
    "    \n",
    "n2, n1 = forward_pass(input, a1, a2, b)\n",
    "\n",
    "# print(d_loss_d_weight_2(n2, n1, 1.5))\n",
    "\n",
    "\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "def integral_of_exp(x0, x, a, b, layer=0):\n",
    "    \"\"\"\n",
    "    Compute the definite integral:\n",
    "    âˆ«[x0 to x] exp(-((a - t + 0.5)^2)/b) dt\n",
    "    \"\"\"\n",
    "    result, _ = quad(lambda t: np.exp(-((a - t + (layer_bias * layer))**2) / b), x0, x)\n",
    "    return result\n",
    "\n",
    "\n",
    "    \n",
    "n2, n1 = forward_pass(input, a1, a2, b)\n",
    "\n",
    "grad_w2 =d_loss_d_weight_2(n2, n1, 1.5)\n",
    "\n",
    "def d_loss_d_weight_1(n2, n1, desired, w2, input):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2-layer_bias) * (1-(n2-layer_bias)) * n1 * w2 * (1- n1) * input)\n",
    "\n",
    "w2 = integral_of_exp(0, n2, a1, b, layer=1)\n",
    "print(w2)\n",
    "\n",
    "grad_w1 = d_loss_d_weight_1(n2, n1, 1.5, w2, input)\n",
    "\n",
    "print(grad_w1, grad_w2)\n",
    "\n",
    "lr = 0.1 \n",
    "\n",
    "w1 = integral_of_exp(0, n1, a2, b, layer=0)\n",
    "print(w1)\n",
    "\n",
    "def new_weight(lr, grad, w):\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "w2_new = new_weight(lr, grad_w2, w2)\n",
    "w1_new = new_weight(lr, grad_w1, w1)\n",
    "print(\"old w's\", w1, w2)\n",
    "print(\"new W's\", w1_new, w2_new)\n",
    "\n",
    "def shift_curve():\n",
    "\n",
    "    # shift the curve to the right or left\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "784138d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017380186922928538"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "integral_of_exp(0, 0.73, a1, b, layer=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
