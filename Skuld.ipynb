{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d3ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1543436937742046 1.6736270862684832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0027900354800456,\n",
       " 0.977404037283446,\n",
       " 0.0,\n",
       " 0.0442927493326699,\n",
       " ([1.1543436937742046, 1.6736270862684832],\n",
       "  [0.35434369377420455, 0.5192833924942787]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Math Section for Skuld model v2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "a = 1 # the main part of what i will be changing for weights changes the size of the sigmoid without changing near zero\n",
    "b = 0  # bias will add to updating with ^ later\n",
    "m = 0.5 # the rate of change of the sigmoid \n",
    "c = 1 # a constant I used, same as but but shift the sigmod to a place where i 0 is close to 0 (actuyll 0.04 but whatever)\n",
    "layer_bias = -0.5\n",
    "\n",
    "\n",
    "def ians_sigmoid(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "    # width = max(width, 0.01)  # prevent division by 0 or instability\n",
    "    # layer_shift = layer * 0.1  # optional small bias per layer\n",
    "    # return scale * np.exp(-((x - center - layer_shift) ** 2) / (2 * width ** 2))\n",
    "    return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "    #return 1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias)))\n",
    "#print(ians_sigmoid(0.8, a, b, m, c))\n",
    "\n",
    "# def ians_sigmoid_new(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "#     return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "\n",
    "def calc_loss(output, desired):\n",
    "    # can change but not a requirement yet\n",
    "    return (0.5 * ((output - desired) ** 2))\n",
    "\n",
    "\n",
    "def d_loss_d_weight_2(n1, n2, desired):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1) # removedall layer_bias can check layer if correct call\n",
    "\n",
    "def d_loss_d_weight_1(n1, n2, desired, w2, input):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1 * w2 * (1- n1) * input)\n",
    "\n",
    "\n",
    "def d_loss_d_bias_2(n2, desired):\n",
    "    return (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "\n",
    "def d_loss_d_bias_1(n1, n2, delta2, desired, a2):\n",
    "    #delta2 = (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "    return delta2 * a2 * (n1 - layer_bias) * (1 - (n1 - layer_bias))\n",
    "\n",
    "\n",
    "def update_param(lr, grad, w):\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(input, a1, a2, b1, b2):\n",
    "    sig_r1 = ians_sigmoid(input, a1, b1, m, c)\n",
    "    delay_1 = sig_r1 + input\n",
    "    sig_r2 = ians_sigmoid(delay_1, a2, b2, m, c, layer=1)\n",
    "    delay_2 = sig_r2 + delay_1\n",
    "\n",
    "    return ([delay_1, delay_2], [sig_r1, sig_r2])\n",
    "\n",
    "\n",
    "def run_basic(input, a1, a2, b1, b2, desired, lr):   # will need to check work on bias/b and make sure everything is correct in this\n",
    "\n",
    "    result = forward_pass(input, a1, a2, b1, b2)\n",
    "    output = result[0][1]\n",
    "    n1 = result[0][0]\n",
    "    n2 = result[0][1]\n",
    "    print(n1, n2)\n",
    "    #loss = calc_loss(output, desired)\n",
    "\n",
    "    delta_a2 = - d_loss_d_weight_2(n1, n2, desired)\n",
    "\n",
    "    delta_a1 = - d_loss_d_weight_1(n1, n2, desired, a2, input) # trying to just input a as a the weight directly\n",
    "\n",
    "    new_a1 = update_param(lr, delta_a1, a1)\n",
    "    new_a2 = update_param(lr, delta_a2, a2)\n",
    "\n",
    "\n",
    "    delta_b2 = d_loss_d_bias_2(n2, desired)\n",
    "    delta_b1 = d_loss_d_bias_1(n1, n2, delta_b2, desired, b2) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "    new_b1 = update_param(lr, delta_b1, b1)\n",
    "    new_b2 = update_param(lr, delta_b2, b2)\n",
    "\n",
    "    return new_a1, new_a2, new_b1, new_b2, result\n",
    "\n",
    "\n",
    "run_basic(0.8, 1, 1, 0, 0, 1.5, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34adcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron: \n",
    "    def __init__(self, layer, number):\n",
    "\n",
    "        self.layer = layer\n",
    "        self.number = number\n",
    "    \n",
    "\n",
    "        self.current_layer = 0\n",
    "        self.next_layer = 0\n",
    "        self.past_layer = 1\n",
    "        self.input_syn = 0\n",
    "        self.output_syn = 0\n",
    "        \n",
    "        \n",
    "        self.spikes = 0\n",
    "        #self.firetime = -1\n",
    "        self.fired = False\n",
    "\n",
    "        self.set_outputs(layer)\n",
    "\n",
    "        if self.layer == 0:\n",
    "            self.threshold = 1.0\n",
    "        else: \n",
    "            self.threshold = 1.0  #2/3 * (self.past_layer)   #+ (self.current_layer + 1) # previous layer # of n /2 seem like a good fit  # will change to fun that reflect # of inputs and layer -- number of inputs\n",
    "        self.voltage = 0\n",
    "\n",
    "        #self.collected = 0\n",
    "        self.scheduled_spike = -10 # change to just runtime or somthng else\n",
    "        self.spike_time = 1000\n",
    "        \n",
    "\n",
    "    def set_outputs(self, layer):\n",
    "        '''Called with the creation of neuron create all the correct assocations for later here'''\n",
    "        global input_layer, output_layer, syn_layer_1, syn_layer_2, hidden_layer, syn_outputs, past_layer\n",
    "        if layer == 0:\n",
    "            self.next_layer = hidden_layer\n",
    "            #self.input_syn = 0 # remains nothing\n",
    "            self.output_syn = syn_layer_1\n",
    "        elif layer == 1:\n",
    "            self.current_layer = hidden_layer\n",
    "            self.next_layer = output_layer\n",
    "            self.past_layer = input_layer\n",
    "            self.input_syn = syn_layer_1\n",
    "            self.output_syn = syn_layer_2\n",
    "        elif layer == 2:\n",
    "            self.current_layer = output_layer\n",
    "            self.next_layer = output_layer # same amount of outputs\n",
    "            self.past_layer = hidden_layer\n",
    "            self.input_syn = syn_layer_2\n",
    "            self.output_syn = syn_outputs\n",
    "  \n",
    "\n",
    "    def send_to_syn(self, time, delay):\n",
    "        '''Sends out the times to the synapses of when they should arive at their next neuron'''\n",
    "        global l1_a, l2_a, l1_b, l2_b, m, c\n",
    "        self.fired = True\n",
    "        #print(self.layer, self.number, time, delay)\n",
    "        start = int(self.number * self.next_layer)\n",
    "        for syn in self.output_syn[start:(start+(self.next_layer))]:\n",
    "            #print('input: ', syn.input_neuron, \"output \", syn.output_neuron, \"on layer \", syn.layer)\n",
    "            a = 1\n",
    "            b = 0\n",
    "            # if self.layer == 1:\n",
    "            #     a = l1_a[syn.input_neuron][syn.output_neuron]\n",
    "            #     b = l1_b[syn.input_neuron][syn.output_neuron]\n",
    "            # if self.layer == 2:\n",
    "            #     a = l2_a[syn.input_neuron][syn.output_neuron]\n",
    "            #     b = l2_b[syn.input_neuron][syn.output_neuron]\n",
    "\n",
    "            syn.spike_time = ians_sigmoid(time, syn.a, syn.b, m, c, layer=syn.layer) + delay\n",
    "            #print(ians_sigmoid(time, 1, 0, m, c, layer=syn.layer) + delay)\n",
    "            #print(ians_sigmoid(time, 2, 0, m, c, layer=syn.layer) + delay)\n",
    "            #print(syn.spike_time, syn.layer)\n",
    "            #print(\"syn a and b\", a, b)\n",
    "            #print(\"syna and b\", syn.a, syn.b)\n",
    "            #print(\"syn spike time & delay \", syn.spike_time, delay)\n",
    "            #print(\"spike time send to syn \",syn.input_neuron,syn.output_neuron, \"at time \", syn.spike_time)\n",
    "            pass\n",
    "        # for all sync going out from this one: \n",
    "            # set call ian_sig and set that synapaces time to reach at calculated time\n",
    "\n",
    "\n",
    "    def check_arival(self):\n",
    "        '''Check first if this neuron has alreay sent or not. If it hasn't then it goes to check the synapses to see if one has arrived'''\n",
    "        if self.fired:\n",
    "            return\n",
    "        else:\n",
    "            self.check_syn_arival()\n",
    "\n",
    "\n",
    "    def check_syn_arival(self):\n",
    "        '''used in check_arival it will see if neruon is ready to act now basied on arriving spikes'''\n",
    "        global current_time\n",
    "        #print(self.layer)\n",
    "        if self.layer == 0: # only for input layer\n",
    "            if self.scheduled_spike <= current_time: # if fired already checked\n",
    "                self.voltage += 1\n",
    "                #print(\"input voltage set at time \", current_time)\n",
    "        else:\n",
    "\n",
    "            start = self.number * (self.current_layer -1)\n",
    "            for syn in self.input_syn[start: : self.current_layer]: # goes over all incoming synapaces\n",
    "                if (syn.message_complete) == False: # if syn is has not yet given this neuron its message\n",
    "                    #print(self.layer, syn.layer)\n",
    "                    #print(syn.check_spike(), current_time, syn.layer)\n",
    "                    if syn.check_spike() <= current_time: # will check if current step has been reached\n",
    "                        #print(syn.check_spike(), current_time)\n",
    "                        #print(self.layer, syn.layer)\n",
    "\n",
    "                        #print(syn.check_spike(), current_time, self.layer)\n",
    "                        # print(\"layer: \", self.layer)\n",
    "                        # print(\"recived spike at: \",current_time)\n",
    "                        syn.message_complete = True\n",
    "                        #update neruons voltage\n",
    "                        self.voltage += 1.2\n",
    "\n",
    "        self.check_threshold_voltage()\n",
    "        \n",
    "                    \n",
    "                    \n",
    "    def check_threshold_voltage(self):\n",
    "        global time_step\n",
    "        if self.voltage >= self.threshold:\n",
    "            #print(self.layer, self.voltage, self.threshold)\n",
    "            #print(\"self.voltage: \", self.voltage)\n",
    "            #print(\"threshold: \", self.threshold)\n",
    "            self.spike_time = current_time\n",
    "            #print(self.layer, self.spike_time)\n",
    "            #print(\"spike time: \", current_time)\n",
    "            self.send_to_syn(current_time, current_time)  # CHANGE IF LAYER NOT == 0?\n",
    "        # else:\n",
    "        #     if current_time >= (self.layer):\n",
    "        #         self.voltage += (time_step*self.past_layer) * 0.5 # will change such and whatnot just playing to start# \n",
    "\n",
    "\n",
    "\n",
    "class synapse:\n",
    "    def __init__(self, input_neuron, output_neuron, layer, a, b):\n",
    "        global run_time\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.input_neuron = input_neuron\n",
    "        self.output_neuron = output_neuron\n",
    "        self.layer = layer\n",
    "        self.spike_time = run_time\n",
    "        self.message_complete = False\n",
    "\n",
    "    def set_spiketime(self, time):\n",
    "        self.spike_time = time\n",
    "\n",
    "\n",
    "    def check_spike(self):\n",
    "        return self.spike_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f373296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "input_layer = 1\n",
    "hidden_layer = 1\n",
    "output_layer = 1 \n",
    "m = 5 # play with values and such\n",
    "c = 3\n",
    "\n",
    "inputs = [.3] #* input_layer # firing at 2 at lastest as of current if all ==2 or larger bc of the v+= x/2\n",
    "run_time = 10\n",
    "\n",
    "neurons_input = []\n",
    "neurons_hidden = []\n",
    "neurons_output = []\n",
    "\n",
    "syn_layer_1 = []\n",
    "syn_layer_2 = []\n",
    "syn_outputs = []\n",
    "\n",
    "current_time = 0\n",
    "time_step = 1\n",
    "\n",
    "def create_new_neurons_network():\n",
    "    count = 0\n",
    "    for i in range(len(inputs)):\n",
    "        n = Neuron(layer=0, number=i)  # will add later to add a and b and such\n",
    "        #n.set_firetime(val)\n",
    "        neurons_input.append(n)\n",
    "        n.scheduled_spike = inputs[i] \n",
    "        #print('Neuron Input neuron ' + str(neurons_input[i].number), \"scheduled time: \", n.scheduled_spike)\n",
    "\n",
    "    count = 0\n",
    "    for i in range (hidden_layer):\n",
    "        n = Neuron(layer=1, number=i)\n",
    "        neurons_hidden.append(n)\n",
    "        #print(\"Neuron at layer 1: \" + str(neurons_hidden[i].number))\n",
    "\n",
    "    count = 0\n",
    "    for i in range (output_layer):\n",
    "        n = Neuron(layer=2, number=i)\n",
    "        neurons_output.append(n)\n",
    "        #print(\"Neuron at output layer: \" + str(neurons_output[i].number))\n",
    "\n",
    "\n",
    "def create_synapses():\n",
    "    for input_n in neurons_input:\n",
    "        for hidden_n in neurons_hidden:\n",
    "            #print(input_n.number)\n",
    "            #print(hidden_n.number)\n",
    "            # create new object of synapics \n",
    "            #give the object to both neurons so one can output to and the other can edit it\n",
    "            syn = synapse(input_neuron=input_n.number, output_neuron=hidden_n.number, layer=0, a= 1 + random.uniform(-0.1, 0.1), b= random.uniform(-0.1, 0.1)) \n",
    "            syn_layer_1.append(syn)\n",
    "\n",
    "    for hidden_n in neurons_hidden:\n",
    "        for output_n in neurons_output:\n",
    "            #print(\"h\", hidden_n.number)\n",
    "            #print(\"0\" , output_n.number)\n",
    "            # create new object of synapics \n",
    "            #give the object to both neurons so one can output to and the other can edit it\n",
    "            syn = synapse(input_neuron=hidden_n.number, output_neuron=output_n.number, layer=1, a= 1 + random.uniform(-0.1, 0.1), b= random.uniform(-0.1, 0.1)) \n",
    "            syn_layer_2.append(syn)\n",
    "\n",
    "    \n",
    "    for count in range(output_layer): # will check layer \n",
    "            syn = synapse(input_neuron=count, output_neuron=count, layer=2, a=a, b=b) \n",
    "            syn_outputs.append(syn)\n",
    "\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def genesis():\n",
    "    create_new_neurons_network()\n",
    "    create_synapses()\n",
    "    return\n",
    "\n",
    "def run_simulation(runtime, dx):\n",
    "    global current_time, time_step\n",
    "    time_step = 1.0/dx\n",
    "    \n",
    "    current_time = 0\n",
    "\n",
    "    for i in range(runtime*dx):\n",
    "       \n",
    "        for n1 in neurons_input:\n",
    "            n1.check_arival()\n",
    "            pass \n",
    "\n",
    "        for n2 in neurons_hidden:\n",
    "            n2.check_arival()\n",
    "            pass \n",
    "\n",
    "        for n3 in neurons_output:\n",
    "            n3.check_arival()\n",
    "            pass\n",
    "\n",
    "        #print(\"TIME\", current_time)\n",
    "        current_time += time_step\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "run_simulation(run_time, 1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 and a2:  0.824212688843635 0.338115663257998\n",
      "b1 and b2:  0.41817342509562705 0.40375394879910154\n",
      "3.492\n",
      "a1 and a2:  0.8241893520194208 0.33777903395119996\n",
      "b1 and b2:  0.4181739491804682 0.4038616696\n",
      "0.466\n",
      "a1 and a2:  0.8241693727822073 0.3374390339512\n",
      "b1 and b2:  0.4181739502564059 0.40397166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8241493935449938 0.3370990339512\n",
      "b1 and b2:  0.4181739513323436 0.40408166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8241294143077805 0.3367590339512\n",
      "b1 and b2:  0.4181739524082813 0.40419166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8241094350705671 0.3364190339512\n",
      "b1 and b2:  0.41817395348421904 0.40430166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8240894558333536 0.3360790339512\n",
      "b1 and b2:  0.4181739545601567 0.40441166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8240694765961402 0.33573903395119997\n",
      "b1 and b2:  0.41817395563609444 0.40452166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8240494973589267 0.33539903395119997\n",
      "b1 and b2:  0.41817395671203217 0.40463166959999997\n",
      "0.466\n",
      "a1 and a2:  0.8240289215598248 0.3350491028625\n",
      "b1 and b2:  0.4181741896810805 0.4047482125\n",
      "0.465\n",
      "a1 and a2:  0.8240089435558119 0.33469910286250004\n",
      "b1 and b2:  0.41817419092360264 0.4048682125\n",
      "0.465\n",
      "a1 and a2:  0.8239889655517989 0.3343491028625\n",
      "b1 and b2:  0.4181741921661248 0.4049882125\n",
      "0.465\n",
      "a1 and a2:  0.8239689875477859 0.3339991028625\n",
      "b1 and b2:  0.4181741934086469 0.4051082125\n",
      "0.465\n",
      "a1 and a2:  0.823949009543773 0.33364910286250005\n",
      "b1 and b2:  0.41817419465116906 0.4052282125\n",
      "0.465\n",
      "a1 and a2:  0.82392903153976 0.33329910286250003\n",
      "b1 and b2:  0.41817419589369115 0.40534821249999997\n",
      "0.465\n",
      "a1 and a2:  0.8239090535357472 0.3329491028625\n",
      "b1 and b2:  0.4181741971362133 0.4054682125\n",
      "0.465\n",
      "a1 and a2:  0.8238890755317343 0.33259910286250005\n",
      "b1 and b2:  0.41817419837873543 0.4055882125\n",
      "0.465\n",
      "a1 and a2:  0.8238690975277213 0.33224910286250003\n",
      "b1 and b2:  0.41817419962125757 0.4057082125\n",
      "0.465\n",
      "a1 and a2:  0.8238491195237083 0.3318991028625\n",
      "b1 and b2:  0.4181742008637797 0.4058282125\n",
      "0.465\n",
      "a1 and a2:  0.8238285516861783 0.33153918023679996\n",
      "b1 and b2:  0.4181744410500544 0.4059549344\n",
      "0.464\n",
      "a1 and a2:  0.8238085749503934 0.3311791802368\n",
      "b1 and b2:  0.41817444236322987 0.4060749344\n",
      "0.464\n",
      "a1 and a2:  0.8237885982146085 0.33081918023679996\n",
      "b1 and b2:  0.4181744436764054 0.4061949344\n",
      "0.464\n",
      "a1 and a2:  0.8237686214788236 0.3304591802368\n",
      "b1 and b2:  0.41817444498958084 0.4063149344\n",
      "0.464\n",
      "a1 and a2:  0.8237486447430385 0.33009918023679996\n",
      "b1 and b2:  0.41817444630275635 0.4064349344\n",
      "0.464\n",
      "a1 and a2:  0.8237286680072536 0.3297391802368\n",
      "b1 and b2:  0.4181744476159318 0.4065549344\n",
      "0.464\n",
      "a1 and a2:  0.8237086912714686 0.32937918023679996\n",
      "b1 and b2:  0.41817444892910727 0.4066749344\n",
      "0.464\n",
      "a1 and a2:  0.8236887145356838 0.3290191802368\n",
      "b1 and b2:  0.4181744502422828 0.4067949344\n",
      "0.464\n",
      "a1 and a2:  0.8236687377998988 0.32865918023679996\n",
      "b1 and b2:  0.41817445155545824 0.4069149344\n",
      "0.464\n",
      "a1 and a2:  0.8236481775009284 0.3282892663159\n",
      "b1 and b2:  0.4181746988067375 0.4070418347\n",
      "0.463\n",
      "a1 and a2:  0.8236282020683374 0.3279192663159\n",
      "b1 and b2:  0.41817470030791687 0.4071718347\n",
      "0.463\n",
      "a1 and a2:  0.8236082266357465 0.3275492663159\n",
      "b1 and b2:  0.41817470180909616 0.4073018347\n",
      "0.463\n",
      "a1 and a2:  0.8235882512031555 0.3271792663159\n",
      "b1 and b2:  0.4181747033102755 0.4074318347\n",
      "0.463\n",
      "a1 and a2:  0.8235682757705647 0.3268092663159\n",
      "b1 and b2:  0.4181747048114548 0.4075618347\n",
      "0.463\n",
      "a1 and a2:  0.8235483003379737 0.3264392663159\n",
      "b1 and b2:  0.4181747063126341 0.4076918347\n",
      "0.463\n",
      "a1 and a2:  0.8235283249053826 0.3260692663159\n",
      "b1 and b2:  0.41817470781381344 0.4078218347\n",
      "0.463\n",
      "a1 and a2:  0.8235083494727917 0.3256992663159\n",
      "b1 and b2:  0.41817470931499273 0.4079518347\n",
      "0.463\n",
      "a1 and a2:  0.8234883740402007 0.3253292663159\n",
      "b1 and b2:  0.4181747108161721 0.4080818347\n",
      "0.463\n",
      "a1 and a2:  0.8234678214784046 0.3249493613416\n",
      "b1 and b2:  0.4181749653179153 0.4082189128\n",
      "0.462\n",
      "a1 and a2:  0.8234478473839104 0.3245693613416\n",
      "b1 and b2:  0.41817496702136686 0.40835891280000003\n",
      "0.462\n",
      "a1 and a2:  0.8234278732894162 0.32418936134160004\n",
      "b1 and b2:  0.4181749687248184 0.4084989128\n",
      "0.462\n",
      "a1 and a2:  0.823407899194922 0.3238093613416\n",
      "b1 and b2:  0.41817497042826995 0.4086389128\n",
      "0.462\n",
      "a1 and a2:  0.8233879251004278 0.3234293613416\n",
      "b1 and b2:  0.4181749721317215 0.4087789128\n",
      "0.462\n",
      "a1 and a2:  0.8233679510059336 0.3230493613416\n",
      "b1 and b2:  0.41817497383517305 0.4089189128\n",
      "0.462\n",
      "a1 and a2:  0.8233479769114395 0.3226693613416\n",
      "b1 and b2:  0.4181749755386246 0.4090589128\n",
      "0.462\n",
      "a1 and a2:  0.8233280028169453 0.3222893613416\n",
      "b1 and b2:  0.41817497724207614 0.4091989128\n",
      "0.462\n",
      "a1 and a2:  0.823308028722451 0.32190936134160003\n",
      "b1 and b2:  0.4181749789455277 0.4093389128\n",
      "0.462\n",
      "a1 and a2:  0.823287484095515 0.3215194655557\n",
      "b1 and b2:  0.41817524078413265 0.4094861681\n",
      "0.461\n",
      "a1 and a2:  0.8232675113739554 0.3211294655557\n",
      "b1 and b2:  0.4181852427045842 0.40963616810000003\n",
      "0.461\n",
      "a1 and a2:  0.8232475386523957 0.32073946555570004\n",
      "b1 and b2:  0.4181952446250357 0.4097861681\n",
      "0.461\n",
      "a1 and a2:  0.8232275659308362 0.32034946555570004\n",
      "b1 and b2:  0.41820524654548724 0.4099361681\n",
      "0.461\n",
      "a1 and a2:  0.8232075932092766 0.31995946555570004\n",
      "b1 and b2:  0.4182152484659387 0.41008616810000004\n",
      "0.461\n",
      "a1 and a2:  0.823187620487717 0.31956946555570004\n",
      "b1 and b2:  0.4182252503863902 0.4102361681\n",
      "0.461\n",
      "a1 and a2:  0.8231676477661574 0.31917946555570004\n",
      "b1 and b2:  0.4182352523068417 0.4103861681\n",
      "0.461\n",
      "a1 and a2:  0.8231476750445978 0.31878946555570004\n",
      "b1 and b2:  0.41824525422729325 0.4105361681\n",
      "0.461\n",
      "a1 and a2:  0.8231277023230383 0.31839946555570003\n",
      "b1 and b2:  0.4182552561477447 0.4106861681\n",
      "0.461\n",
      "a1 and a2:  0.8231071658276948 0.3179995792\n",
      "b1 and b2:  0.4182655254140606 0.4108436\n",
      "0.46\n",
      "a1 and a2:  0.8230871945138409 0.3175995792\n",
      "b1 and b2:  0.4182755274321572 0.41099359999999996\n",
      "0.46\n",
      "a1 and a2:  0.823067223199987 0.31719957919999997\n",
      "b1 and b2:  0.4182855294502538 0.4111436\n",
      "0.46\n",
      "a1 and a2:  0.8230472518861331 0.31679957919999996\n",
      "b1 and b2:  0.41829553146835047 0.4112936\n",
      "0.46\n",
      "a1 and a2:  0.8230272805722793 0.3163995792\n",
      "b1 and b2:  0.4183055334864471 0.41144359999999996\n",
      "0.46\n",
      "a1 and a2:  0.8230073092584255 0.3159995792\n",
      "b1 and b2:  0.41831553550454376 0.41159359999999995\n",
      "0.46\n",
      "a1 and a2:  0.8229873379445716 0.3155995792\n",
      "b1 and b2:  0.4183255375226404 0.4117436\n",
      "0.46\n",
      "a1 and a2:  0.8229673666307177 0.31519957919999997\n",
      "b1 and b2:  0.418335539540737 0.41189359999999997\n",
      "0.46\n",
      "a1 and a2:  0.8229473953168637 0.31479957919999996\n",
      "b1 and b2:  0.41834554155883363 0.41204359999999995\n",
      "0.46\n",
      "a1 and a2:  0.8229268671488666 0.31438970251630005\n",
      "b1 and b2:  0.41835581815361605 0.41220120790000003\n",
      "0.459\n",
      "a1 and a2:  0.8229068972774213 0.31397970251630003\n",
      "b1 and b2:  0.41836582041287385 0.4123612079\n",
      "0.459\n",
      "a1 and a2:  0.8228869274059758 0.3135697025163\n",
      "b1 and b2:  0.41837582267213164 0.4125212079\n",
      "0.459\n",
      "a1 and a2:  0.8228669575345303 0.31315970251630004\n",
      "b1 and b2:  0.41838582493138937 0.4126812079\n",
      "0.459\n",
      "a1 and a2:  0.8228469876630848 0.3127497025163\n",
      "b1 and b2:  0.41839582719064716 0.4128412079\n",
      "0.459\n",
      "a1 and a2:  0.8228270177916392 0.3123397025163\n",
      "b1 and b2:  0.41840582944990495 0.4130012079\n",
      "0.459\n",
      "a1 and a2:  0.8228070479201938 0.31192970251630003\n",
      "b1 and b2:  0.41841583170916274 0.4131612079\n",
      "0.459\n",
      "a1 and a2:  0.8227870780487484 0.3115197025163\n",
      "b1 and b2:  0.41842583396842054 0.41332120790000004\n",
      "0.459\n",
      "a1 and a2:  0.8227665576783209 0.31109983574640004\n",
      "b1 and b2:  0.4184361180073609 0.41348899120000004\n",
      "0.458\n",
      "a1 and a2:  0.8227465892839163 0.3106798357464\n",
      "b1 and b2:  0.41844612052371927 0.41365899120000005\n",
      "0.458\n",
      "a1 and a2:  0.8227266208895118 0.31025983574640004\n",
      "b1 and b2:  0.4184561230400777 0.41382899120000005\n",
      "0.458\n",
      "a1 and a2:  0.8227066524951072 0.3098398357464\n",
      "b1 and b2:  0.4184661255564361 0.4139989912\n",
      "0.458\n",
      "a1 and a2:  0.8226866841007028 0.30941983574640003\n",
      "b1 and b2:  0.4184761280727945 0.4141689912\n",
      "0.458\n",
      "a1 and a2:  0.8226667157062982 0.3089998357464\n",
      "b1 and b2:  0.4184861305891529 0.4143389912\n",
      "0.458\n",
      "a1 and a2:  0.8226467473118937 0.3085798357464\n",
      "b1 and b2:  0.4184961331055112 0.4145089912\n",
      "0.458\n",
      "a1 and a2:  0.8226267789174891 0.30815983574640005\n",
      "b1 and b2:  0.4185061356218696 0.4146789912\n",
      "0.458\n",
      "a1 and a2:  0.8226068105230846 0.3077398357464\n",
      "b1 and b2:  0.418516138138228 0.4148489912\n",
      "0.458\n",
      "a1 and a2:  0.8225862988692199 0.3073099791321\n",
      "b1 and b2:  0.4185264298290716 0.4150269493\n",
      "0.457\n",
      "a1 and a2:  0.822566331986417 0.3068799791321\n",
      "b1 and b2:  0.41853643261892154 0.4152069493\n",
      "0.457\n",
      "a1 and a2:  0.8225463651036141 0.3064499791321\n",
      "b1 and b2:  0.41854643540877146 0.41538694930000003\n",
      "0.457\n",
      "a1 and a2:  0.8225263982208112 0.3060199791321\n",
      "b1 and b2:  0.4185564381986213 0.4155669493\n",
      "0.457\n",
      "a1 and a2:  0.8225064313380083 0.3055899791321\n",
      "b1 and b2:  0.41856644098847123 0.4157469493\n",
      "0.457\n",
      "a1 and a2:  0.8224864644552055 0.30515997913209997\n",
      "b1 and b2:  0.41857644377832115 0.4159269493\n",
      "0.457\n",
      "a1 and a2:  0.8224664975724026 0.3047299791321\n",
      "b1 and b2:  0.41858644656817107 0.41610694930000003\n",
      "0.457\n",
      "a1 and a2:  0.8224465306895997 0.3042999791321\n",
      "b1 and b2:  0.418596449358021 0.4162869493\n",
      "0.457\n",
      "a1 and a2:  0.8224260271866957 0.30386013291520003\n",
      "b1 and b2:  0.418606748678019 0.41647508159999996\n",
      "0.456\n",
      "a1 and a2:  0.8224060618499814 0.30342013291520004\n",
      "b1 and b2:  0.4186167517582006 0.4166650816\n",
      "0.456\n",
      "a1 and a2:  0.8223860965132671 0.30298013291520004\n",
      "b1 and b2:  0.41862675483838224 0.41685508159999995\n",
      "0.456\n",
      "a1 and a2:  0.8223661311765529 0.30254013291520004\n",
      "b1 and b2:  0.4186367579185639 0.4170450816\n",
      "0.456\n",
      "a1 and a2:  0.8223461658398388 0.3021001329152\n",
      "b1 and b2:  0.4186467609987456 0.41723508159999995\n",
      "0.456\n",
      "a1 and a2:  0.8223262005031244 0.3016601329152\n",
      "b1 and b2:  0.41865676407892727 0.41742508159999997\n",
      "0.456\n",
      "a1 and a2:  0.8223062351664102 0.3012201329152\n",
      "b1 and b2:  0.4186667671591089 0.4176150816\n",
      "0.456\n",
      "a1 and a2:  0.8222862698296959 0.3007801329152\n",
      "b1 and b2:  0.41867677023929056 0.41780508159999996\n",
      "0.456\n",
      "a1 and a2:  0.8222657746533812 0.3003302973375\n",
      "b1 and b2:  0.4186870772854754 0.4180033875\n",
      "0.455\n",
      "a1 and a2:  0.8222458108971673 0.2998802973375\n",
      "b1 and b2:  0.4186970805038862 0.41819338749999996\n",
      "0.455\n",
      "a1 and a2:  0.8222258471409535 0.2994302973375\n",
      "b1 and b2:  0.4187070837222971 0.4183833875\n",
      "0.455\n",
      "a1 and a2:  0.8222258471409535 0.2994302973375\n",
      "b1 and b2:  0.4187070837222971 0.4183833875\n"
     ]
    }
   ],
   "source": [
    "def run_new_111(desired, lr):   # will need to check work on bias/b and make sure everything is correct in this\n",
    "\n",
    "    #desired = desired -.032\n",
    "    run_simulation(run_time, 1000)\n",
    "    \n",
    "    n1 = neurons_hidden[0].spike_time\n",
    "    n2 = neurons_output[0].spike_time\n",
    "    #print(\"n1, n2 : \", n1, n2)\n",
    "    #loss = calc_loss(output, desired)\n",
    "\n",
    "    delta_a2 = - d_loss_d_weight_2(n1, n2, desired)\n",
    "\n",
    "    syn_a1 = syn_layer_1[0].a\n",
    "    syn_a2 = syn_layer_2[0].a\n",
    "\n",
    "    delta_a1 = - d_loss_d_weight_1(n1, n2, desired, syn_a2, inputs[0]) # trying to just input a as a the weight directly\n",
    "\n",
    "    new_a1 = update_param(lr, delta_a1, syn_a1)\n",
    "    new_a2 = update_param(lr, delta_a2, syn_a2)\n",
    "\n",
    "    syn_b1 = syn_layer_1[0].b\n",
    "    syn_b2 = syn_layer_2[0].b\n",
    "\n",
    "    delta_b2 = d_loss_d_bias_2(n2, desired) # b is not - but left as + here\n",
    "    delta_b1 = d_loss_d_bias_1(n1, n2, delta_b2, desired, syn_b2) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "    new_b1 = update_param(lr, delta_b1, syn_b1)\n",
    "    new_b2 = update_param(lr, delta_b2, syn_b2)\n",
    "\n",
    "    return new_a1, new_a2, new_b1, new_b2, n1, n2\n",
    "\n",
    "\n",
    "#run_new_111(input, 1.5, 0.1)\n",
    "\n",
    "\n",
    "def reset_network():\n",
    "    global current_time\n",
    "    current_time = 0\n",
    "    for n in neurons_input: \n",
    "        n.voltage = 0\n",
    "        n.fired = False\n",
    "        n.spike_time = run_time\n",
    "    for n in neurons_hidden:\n",
    "        n.voltage = 0\n",
    "        n.fired = False\n",
    "        n.spike_time = run_time\n",
    "    for n in neurons_output:\n",
    "        n.voltage = 0\n",
    "        n.fired = False\n",
    "        n.spike_time = run_time\n",
    "    for s in syn_layer_1:\n",
    "        s.spike_time = run_time\n",
    "        s.message_complete = False\n",
    "    for s in syn_layer_2:\n",
    "        s.spike_time = run_time\n",
    "        s.message_complete = False\n",
    "\n",
    "\n",
    "genesis()\n",
    "\n",
    "for i in range(100):\n",
    "   \n",
    "    reset_network() # needs to add reset for all that isn't the genesis call\n",
    "   \n",
    "    sa1, sa2, sb1, sb2, n1, n2 = run_new_111(desired=0.5, lr=.1) # basic range should work just need to find the range and such but can do later then \n",
    "                                # desired has to be between 1.532 and 3.335\n",
    "                                # also need to fully reset when values overeach to get that 1.532 and 3.335\n",
    "                                # limited ranged based on limit else it crashes and resets to at near start spike time\n",
    "                                # need roundeding else it falls off for speed\n",
    "    # l1_a[0][0] = sa1\n",
    "    # l2_a[0][0] = sa2\n",
    "    # l1_b[0][0] = sb1\n",
    "    # l2_b[0][0] = sb2\n",
    "    syn_layer_1[0].a = np.round(sa1, 5)\n",
    "    syn_layer_2[0].a = np.round(sa2, 5)\n",
    "    syn_layer_1[0].b = np.round(sb1, 5)\n",
    "    syn_layer_2[0].b = np.round(sb2, 5)\n",
    "\n",
    "    print(\"a1 and a2: \", sa1, sa2)\n",
    "    print(\"b1 and b2: \", sb1, sb2)\n",
    "    print(np.round(n2, 3))\n",
    "\n",
    "print(\"a1 and a2: \", sa1, sa2)\n",
    "print(\"b1 and b2: \", sb1, sb2)\n",
    "\n",
    "# print(l1_a)\n",
    "# print(l1_b)\n",
    "# print(l2_a)\n",
    "# print(l2_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291febc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_synapses()      # create synapses objects once\n",
    "# for epoch in range(10):\n",
    "#    # clear out last run’s flags & times\n",
    "#     sa1, sa2, sb1, sb2, n1, n2 = run_new_111(1.5, 0.1)\n",
    "#     # write back into synapse objects:\n",
    "#     syn_layer_1[0].a = sa1\n",
    "#     syn_layer_2[0].a = sa2\n",
    "#     syn_layer_1[0].b = sb1\n",
    "#     syn_layer_2[0].b = sb2\n",
    "#     print(f\"Epoch {epoch}: a1={sa1:.4f}, a2={sa2:.4f}, n2={n2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef1b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.48242552380635634 0.960471553313154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s1 = ians_sigmoid(inputs[0], 1, 0, m, c, layer=0)  + inputs[0]\n",
    "\n",
    "s2 = ians_sigmoid(s1, 1, 0, m, c, layer=1) + s1\n",
    "\n",
    "\n",
    "print(inputs[0], s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c1d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_spike_raster(inputs,\n",
    "#                       syn_layer_1,\n",
    "#                       syn_layer_2,\n",
    "#                       syn_outputs):\n",
    "#     \"\"\"\n",
    "#     inputs: list of input‐neuron spike times\n",
    "#     syn_layer_1: list of synapse objects for layer 1 (must have .spike_time)\n",
    "#     syn_layer_2: list of synapse objects for layer 2\n",
    "#     syn_outputs: list of synapse objects for output layer\n",
    "#     \"\"\"\n",
    "#     # collect times & y‐positions\n",
    "#     times = []\n",
    "#     rows  = []\n",
    "#     labels = []\n",
    "\n",
    "#     # row 0 = input spikes\n",
    "#     for t in inputs:\n",
    "#         times.append(t)\n",
    "#         rows.append(0)\n",
    "#         labels.append(\"input\")\n",
    "\n",
    "#     # row 1 = layer‐1 synapses\n",
    "#     for idx, s in enumerate(syn_layer_1):\n",
    "#         times.append(s.spike_time)\n",
    "#         rows.append(1)\n",
    "#         labels.append(f\"syn1_{idx}\")\n",
    "\n",
    "#     # row 2 = layer‐2 synapses\n",
    "#     for idx, s in enumerate(syn_layer_2):\n",
    "#         times.append(s.spike_time)\n",
    "#         rows.append(2)\n",
    "#         labels.append(f\"syn2_{idx}\")\n",
    "\n",
    "#     # row 3 = output synapses\n",
    "#     for idx, s in enumerate(syn_outputs):\n",
    "#         times.append(s.spike_time)\n",
    "#         rows.append(3)\n",
    "#         labels.append(f\"out_{idx}\")\n",
    "\n",
    "#     # now plot\n",
    "#     plt.figure(figsize=(8, 3))\n",
    "#     plt.scatter(times, rows, marker='|', s=200)\n",
    "#     plt.yticks([0,1,2,3], ['input','syn1','syn2','output'])\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.title(\"Spike / Synapse firing times\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # (assuming you kept your syn_layer_1, syn_layer_2, syn_outputs in scope)\n",
    "# final_time = run_simulation(inputs, run_time, 10000)\n",
    "# print(\"final spike at\", final_time)\n",
    "\n",
    "# plot_spike_raster(\n",
    "#     inputs,\n",
    "#     syn_layer_1,\n",
    "#     syn_layer_2,\n",
    "#     syn_outputs\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7860c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9581585445634135 1.7005506779221804\n",
      "1.701\n",
      "0.9572940972071813 1.568462565138125\n",
      "1.568\n",
      "0.9612092221388677 1.4999299780466302\n",
      "1.5\n",
      "0.9668643242246178 1.4553180458752166\n",
      "1.455\n",
      "0.9735044787469079 1.4234802143431717\n",
      "1.423\n",
      "0.9808520885916617 1.3996103108945461\n",
      "1.4\n",
      "0.9887954521650565 1.3812032106314938\n",
      "1.381\n",
      "0.9972973107365857 1.3667995740955883\n",
      "1.367\n",
      "1.0063608384336407 1.3554860363176706\n",
      "1.355\n",
      "1.0160154345479842 1.3466635982895394\n",
      "1.347\n",
      "1.0263107560615845 1.339928680730503\n",
      "1.34\n",
      "1.0373147939035106 1.3350075015030896\n",
      "1.335\n",
      "1.0491143335741757 1.3317181537772353\n",
      "1.332\n",
      "1.061817168263852 1.3299482908743696\n",
      "1.33\n",
      "1.0755559289413137 1.3296423586871846\n",
      "1.33\n",
      "1.0904937006406836 1.3307952722392853\n",
      "1.331\n",
      "1.106831845552876 1.3334510329651874\n",
      "1.333\n",
      "1.1248207193243456 1.3377057722350991\n",
      "1.338\n",
      "1.1447742847273208 1.3437154403788643\n",
      "1.344\n",
      "1.167090004065624 1.3517090242662282\n",
      "1.352\n",
      "1.1922757602896628 1.3620088504308705\n",
      "1.362\n",
      "1.2209856211563157 1.375060137980982\n",
      "1.375\n",
      "1.2540650502459467 1.3914720020523004\n",
      "1.391\n",
      "1.2926007619633415 1.4120697879187432\n",
      "1.412\n",
      "1.3379524222544368 1.4379484300721441\n",
      "1.438\n",
      "1.3916900406303612 1.470483120885014\n",
      "1.47\n",
      "1.4552182608485729 1.511161517538911\n",
      "1.511\n",
      "1.5285590024423334 1.5609132750614845\n",
      "1.561\n",
      "1.6074651693548239 1.618549632642498\n",
      "1.619\n",
      "1.679413314815791 1.6794482004173776\n",
      "1.679\n",
      "1.72485221170675 1.7403477750579939\n",
      "1.74\n",
      "1.7292782713939001 1.813384235465136\n",
      "1.813\n",
      "1.6874548811592738 1.939287814096226\n",
      "1.939\n",
      "1.5836391943675439 2.2541968594814694\n",
      "2.254\n",
      "1.3324502185763472 18.441433022314918\n",
      "18.441\n",
      "27689.883803231045 588457.9311052058\n",
      "588457.931\n",
      "3.4218896759783576e+55 3.421889675978389e+55\n",
      "3.421889675978389e+55\n",
      "inf inf\n",
      "inf\n",
      "-inf nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "nan nan\n",
      "nan\n",
      "a1 and a2:  nan nan\n",
      "b1 and b2:  nan nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_29944\\4182022960.py:16: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
      "C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_29944\\4182022960.py:16: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
      "C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_29944\\4182022960.py:16: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    " \n",
    "input = 0.5  # all inputs have to be larger then 0.5?\n",
    "lr = 0.1\n",
    "a1 = 1.1\n",
    "a2 = 0.9\n",
    "b1 = -0.1\n",
    "b2 = 0.0\n",
    "desired = 1.0\n",
    "# weird thing with moving the lr too much need to keep low nery steep start with changing lower m maybe?\n",
    "for i in range(1000):\n",
    "    a1, a2, b1, b2, r = run_basic(input, a1, a2, b1, b2, desired, lr)\n",
    "    # print(\"a1 and a2: \", a1, a2)\n",
    "    # print(\"b1 and b2: \", b1, b2)\n",
    "    print(np.round(r[0][1], 3))\n",
    "print(\"a1 and a2: \", a1, a2)\n",
    "print(\"b1 and b2: \", b1, b2)\n",
    "print(np.round(r[0][1], 3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2449905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Brian Simulation: \n",
    "# from brian2 import *\n",
    "# import numpy as np\n",
    "# import logging\n",
    "# import warnings\n",
    "# from brian2 import prefs, set_device\n",
    "# # Tell Brian2 to use the Cython code generator:\n",
    "# prefs.codegen.target = 'cython'\n",
    "\n",
    "# # Optionally compile but keep Python interface:\n",
    "# set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "# # suppress overflow warnings\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "# numpy.seterr(over='ignore', under='ignore')\n",
    "# logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Spike timing and derivative\n",
    "# layer_bias = 0.5\n",
    "# start_scope()\n",
    "# defaultclock.dt = 0.001*ms\n",
    "\n",
    "# @implementation('numpy', discard_units=True)\n",
    "# @check_units(time=1, a=1, b=1, m=1, c=1, layer=1, result=1)\n",
    "# def ians_sigmoid(time, a, b, m, c, layer):\n",
    "#     print(time)\n",
    "#     global layer_bias\n",
    "#     print(\"result of ians_sig: \", 1/ ((1/a) + np.exp(((-time*m) + b + c + layer * layer_bias))))\n",
    "#     return 1/ ((1/a) + np.exp(((-time*m) + b + c + layer * layer_bias)))\n",
    "\n",
    "\n",
    "\n",
    "# def mini1x1x1(inputs):\n",
    "#     \"\"\"\n",
    "#     Mini 1x1x1 network with a single input, hidden, and output neuron.\n",
    "#     This is a minimal example to demonstrate the basic structure of a spiking neural network.\n",
    "#     \"\"\"\n",
    "#     # Define network sizes\n",
    "#     n_input = 1\n",
    "#     n_hidden = 1\n",
    "#     n_output = 1\n",
    "#     n_total = n_input + n_hidden + n_output\n",
    "\n",
    "#     # Full neuron group\n",
    "#     neurons = NeuronGroup(n_total, '''\n",
    "#         v : 1\n",
    "#         sum : 1\n",
    "#         spikes_received : 1\n",
    "#         scheduled_time : second\n",
    "#         global_clock : 1\n",
    "#         spiked : boolean\n",
    "#     ''', threshold='v > 0.5', reset='''\n",
    "#     v = 0\n",
    "#     ''', method='exact')\n",
    "#     neurons.v = 0\n",
    "#     neurons.scheduled_time = 1e9 * second\n",
    "#     neurons.global_clock = 0.0\n",
    "#     neurons.sum = 0.0\n",
    "#     neurons.spikes_received = 0.0\n",
    "\n",
    "#     # Spike inputs (one per input neuron)     \n",
    "#     stim = SpikeGeneratorGroup(n_input, indices=range(n_input), times=(inputs) * ms)\n",
    "\n",
    "#     # Input → Hidden connections\n",
    "#     syn_input = Synapses(stim, neurons[0:n_input], '''\n",
    "#         layer : 1\n",
    "#         a : 1\n",
    "#         b : 1\n",
    "#     ''', on_pre='''\n",
    "#         spikes_received += 1\n",
    "#         scheduled_time = (ians_sigmoid(t/ms, a, b, 3, 5, 0) + t/ms) * ms \n",
    "#     ''') \n",
    "#     syn_input.connect(j='i')  # connect stim[i] to neurons[i]\n",
    "#     syn_input.layer = 0\n",
    "#     syn_input.a = 5.0\n",
    "#     syn_input.b = 0.0\n",
    "\n",
    "\n",
    "#     # Hidden layer: input → hidden\n",
    "#     syn_hidden = Synapses(neurons[0:n_input], neurons[n_input:n_input + n_hidden], '''\n",
    "#         w : 1\n",
    "#         layer : 1\n",
    "#     ''', on_pre='''\n",
    "#         spikes_received += 1\n",
    "#         scheduled_time = (ians_sigmoid(t/ms, a, b, 3, 5, 0) + t/ms) * ms \n",
    "#     ''')\n",
    "#     syn_hidden.connect()\n",
    "#     #syn_hidden.w = w1 \n",
    "#     syn_hidden.layer = 1\n",
    "\n",
    "#     # Output layer: hidden → output\n",
    "#     # syn_output = Synapses(neurons[n_input:n_input + n_hidden], neurons[n_input + n_hidden:n_total], '''\n",
    "#     #     w : 1\n",
    "#     #     layer : 1\n",
    "#     # ''', on_pre='''\n",
    "#     #     spikes_received += 1\n",
    "#     #     v += 1\n",
    "    \n",
    "#     # ''')\n",
    "#     # syn_output.connect()\n",
    "#     # #syn_output.w = w2 \n",
    "#     # syn_output.layer = 2\n",
    "\n",
    "#     window = 0.02*ms\n",
    "\n",
    "#     neurons.run_regularly('''\n",
    "                          \n",
    "#         v = int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "#         in_window = (scheduled_time >= (t - 0.02*ms)) * (scheduled_time <= (t + 0.02*ms))\n",
    "   \n",
    "#         fire_flag = int(in_window * (1 - spiked))\n",
    "        \n",
    "#         v      = fire_flag * 2\n",
    "#         spiked = spiked or (fire_flag > 0)\n",
    "  \n",
    "#     ''', dt=0.001*ms)\n",
    "\n",
    "#         #v = int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "\n",
    "#     # Monitors\n",
    "#     mon = StateMonitor(neurons, 'v', record=True, dt=0.0001*ms)\n",
    "#     mon_sum = StateMonitor(neurons, 'sum', record=True)\n",
    "#     sp_mon = StateMonitor(neurons, 'spikes_received', record=True)\n",
    "#     sch_time = StateMonitor(neurons, 'scheduled_time', record=True)\n",
    "\n",
    "\n",
    "#     spikemon = SpikeMonitor(neurons)\n",
    "\n",
    "\n",
    "#     run(5*ms)\n",
    "\n",
    "#     # Plot voltages\n",
    "#     figure(figsize=(10, 6))\n",
    "#     for i in range(n_total):  # All neurons\n",
    "#         plot(mon.t/ms, mon.v[i], label=f'Neuron {i}')\n",
    "#     xlabel('Time (ms)')\n",
    "#     ylabel('Membrane potential')\n",
    "#     legend()\n",
    "#     title('SNN Spike Propagation Across All Layers')\n",
    "#     show()\n",
    "\n",
    "#     # plot(mon_sum.t/ms, mon_sum.sum[3])  # or any neuron index\n",
    "#     # print(mon_sum.sum[1])\n",
    "#     # print(sp_mon.spikes_received[1])\n",
    "#     # print(sch_time.scheduled_time[1])\n",
    "\n",
    "#     for i in range(n_total):\n",
    "#         times = spikemon.spike_trains()[i]\n",
    "#         if len(times) > 0:\n",
    "#             formatted_times = [f\"{t/ms:.3f} ms\" for t in times]\n",
    "#             print(f\"Neuron {i} spike times: {formatted_times}\")\n",
    "\n",
    "# input = [0.5]\n",
    "\n",
    "# mini1x1x1(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
