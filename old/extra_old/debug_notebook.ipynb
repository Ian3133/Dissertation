{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1f44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n",
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.599, 0.623, 1.697]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "# Your custom timing function (unchanged)\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "def mini_urd(inputs, weights_1):\n",
    "    n_input  = 2\n",
    "    n_hidden = 1\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    neurons = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v > 1',\n",
    "        reset='v = 0',\n",
    "        method='exact'\n",
    "    )\n",
    "    neurons.v = 0\n",
    "    neurons.sum = 0\n",
    "    neurons.spikes_received = 0\n",
    "    neurons.global_clock = 0\n",
    "    neurons.scheduled_time = 1e9*second\n",
    "\n",
    "    # 2‐neuron stimulus\n",
    "    stim = SpikeGeneratorGroup(2,\n",
    "        indices=[0,1],\n",
    "        times=inputs*ms\n",
    "    )\n",
    "\n",
    "    # stim → “input” neurons\n",
    "    syn_input = Synapses(\n",
    "        stim, neurons[0:n_input],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "spikes_received += 1\n",
    "sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "\n",
    "scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "'''\n",
    "    )\n",
    "    syn_input.connect(j='i')\n",
    "    # syn_input.w = weights_1\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    # “input” → hidden\n",
    "    syn_hidden = Synapses(\n",
    "        neurons[0:n_input], neurons[n_input:],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "spikes_received += 1\n",
    "sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "'''\n",
    "    )\n",
    "    syn_hidden.connect()\n",
    "    syn_hidden.w = weights_1\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "    # fire at scheduled_time\n",
    "    neurons.run_regularly('''\n",
    "v = int(abs(t - scheduled_time) < 0.0005*ms)*1.2\n",
    "global_clock += 0.001\n",
    "''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(neurons)\n",
    "    run(5*ms)\n",
    "\n",
    "    # collect first spikes\n",
    "    result = []\n",
    "    for idx in range(n_total):\n",
    "        ts = mon.spike_trains()[idx]\n",
    "        result.append(round(float(ts[0]/ms),3) if len(ts) else None)\n",
    "    return result\n",
    "\n",
    "# Example call (2 inputs, 2→1 weights)\n",
    "mini_urd_inputs    = np.array([0.4, 0.5]) # bias of .123\n",
    "#mini_urd_weights_1 = np.array([1, 1])\n",
    "mini_urd_weights_1 = np.array([.2, 1])\n",
    "print(mini_urd(mini_urd_inputs, mini_urd_weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47afa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.622ms, target=1.000ms, loss=0.1934\n",
      " ∇w = [0.11762763 0.05065946]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.728ms, target=1.200ms, loss=0.1394\n",
      " ∇w = [0.13643476 0.10232718]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.493ms, target=1.800ms, loss=0.0471\n",
      " ∇w = [-0.0126795  -0.01305307]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.726ms, target=1.500ms, loss=0.0255\n",
      " ∇w = [0.02357803 0.06605228]\n",
      " Updated w after epoch: [ 0.94700782 -1.04119717]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.614ms, target=1.000ms, loss=0.1885\n",
      " ∇w = [0.10799947 0.0521212 ]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.722ms, target=1.200ms, loss=0.1362\n",
      " ∇w = [0.12212377 0.11013685]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.484ms, target=1.800ms, loss=0.0499\n",
      " ∇w = [-0.0128775  -0.01369549]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.722ms, target=1.500ms, loss=0.0246\n",
      " ∇w = [0.02250171 0.0727253 ]\n",
      " Updated w after epoch: [ 0.89905832 -1.08545474]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.605ms, target=1.000ms, loss=0.1830\n",
      " ∇w = [0.09953924 0.0535088 ]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.716ms, target=1.200ms, loss=0.1331\n",
      " ∇w = [0.11003231 0.11858747]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.473ms, target=1.800ms, loss=0.0535\n",
      " ∇w = [-0.01316674 -0.01446025]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.717ms, target=1.500ms, loss=0.0235\n",
      " ∇w = [0.02141926 0.08012517]\n",
      " Updated w after epoch: [ 0.85549351 -1.13300698]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.596ms, target=1.000ms, loss=0.1776\n",
      " ∇w = [0.09220302 0.05494664]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.711ms, target=1.200ms, loss=0.1306\n",
      " ∇w = [0.09992519 0.12815328]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.461ms, target=1.800ms, loss=0.0575\n",
      " ∇w = [-0.0135037  -0.01531316]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.713ms, target=1.500ms, loss=0.0227\n",
      " ∇w = [0.02049658 0.08915577]\n",
      " Updated w after epoch: [ 0.81566929 -1.18439548]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.586ms, target=1.000ms, loss=0.1717\n",
      " ∇w = [0.08564047 0.0563872 ]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.707ms, target=1.200ms, loss=0.1285\n",
      " ∇w = [0.0913818 0.1391703]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.447ms, target=1.800ms, loss=0.0623\n",
      " ∇w = [-0.01392621 -0.01631143]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.710ms, target=1.500ms, loss=0.0221\n",
      " ∇w = [0.01968529 0.10020789]\n",
      " Updated w after epoch: [ 0.77911302 -1.24028627]\n",
      "\n",
      "Final learned weights: [ 0.77911302 -1.24028627]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Your original spike_timing + its ∂/∂w\n",
    "# ---------------------------------------------------------------------\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    # avoid log(0)\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - x**(1 - w) * np.log(x + eps)\n",
    "    else:\n",
    "        return - (1 - x)**(1 + w) * np.log(1 - x + eps)\n",
    "\n",
    "# sigmoid derivative\n",
    "def dsigmoid(z):\n",
    "    s = 1.0/(1 + np.exp(-z))\n",
    "    return s*(1 - s)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Your mini_urd forward pass (unchanged)\n",
    "# ---------------------------------------------------------------------\n",
    "def mini_urd(inputs, weights_1):\n",
    "    n_input  = 2\n",
    "    n_hidden = 1\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    neurons = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v > 1',\n",
    "        reset='v = 0',\n",
    "        method='exact'\n",
    "    )\n",
    "    neurons.v = 0\n",
    "    neurons.sum = 0\n",
    "    neurons.spikes_received = 0\n",
    "    neurons.global_clock = 0\n",
    "    neurons.scheduled_time = 1e9*second\n",
    "\n",
    "    stim = SpikeGeneratorGroup(2,\n",
    "        indices=[0,1],\n",
    "        times=inputs*ms\n",
    "    )\n",
    "\n",
    "    # stim → input‐layer (no weights here)\n",
    "    syn_input = Synapses(\n",
    "        stim, neurons[0:n_input],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "        scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    syn_input.connect(j='i')\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    # input → hidden (this is the only trainable W)\n",
    "    syn_hidden = Synapses(\n",
    "        neurons[0:n_input], neurons[n_input:],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "        scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    syn_hidden.connect()\n",
    "    syn_hidden.w = weights_1\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "    # run neurons at their scheduled_time\n",
    "    neurons.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time) < 0.0005*ms)*1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(neurons)\n",
    "    run(5*ms)\n",
    "\n",
    "    result = []\n",
    "    for idx in range(n_total):\n",
    "        ts = mon.spike_trains()[idx]\n",
    "        if len(ts):\n",
    "            result.append(float(ts[0]/ms))\n",
    "        else:\n",
    "            result.append(None)\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Training function for the 1‐layer mini model\n",
    "# ---------------------------------------------------------------------\n",
    "def train_mini_urd(training_data, target_times, w_init,\n",
    "                   epochs=5, lr=0.1):\n",
    "    \"\"\"\n",
    "    training_data: list of length-N samples, each is array([t0, t1])\n",
    "    target_times:  list of length-N floats (desired hidden spike time in ms)\n",
    "    w_init:        np.array of shape (2,)\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n=== Epoch {ep+1}/{epochs} ===\")\n",
    "        for i, (inp, t_target) in enumerate(zip(training_data, target_times)):\n",
    "            # ---- forward ----\n",
    "            outs = mini_urd(inp, w)\n",
    "            t_hidden = outs[-1]\n",
    "            if t_hidden is None:\n",
    "                # no spike → treat as very late\n",
    "                t_hidden = 5.0\n",
    "\n",
    "            # ---- loss & dL/dt ----\n",
    "            # use ½*(t_h - t*)²\n",
    "            loss = 0.5 * (t_hidden - t_target)**2\n",
    "            dL_dt = (t_hidden - t_target)\n",
    "\n",
    "            # ---- analytic backprop to w[0], w[1] ----\n",
    "            # recompute sum and sr for hidden\n",
    "            layer_h = 1\n",
    "            # each input‐neuron spikes once, so sr = 2\n",
    "            sr_h = 2.0\n",
    "            # global_clock at arrival: use inp[i] ms for each\n",
    "            s0 = spike_timing(w[0], inp[0], layer_h, 0, 1)\n",
    "            s1 = spike_timing(w[1], inp[1], layer_h, 0, 1)\n",
    "            sum_h = s0 + s1\n",
    "            z = sum_h/sr_h\n",
    "\n",
    "            # d t_sched / d sum_h\n",
    "            # scheduled_time = (sigmoid(z) + layer_h) * ms\n",
    "            # dt/dsum = ms * dsigmoid(z) * (1/sr_h)\n",
    "            dt_dsum = dsigmoid(z) * (1/sr_h)\n",
    "\n",
    "            # dsum/dw_i = d_spike_timing_dw(w_i, global_clock=inp[i], ...)\n",
    "            dsum_dw = np.array([\n",
    "                d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1),\n",
    "                d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1),\n",
    "            ])\n",
    "\n",
    "            # full gradient: dL/dw_i = dL/dt * dt/dsum * dsum/dw_i\n",
    "            grads = dL_dt * dt_dsum * dsum_dw\n",
    "\n",
    "            # ---- print & update ----\n",
    "            print(f\"Sample {i}: inp={inp}, t_h={t_hidden:.3f}ms, target={t_target:.3f}ms, loss={loss:.4f}\")\n",
    "            print(\" ∇w =\", grads)\n",
    "            w -= lr * grads\n",
    "\n",
    "        print(\" Updated w after epoch:\", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Quick demo\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # four toy samples: two input‐spike times, and a target hidden time\n",
    "    X = [np.array([0.2,0.5]),\n",
    "         np.array([0.1,0.8]),\n",
    "         np.array([0.7,0.3]),\n",
    "         np.array([0.4,0.9])]\n",
    "    Y = [1.0, 1.2, 1.8, 1.5]   # desired hidden spike times in ms\n",
    "\n",
    "    w0 = np.array([1.0, -1.0])\n",
    "    w_final = train_mini_urd(X, Y, w0, epochs=5, lr=0.2)\n",
    "\n",
    "    print(\"\\nFinal learned weights:\", w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d37fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.276, s1=1.000, t_h=1.709\n",
      "  L0=0.0003, L1=0.0450, Lf=0.2513, L=0.1710\n",
      "  ∇w = [0.00712, 0.23573]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.158, s1=0.995, t_h=1.685\n",
      "  L0=0.0009, L1=0.0190, Lf=0.1176, L=0.0786\n",
      "  ∇w = [-0.00504, 0.04943]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.752, s1=0.966, t_h=1.708\n",
      "  L0=0.0115, L1=0.1603, Lf=0.0042, L=0.1739\n",
      "  ∇w = [0.03938, 0.65315]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.479, s1=0.990, t_h=1.685\n",
      "  L0=0.0002, L1=0.0761, Lf=0.0171, L=0.0849\n",
      "  ∇w = [-0.00493, 0.04176]\n",
      " Updated w: [0.19634647 0.90199285]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.274, s1=0.934, t_h=1.706\n",
      "  L0=0.0003, L1=0.0275, Lf=0.2492, L=0.1524\n",
      "  ∇w = [0.00647, 0.17787]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.157, s1=0.974, t_h=1.685\n",
      "  L0=0.0009, L1=0.0152, Lf=0.1176, L=0.0750\n",
      "  ∇w = [-0.00544, 0.04403]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.751, s1=0.865, t_h=1.705\n",
      "  L0=0.0114, L1=0.1082, Lf=0.0045, L=0.1219\n",
      "  ∇w = [0.03901, 0.47942]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.477, s1=0.982, t_h=1.685\n",
      "  L0=0.0003, L1=0.0731, Lf=0.0171, L=0.0819\n",
      "  ∇w = [-0.00558, 0.04064]\n",
      " Updated w: [0.1929002 0.8277963]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.273, s1=0.887, t_h=1.703\n",
      "  L0=0.0004, L1=0.0176, Lf=0.2471, L=0.1415\n",
      "  ∇w = [0.00582, 0.14021]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.156, s1=0.959, t_h=1.685\n",
      "  L0=0.0010, L1=0.0127, Lf=0.1176, L=0.0725\n",
      "  ∇w = [-0.00581, 0.04011]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.750, s1=0.795, t_h=1.703\n",
      "  L0=0.0112, L1=0.0781, Lf=0.0047, L=0.0917\n",
      "  ∇w = [0.03868, 0.37349]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.476, s1=0.976, t_h=1.685\n",
      "  L0=0.0003, L1=0.0708, Lf=0.0171, L=0.0797\n",
      "  ∇w = [-0.00618, 0.03975]\n",
      " Updated w: [0.18965049 0.76844043]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.271, s1=0.852, t_h=1.701\n",
      "  L0=0.0004, L1=0.0115, Lf=0.2457, L=0.1348\n",
      "  ∇w = [0.00521, 0.11349]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.155, s1=0.947, t_h=1.685\n",
      "  L0=0.0010, L1=0.0108, Lf=0.1176, L=0.0707\n",
      "  ∇w = [-0.00616, 0.03707]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.749, s1=0.743, t_h=1.701\n",
      "  L0=0.0111, L1=0.0589, Lf=0.0049, L=0.0724\n",
      "  ∇w = [0.03837, 0.30214]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.474, s1=0.971, t_h=1.685\n",
      "  L0=0.0003, L1=0.0689, Lf=0.0171, L=0.0778\n",
      "  ∇w = [-0.00675, 0.03903]\n",
      " Updated w: [0.18658432 0.71926742]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.270, s1=0.823, t_h=1.700\n",
      "  L0=0.0004, L1=0.0076, Lf=0.2450, L=0.1305\n",
      "  ∇w = [0.00465, 0.09347]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.154, s1=0.937, t_h=1.685\n",
      "  L0=0.0011, L1=0.0094, Lf=0.1176, L=0.0693\n",
      "  ∇w = [-0.00648, 0.03461]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.748, s1=0.702, t_h=1.699\n",
      "  L0=0.0110, L1=0.0457, Lf=0.0051, L=0.0592\n",
      "  ∇w = [0.03807, 0.25090]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.473, s1=0.967, t_h=1.685\n",
      "  L0=0.0004, L1=0.0673, Lf=0.0171, L=0.0763\n",
      "  ∇w = [-0.00729, 0.03843]\n",
      " Updated w: [0.18368871 0.67752675]\n",
      "\n",
      "Final weights: [0.18368871 0.67752675]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) spike_timing + its derivative\n",
    "# -----------------------------------------------------------------------------\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - x**(1 - w) * np.log(x + eps)\n",
    "    else:\n",
    "        return - (1 - x)**(1 + w) * np.log(1 - x + eps)\n",
    "\n",
    "def dsigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s*(1 - s)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) mini_urd forward: returns hidden‐spike‐time only\n",
    "# -----------------------------------------------------------------------------\n",
    "def mini_urd(inputs, w):\n",
    "    n_input  = 2\n",
    "    n_hidden = 1\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    G = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        sr              : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v>1', reset='v=0', method='exact'\n",
    "    )\n",
    "    G.v = 0; G.sum = 0; G.sr = 0\n",
    "    G.global_clock = 0\n",
    "    G.scheduled_time = 1e9*second\n",
    "\n",
    "    stim = SpikeGeneratorGroup(2, indices=[0,1], times=inputs*ms)\n",
    "\n",
    "    # first layer has fixed identity weights\n",
    "    S1 = Synapses(stim, G[:2],\n",
    "        'layer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(1, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S1.connect(j='i')\n",
    "    S1.layer = 0\n",
    "\n",
    "    # trainable synapse 2→hidden\n",
    "    S2 = Synapses(G[:2], G[2:],\n",
    "        'w : 1\\nlayer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S2.connect()\n",
    "    S2.w = w\n",
    "    S2.layer = 1\n",
    "\n",
    "    # drive v when scheduled_time hits\n",
    "    G.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time)<0.0005*ms)*1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(G)\n",
    "    run(5*ms)\n",
    "\n",
    "    # return hidden spike time (or a large value if no spike)\n",
    "    ts = mon.spike_trains()[2]\n",
    "    return float(ts[0]/ms) if len(ts) else 5.0\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Training with multi‐loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_multi_loss(\n",
    "    X,                    # list of np.array([t0,t1])\n",
    "    t_hidden_targets,     # list of floats\n",
    "    t0_targets,           # list of floats for s0\n",
    "    t1_targets,           # list of floats for s1\n",
    "    w_init,\n",
    "    alpha=1.0, beta=1.0, gamma=1.0,\n",
    "    epochs=5, lr=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-loss:\n",
    "      L0 = ½ (s0 - t0)^2\n",
    "      L1 = ½ (s1 - t1)^2\n",
    "      Lf = ½ (t_h  - t_h*)^2\n",
    "      L = α L0 + β L1 + γ Lf\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n=== Epoch {ep+1}/{epochs} ===\")\n",
    "        for i, inp in enumerate(X):\n",
    "            # forward pass\n",
    "            t_h = mini_urd(inp, w)\n",
    "            # recompute s0,s1 exactly the same way Brian did\n",
    "            layer_h = 1\n",
    "            # each input is first spike, so sr_i=1, sum_i=0 → use that\n",
    "            s0 = spike_timing(w[0], inp[0], layer_h, 0, 1)\n",
    "            s1 = spike_timing(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # --- compute loss terms ---\n",
    "            t0_tgt = t0_targets[i]\n",
    "            t1_tgt = t1_targets[i]\n",
    "            th_tgt = t_hidden_targets[i]\n",
    "\n",
    "            L0 = 0.5*(s0 - t0_tgt)**2\n",
    "            L1 = 0.5*(s1 - t1_tgt)**2\n",
    "            Lf = 0.5*(t_h - th_tgt)**2\n",
    "            L  = alpha*L0 + beta*L1 + gamma*Lf\n",
    "\n",
    "            # --- gradients ---\n",
    "            # ∂L0/∂w0 = (s0 - t0)*∂s0/∂w0\n",
    "            dL0_dw0 = (s0 - t0_tgt) * d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1)\n",
    "            # ∂L1/∂w1\n",
    "            dL1_dw1 = (s1 - t1_tgt) * d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # ∂Lf/∂w0,w1 = ∂Lf/∂t_h × ∂t_h/∂sum × ∂sum/∂w_i\n",
    "            dLf_dt  = (t_h - th_tgt)\n",
    "            sum_tot = s0 + s1\n",
    "            sr = 2.0\n",
    "            z = sum_tot/sr\n",
    "            dt_dsum = dsigmoid(z)*(1/sr)\n",
    "            dsum_dw0 = d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1)\n",
    "            dsum_dw1 = d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1)\n",
    "            dLf_dw0 = dLf_dt * dt_dsum * dsum_dw0\n",
    "            dLf_dw1 = dLf_dt * dt_dsum * dsum_dw1\n",
    "\n",
    "            # combine\n",
    "            grad0 = alpha*dL0_dw0 + gamma*dLf_dw0\n",
    "            grad1 = beta *dL1_dw1 + gamma*dLf_dw1\n",
    "\n",
    "            # print & update\n",
    "            print(f\"Sample {i}: inp={inp}, s0={s0:.3f}, s1={s1:.3f}, t_h={t_h:.3f}\")\n",
    "            print(f\"  L0={L0:.4f}, L1={L1:.4f}, Lf={Lf:.4f}, L={L:.4f}\")\n",
    "            print(f\"  ∇w = [{grad0:.5f}, {grad1:.5f}]\")\n",
    "            w[0] -= lr * grad0\n",
    "            w[1] -= lr * grad1\n",
    "\n",
    "        print(\" Updated w:\", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Demo\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # toy data: 4 samples\n",
    "    X = [np.array([0.2,0.5]),\n",
    "         np.array([0.1,0.8]),\n",
    "         np.array([0.7,0.3]),\n",
    "         np.array([0.4,0.9])]\n",
    "\n",
    "    # main target: hidden spike at these ms\n",
    "    T_hidden = [1.0, 1.2, 1.8, 1.5]\n",
    "    # aux targets for each synapse\n",
    "    T0 = [0.3, 0.2, 0.6, 0.5]\n",
    "    T1 = [0.7, 0.8, 0.4, 0.6]\n",
    "\n",
    "    w0 = np.array([0.2, 1.0])\n",
    "    w_final = train_multi_loss(X, T_hidden, T0, T1, w0,\n",
    "                               alpha=1.0, beta=1.0, gamma=0.5,\n",
    "                               epochs=5, lr=0.1)\n",
    "\n",
    "    print(\"\\nFinal weights:\", w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b3ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    The object 'neurongroup' is getting deleted, but was never included in a network. This probably means that you did not store the object reference in a variable, or that the variable was not used to construct the network.\n",
      "The object was created here (most recent call only):\n",
      "  File 'C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_23520\\1024278557.py', line 41, in urd_forward\n",
      "    G = NeuronGroup(n_total, [brian2.core.base.unused_brian_object]\n"
     ]
    },
    {
     "ename": "BrianObjectException",
     "evalue": "Error encountered with object named 'neurongroup_1'.\nObject was created here (most recent call only, full details in debug log):\n  File 'C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_23520\\3118840922.py', line 74, in <module>\n    hidden = NeuronGroup(n_hidden,\n\nAn error occurred when preparing an object. (See above for original error message and traceback.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDimensionMismatchError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\equations\\equations.py:1177\u001b[0m, in \u001b[0;36mEquations.check_units\u001b[1;34m(self, group, run_namespace)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1177\u001b[0m     \u001b[43mcheck_dimensions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimensions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_variables\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DimensionMismatchError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\equations\\unitcheck.py:46\u001b[0m, in \u001b[0;36mcheck_dimensions\u001b[1;34m(expression, dimensions, variables)\u001b[0m\n\u001b[0;32m     43\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpression \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpression\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not have the expected unit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[43mfail_for_dimension_mismatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_msg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:264\u001b[0m, in \u001b[0;36mfail_for_dimension_mismatch\u001b[1;34m(obj1, obj2, error_message, **error_quantities)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj2, (Dimension, Unit)):\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(error_message, dim1)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mDimensionMismatchError\u001b[0m: Expression '-v/tau' does not have the expected unit hertz (unit is 1).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDimensionMismatchError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1003\u001b[0m, in \u001b[0;36mNetwork.before_run\u001b[1;34m(self, run_namespace)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m     \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_namespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\groups\\neurongroup.py:989\u001b[0m, in \u001b[0;36mNeuronGroup.before_run\u001b[1;34m(self, run_namespace)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# Check units\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_units\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_namespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# Check that subexpressions that refer to stateful functions are labeled\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# as \"constant over dt\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\equations\\equations.py:1181\u001b[0m, in \u001b[0;36mEquations.check_units\u001b[1;34m(self, group, run_namespace)\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DimensionMismatchError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m-> 1181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   1182\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent units in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1183\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferential equation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1184\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefining variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meq\u001b[38;5;241m.\u001b[39mvarname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1185\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1186\u001b[0m             \u001b[38;5;241m*\u001b[39mex\u001b[38;5;241m.\u001b[39mdims,\n\u001b[0;32m   1187\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m eq\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m SUBEXPRESSION:\n",
      "\u001b[1;31mDimensionMismatchError\u001b[0m: Inconsistent units in differential equation defining variable 'v':\nExpression '-v/tau' does not have the expected unit hertz (unit is 1).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrianObjectException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Run simulation (sufficient time for spikes)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m run_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39mms\n\u001b[1;32m--> 103\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_duration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Read out first spike time for each neuron (ms)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m t_hidden \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_hidden, \u001b[38;5;28mfloat\u001b[39m(run_duration\u001b[38;5;241m/\u001b[39mms))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1141\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1139\u001b[0m     namespace \u001b[38;5;241m=\u001b[39m get_local_namespace(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m-> 1141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_objects) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# TODO: raise an error? warning?\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1005\u001b[0m, in \u001b[0;36mNetwork.before_run\u001b[1;34m(self, run_namespace)\u001b[0m\n\u001b[0;32m   1003\u001b[0m             obj\u001b[38;5;241m.\u001b[39mbefore_run(run_namespace)\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m-> 1005\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m BrianObjectException(\n\u001b[0;32m   1006\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred when preparing an object.\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj\n\u001b[0;32m   1007\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# Check that no object has been run as part of another network before\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m all_objects:\n",
      "\u001b[1;31mBrianObjectException\u001b[0m: Error encountered with object named 'neurongroup_1'.\nObject was created here (most recent call only, full details in debug log):\n  File 'C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_23520\\3118840922.py', line 74, in <module>\n    hidden = NeuronGroup(n_hidden,\n\nAn error occurred when preparing an object. (See above for original error message and traceback.)"
     ]
    }
   ],
   "source": [
    "# Spiking neural network (4-10-3) using Brian2 for Iris classification.\n",
    "# We encode each Iris feature as a single spike time (latency coding):contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure Brian2 is installed\n",
    "try:\n",
    "    from brian2 import *\n",
    "except ImportError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"brian2\"])\n",
    "    from brian2 import *\n",
    "\n",
    "# Load Iris dataset (150 samples, 3 classes, 4 features):contentReference[oaicite:2]{index=2}.\n",
    "data = load_iris()\n",
    "X = data.data  # shape (150,4)\n",
    "y = data.target  # classes 0,1,2\n",
    "\n",
    "# Scale features to [0,1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# Network structure\n",
    "n_input = 4\n",
    "n_hidden = 10\n",
    "n_output = 3\n",
    "\n",
    "# Latency encoding: map feature to spike time (ms), larger feature => earlier spike\n",
    "T_max = 20.0  # max input spike time in ms\n",
    "\n",
    "# Target spike times for output neurons (ms): correct class early, others late\n",
    "target_times = {\n",
    "    0: np.array([5.0, 15.0, 15.0]),\n",
    "    1: np.array([15.0, 5.0, 15.0]),\n",
    "    2: np.array([15.0, 15.0, 5.0])\n",
    "}\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Initialize weights (small random positives)\n",
    "np.random.seed(0)\n",
    "W_input_hidden = np.random.normal(0.5, 0.1, size=(n_input, n_hidden))\n",
    "W_hidden_output = np.random.normal(0.5, 0.1, size=(n_hidden, n_output))\n",
    "\n",
    "# Time constant (ms)\n",
    "tau = 10.0\n",
    "\n",
    "# Training epochs\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle dataset each epoch\n",
    "    indices = np.arange(X_norm.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    total_loss = 0.0\n",
    "    for idx in indices:\n",
    "        # Reset Brian state for this trial\n",
    "        start_scope()\n",
    "\n",
    "        # Encode features as spike times: larger feature -> earlier spike\n",
    "        features = X_norm[idx]\n",
    "        input_spike_times = (1.0 - features) * T_max  # in ms\n",
    "        spike_times = input_spike_times * ms\n",
    "        indices_i = np.arange(n_input, dtype=int)\n",
    "\n",
    "        # Input group: each of 4 neurons emits one spike at given time:contentReference[oaicite:3]{index=3}\n",
    "        inp = SpikeGeneratorGroup(n_input, indices_i, spike_times)\n",
    "\n",
    "        # Hidden and output layers: LIF neurons with threshold=1\n",
    "        hidden = NeuronGroup(n_hidden,\n",
    "                             'dv/dt = -v/tau: 1',\n",
    "                             threshold='v>1', reset='v=0', method='exact')\n",
    "        output = NeuronGroup(n_output,\n",
    "                             'dv/dt = -v/tau: 1',\n",
    "                             threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # Synapses with weights\n",
    "        syn_in_h = Synapses(inp, hidden, model='w:1', on_pre='v_post += w')\n",
    "        syn_h_out = Synapses(hidden, output, model='w:1', on_pre='v_post += w')\n",
    "\n",
    "        # Fully connect layers\n",
    "        syn_in_h.connect()\n",
    "        syn_h_out.connect()\n",
    "\n",
    "        # Assign current weights to synapses\n",
    "        for i in range(n_input):\n",
    "            for j in range(n_hidden):\n",
    "                syn_in_h.w[i, j] = W_input_hidden[i, j]\n",
    "        for i in range(n_hidden):\n",
    "            for j in range(n_output):\n",
    "                syn_h_out.w[i, j] = W_hidden_output[i, j]\n",
    "\n",
    "        # Monitors to record spikes\n",
    "        mon_hidden = SpikeMonitor(hidden)\n",
    "        mon_output = SpikeMonitor(output)\n",
    "\n",
    "        # Run simulation (sufficient time for spikes)\n",
    "        run_duration = 50*ms\n",
    "        run(run_duration)\n",
    "\n",
    "        # Read out first spike time for each neuron (ms)\n",
    "        t_hidden = np.full(n_hidden, float(run_duration/ms))\n",
    "        t_output = np.full(n_output, float(run_duration/ms))\n",
    "        hidden_trains = mon_hidden.spike_trains()\n",
    "        output_trains = mon_output.spike_trains()\n",
    "        for i in range(n_hidden):\n",
    "            if len(hidden_trains[i]) > 0:\n",
    "                t_hidden[i] = hidden_trains[i][0] / ms\n",
    "        for j in range(n_output):\n",
    "            if len(output_trains[j]) > 0:\n",
    "                t_output[j] = output_trains[j][0] / ms\n",
    "\n",
    "        # Compute MSE loss\n",
    "        target = target_times[y[idx]]\n",
    "        loss = np.mean((t_output - target) ** 2)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Compute gradients (analytically) and update weights\n",
    "        grad_hidden_output = np.zeros_like(W_hidden_output)\n",
    "        grad_input_hidden = np.zeros_like(W_input_hidden)\n",
    "        # Output layer gradients\n",
    "        for j in range(n_output):\n",
    "            dL_dt_out = 2.0 * (t_output[j] - target[j]) / n_output\n",
    "            sum_exp = np.sum(W_hidden_output[:, j] * np.exp(t_hidden / tau))\n",
    "            if sum_exp <= 0:\n",
    "                continue\n",
    "            for i in range(n_hidden):\n",
    "                dt_dw = tau * np.exp(t_hidden[i] / tau) / sum_exp\n",
    "                grad_hidden_output[i, j] = dL_dt_out * dt_dw\n",
    "                # Chain rule to hidden neurons, then input->hidden weights\n",
    "                dL_dt_hidden = dL_dt_out * (W_hidden_output[i, j] * np.exp(t_hidden[i] / tau) / sum_exp)\n",
    "                sum_exp_in = np.sum(W_input_hidden[:, i] * np.exp(input_spike_times / tau))\n",
    "                if sum_exp_in <= 0:\n",
    "                    continue\n",
    "                for k in range(n_input):\n",
    "                    dt_hidden_dw = tau * np.exp(input_spike_times[k] / tau) / sum_exp_in\n",
    "                    grad_input_hidden[k, i] += dL_dt_hidden * dt_hidden_dw\n",
    "        # Gradient descent weight update\n",
    "        W_hidden_output -= lr * grad_hidden_output\n",
    "        W_input_hidden  -= lr * grad_input_hidden\n",
    "\n",
    "    # Report average loss this epoch\n",
    "    avg_loss = total_loss / X_norm.shape[0]\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6ea9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 48.4388\n",
      "Epoch 2/10 - Loss: 48.0451\n",
      "Epoch 3/10 - Loss: 227.4150\n",
      "Epoch 4/10 - Loss: 362.0912\n",
      "Epoch 5/10 - Loss: 362.0912\n",
      "Epoch 6/10 - Loss: 362.0912\n",
      "Epoch 7/10 - Loss: 362.0912\n",
      "Epoch 8/10 - Loss: 362.0912\n",
      "Epoch 9/10 - Loss: 362.0912\n",
      "Epoch 10/10 - Loss: 362.0912\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Prepare Iris dataset\n",
    "# ----------------------------------------------------------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data  # shape (150,4)\n",
    "y = iris.target  # classes 0,1,2\n",
    "# normalize features to [0,1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Network hyperparameters\n",
    "# ----------------------------------------------------------------------------\n",
    "n_input = 4\n",
    "n_hidden = 10\n",
    "n_output = 3\n",
    "T_max = 20.0  # max input spike latency in ms\n",
    "\n",
    "tau = 10 * ms  # membrane time constant\n",
    "\n",
    "def ms_to_unitless(arr_ms):\n",
    "    \"\"\"\n",
    "    Convert numpy array in ms to unitless values relative to tau (in ms).\n",
    "    \"\"\"\n",
    "    return arr_ms / float(tau/ms)\n",
    "\n",
    "# target spike times for outputs (ms): desired class early\n",
    "target_times = {\n",
    "    0: np.array([5.0, 15.0, 15.0]),\n",
    "    1: np.array([15.0, 5.0, 15.0]),\n",
    "    2: np.array([15.0, 15.0, 5.0])\n",
    "}\n",
    "\n",
    "# initialize weights\n",
    "np.random.seed(0)\n",
    "W_in_h = np.random.normal(0.5, 0.1, (n_input, n_hidden))\n",
    "W_h_out = np.random.normal(0.5, 0.1, (n_hidden, n_output))\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "# ----------------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    # shuffle\n",
    "    idxs = np.random.permutation(len(X_norm))\n",
    "    for idx in idxs:\n",
    "        start_scope()\n",
    "        # encode features → spike latencies (ms)\n",
    "        features = X_norm[idx]\n",
    "        in_latencies_ms = (1.0 - features) * T_max\n",
    "        # Brian2 spike times with units\n",
    "        sg = SpikeGeneratorGroup(\n",
    "            n_input,\n",
    "            indices=np.arange(n_input),\n",
    "            times=in_latencies_ms * ms\n",
    "        )\n",
    "\n",
    "        # LIF neurons\n",
    "        hid = NeuronGroup(\n",
    "            n_hidden,\n",
    "            'dv/dt = -v/tau : 1',\n",
    "            threshold='v>1', reset='v=0', method='euler'\n",
    "        )\n",
    "        out = NeuronGroup(\n",
    "            n_output,\n",
    "            'dv/dt = -v/tau : 1',\n",
    "            threshold='v>1', reset='v=0', method='euler'\n",
    "        )\n",
    "\n",
    "        # synapses\n",
    "        S1 = Synapses(sg, hid, 'w:1', on_pre='v_post += w')\n",
    "        S2 = Synapses(hid, out, 'w:1', on_pre='v_post += w')\n",
    "        S1.connect()\n",
    "        S2.connect()\n",
    "        # assign weights\n",
    "        S1.w = W_in_h.flatten()\n",
    "        S2.w = W_h_out.flatten()\n",
    "\n",
    "        mon_h = SpikeMonitor(hid)\n",
    "        mon_o = SpikeMonitor(out)\n",
    "\n",
    "        run(50 * ms)\n",
    "\n",
    "        # extract first spike latencies ms\n",
    "        t_h = np.full(n_hidden, float(50))\n",
    "        t_o = np.full(n_output, float(50))\n",
    "        for i in range(n_hidden):\n",
    "            spikes = mon_h.spike_trains()[i]\n",
    "            if len(spikes): t_h[i] = spikes[0] / ms\n",
    "        for j in range(n_output):\n",
    "            spikes = mon_o.spike_trains()[j]\n",
    "            if len(spikes): t_o[j] = spikes[0] / ms\n",
    "\n",
    "        # compute loss (MSE)\n",
    "        target_o = target_times[y[idx]]\n",
    "        loss = np.mean((t_o - target_o) ** 2)\n",
    "        total_loss += loss\n",
    "\n",
    "        # gradients via chain rule\n",
    "        grad_S2 = np.zeros_like(W_h_out)\n",
    "        grad_S1 = np.zeros_like(W_in_h)\n",
    "        # convert latencies to unitless\n",
    "        t_h_u = ms_to_unitless(t_h)\n",
    "        in_lat_u = ms_to_unitless(in_latencies_ms)\n",
    "\n",
    "        # output layer gradients\n",
    "        for j in range(n_output):\n",
    "            dL_dt = 2 * (t_o[j] - target_o[j]) / n_output\n",
    "            exp_h = np.exp(t_h_u)\n",
    "            denom_h = np.dot(W_h_out[:, j], exp_h)\n",
    "            if denom_h <= 0: continue\n",
    "            for i_h in range(n_hidden):\n",
    "                dt_dw = exp_h[i_h] / denom_h\n",
    "                grad_S2[i_h, j] = dL_dt * dt_dw\n",
    "                # backprop into hidden\n",
    "                dL_dh = dL_dt * W_h_out[i_h, j] * exp_h[i_h] / denom_h\n",
    "                # hidden → input gradients\n",
    "                exp_in = np.exp(in_lat_u)\n",
    "                denom_in = np.dot(W_in_h[:, i_h], exp_in)\n",
    "                if denom_in <= 0: continue\n",
    "                for i_in in range(n_input):\n",
    "                    dt_dw1 = exp_in[i_in] / denom_in\n",
    "                    grad_S1[i_in, i_h] += dL_dh * dt_dw1\n",
    "\n",
    "        # update weights\n",
    "        W_h_out -= lr * grad_S2\n",
    "        W_in_h  -= lr * grad_S1\n",
    "\n",
    "    avg_loss = total_loss / len(X_norm)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fd3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c3410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6f7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module='brian2.codegen.generators.base')\n",
    "\n",
    "start_scope()\n",
    "\n",
    "defaultclock.dt = 0.0001*ms  \n",
    "\n",
    "# Custom timing function\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, result=1, sum=1, spikes_received=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received): \n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return (x ** (1 - w)) \n",
    "    else:\n",
    "        return (1 - (1 - x) ** (1 + w)) \n",
    "    \n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(layer=1, result=1, sum=1, spikes_received=1)\n",
    "def math1(layer, sum, spikes_received): \n",
    "    return (sum/spikes_received )+ layer\n",
    "\n",
    "def run_Urd(inputs, weights_1, weights_2, weights_3):\n",
    "    '''4-10-3 SNN'''\n",
    "    # will add check of weights # so it all works\n",
    "    n_input = 4 \n",
    "    n_hidden = 10\n",
    "    n_output = 3\n",
    "    n_total = n_input + n_hidden + n_output\n",
    "\n",
    "    neurons = NeuronGroup(n_total, '''\n",
    "        v : 1\n",
    "        sum : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time : second\n",
    "        global_clock : 1\n",
    "    ''', threshold='v > 1', reset='v = 0', method='exact')\n",
    "    neurons.v = 0\n",
    "    neurons.scheduled_time = 1e9 * second\n",
    "    neurons.global_clock = 0.0\n",
    "    neurons.sum = 0.0\n",
    "    neurons.spikes_received = 0.0\n",
    "\n",
    "\n",
    "    indicess = [i for i in range(n_input)]\n",
    "    stim = SpikeGeneratorGroup(n_input, indices=indicess, times=(inputs*ms))\n",
    "\n",
    "    syn_input = Synapses(stim, neurons[0:n_input], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    syn_input.connect(j='i')\n",
    "    syn_input.w = weights_1\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    syn_hidden = Synapses(neurons[0:n_input], neurons[n_input:n_input+n_hidden], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    for inp in range(n_input):\n",
    "        for hid in range(n_hidden):\n",
    "            syn_hidden.connect(i=inp, j=hid)\n",
    "\n",
    "    syn_hidden.w = weights_2\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "\n",
    "    syn_output = Synapses(\n",
    "        neurons[n_input:n_input+n_hidden], \n",
    "        neurons[n_input+n_hidden:n_total], \n",
    "        '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "        ''',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "        '''\n",
    "    )\n",
    "\n",
    "    for hid in range(n_hidden):\n",
    "        for out in range(n_output):\n",
    "            syn_output.connect(i=hid, j=out)\n",
    "\n",
    "    # Set weights in correct order\n",
    "    syn_output.w[:] = weights_3\n",
    "    syn_output.layer = 2\n",
    "\n",
    "    #print(syn_output.i[:], syn_output.j[:])\n",
    "    #weights_into_output_1 = weights_3[1::3]\n",
    "\n",
    "\n",
    "\n",
    "    neurons.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "\n",
    "    spikemon = SpikeMonitor(neurons)\n",
    "    \n",
    "    # neurons.v = 0\n",
    "    # neurons.scheduled_time = 1e9 * second\n",
    "    # neurons.global_clock = 0.0\n",
    "    # neurons.sum = 0.0\n",
    "    # neurons.spikes_received = 0.0\n",
    "\n",
    "    run(5*ms)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(n_total):\n",
    "        times = spikemon.spike_trains()[i]\n",
    "        if len(times) > 0:\n",
    "            result.append(round(times[0]/ms, 3))\n",
    "        else:\n",
    "            result.append(None)  # or some other placeholder like float('nan')\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ccc62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14345227 0.86066371 0.88725555 0.85814195] [0.44777422 0.76353581 0.48259652 0.3623195  0.99852955 0.99073865\n",
      " 0.90408036 0.39032022 0.29293302 0.22087558 0.15663    0.32492888\n",
      " 0.56082809 0.25731051 0.04950147 0.28284291 0.23587225 0.44035401\n",
      " 0.39615996 0.35721367 0.89914801 0.44307031 0.63992119 0.52551554\n",
      " 0.98827489 0.05136685 0.22214933 0.74473358 0.12725664 0.83123581\n",
      " 0.98797027 0.54554983 0.49111424 0.37928825 0.2765512  0.63761703\n",
      " 0.15451435 0.66322082 0.94605562 0.18985174] [0.22203751 0.43385482 0.91329917 0.06779644 0.40236334 0.81143013\n",
      " 0.4022748  0.1185883  0.60056945 0.44718627 0.4049495  0.8794631\n",
      " 0.85964283 0.19174956 0.60335014 0.80864427 0.46067967 0.14843805\n",
      " 0.04586898 0.5629214  0.93889325 0.55204879 0.64219171 0.40427682\n",
      " 0.87504725 0.92320919 0.09731941 0.36464712 0.61217678 0.59806614]\n",
      "Epoch  0  Loss=0.2828  Outputs=[2.948, 2.955, 2.962]\n",
      "[0.13953711 0.86182532 0.88831144 0.85949145] [0.44743883 0.76336731 0.48216986 0.36189828 0.99909788 0.9911931\n",
      " 0.90395764 0.39029905 0.29326618 0.22012886 0.15599928 0.32419638\n",
      " 0.56023825 0.25677451 0.05653258 0.29145169 0.23518641 0.43993118\n",
      " 0.39599046 0.3567034  0.91026207 0.45410064 0.63883158 0.52466321\n",
      " 0.99408617 0.05810146 0.23281233 0.74405953 0.12696208 0.83034703\n",
      " 0.98733955 0.54481733 0.49048446 0.37874531 0.28337626 0.64570119\n",
      " 0.15382852 0.66279799 0.94588612 0.18934146] [0.21876973 0.4324101  0.91297917 0.06540616 0.40134529 0.81122318\n",
      " 0.39871252 0.11714358 0.6002385  0.4425356  0.40297812 0.87897976\n",
      " 0.85879033 0.19139106 0.60328364 0.80721844 0.46007717 0.14833071\n",
      " 0.04376921 0.56198734 0.93871481 0.54819101 0.64048565 0.40392365\n",
      " 0.86989742 0.92092319 0.09687291 0.35969729 0.60998122 0.5975577 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_loss(predicted, desired):\n",
    "    \"\"\"\n",
    "    MSE loss on the 3 output spike-times.\n",
    "    predicted: full list of 17 times (None → we treat as large penalty)\n",
    "    desired: array-like of length 3\n",
    "    \"\"\"\n",
    "    # pull out last 3 spikes\n",
    "    out_t = np.array([t if t is not None else 10.0 for t in predicted[-3:]])\n",
    "    d = np.array(desired)\n",
    "    return np.mean((out_t - d)**2)\n",
    "\n",
    "def finite_difference_grads(inputs, w1, w2, w3, desired, eps=3):\n",
    "    \"\"\"\n",
    "    Returns numerical gradients of loss wrt each weight array.\n",
    "    \"\"\"\n",
    "    # baseline loss\n",
    "    base_out = run_Urd(inputs, w1, w2, w3)\n",
    "    L0 = compute_loss(base_out, desired)\n",
    "\n",
    "    # grad for layer1\n",
    "    gw1 = np.zeros_like(w1)\n",
    "    for idx in np.ndindex(w1.shape):\n",
    "        w1p = w1.copy()\n",
    "        w1p[idx] += eps\n",
    "        Lp = compute_loss(run_Urd(inputs, w1p, w2, w3), desired)\n",
    "        gw1[idx] = (Lp - L0)/eps\n",
    "\n",
    "    # grad for layer2\n",
    "    gw2 = np.zeros_like(w2)\n",
    "    for idx in np.ndindex(w2.shape):\n",
    "        w2p = w2.copy()\n",
    "        w2p[idx] += eps\n",
    "        Lp = compute_loss(run_Urd(inputs, w1, w2p, w3), desired)\n",
    "        gw2[idx] = (Lp - L0)/eps\n",
    "\n",
    "    # grad for layer3\n",
    "    gw3 = np.zeros_like(w3)\n",
    "    for idx in np.ndindex(w3.shape):\n",
    "        w3p = w3.copy()\n",
    "        w3p[idx] += eps\n",
    "        Lp = compute_loss(run_Urd(inputs, w1, w2, w3p), desired)\n",
    "        gw3[idx] = (Lp - L0)/eps\n",
    "\n",
    "    return gw1, gw2, gw3, L0\n",
    "\n",
    "def backprop_snn_fd(inputs, w1, w2, w3,\n",
    "                    desired=[2.1, 2.6, 2.9],\n",
    "                    lr=0.1, eps=3):\n",
    "    \"\"\"\n",
    "    One gradient‐step on the SNN via finite‐difference.\n",
    "    Returns updated (w1, w2, w3) and the loss before update.\n",
    "    \"\"\"\n",
    "    gw1, gw2, gw3, loss = finite_difference_grads(inputs, w1, w2, w3, desired, eps)\n",
    "    # gradient descent\n",
    "    w1 -= lr * gw1\n",
    "    w2 -= lr * gw2\n",
    "    w3 -= lr * gw3\n",
    "    return w1, w2, w3, loss\n",
    "\n",
    "# — example usage —\n",
    "# random init\n",
    "inputs    = np.random.uniform(0, 1, 4)\n",
    "weights_1 = np.random.uniform(0, 1, 4)\n",
    "weights_2 = np.random.uniform(0, 1, 40)\n",
    "weights_3 = np.random.uniform(0, 1, 30)\n",
    "\n",
    "print(weights_1, weights_2, weights_3)\n",
    "\n",
    "desired = [2.1, 2.6, 2.9]\n",
    "for epoch in range(1):\n",
    "    w1, w2, w3, loss = backprop_snn_fd(\n",
    "        inputs, weights_1, weights_2, weights_3,\n",
    "        desired=desired, lr=0.5, eps=3\n",
    "    )\n",
    "    weights_1, weights_2, weights_3 = w1, w2, w3\n",
    "    out = run_Urd(inputs, w1, w2, w3)[-3:]\n",
    "    print(f\"Epoch {epoch:2d}  Loss={loss:.4f}  Outputs={out}\")\n",
    "\n",
    "print(weights_1, weights_2, weights_3)\n",
    "\n",
    "# After a few epochs you should see the 3 output times marching\n",
    "# closer to [2.1, 2.6, 2.9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 – Avg spikes: 0.00\n",
      "Epoch 2/20 – Avg spikes: 0.00\n",
      "Epoch 3/20 – Avg spikes: 0.00\n",
      "Epoch 4/20 – Avg spikes: 0.00\n",
      "Epoch 5/20 – Avg spikes: 0.00\n",
      "Epoch 6/20 – Avg spikes: 0.00\n",
      "Epoch 7/20 – Avg spikes: 0.00\n",
      "Epoch 8/20 – Avg spikes: 0.00\n",
      "Epoch 9/20 – Avg spikes: 0.00\n",
      "Epoch 10/20 – Avg spikes: 0.00\n",
      "Epoch 11/20 – Avg spikes: 0.00\n",
      "Epoch 12/20 – Avg spikes: 0.00\n",
      "Epoch 13/20 – Avg spikes: 0.00\n",
      "Epoch 14/20 – Avg spikes: 0.00\n",
      "Epoch 15/20 – Avg spikes: 0.00\n",
      "Epoch 16/20 – Avg spikes: 0.00\n",
      "Epoch 17/20 – Avg spikes: 0.00\n",
      "Epoch 18/20 – Avg spikes: 0.00\n",
      "Epoch 19/20 – Avg spikes: 0.00\n",
      "Epoch 20/20 – Avg spikes: 0.00\n",
      "Final first-spike times (ms): [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
