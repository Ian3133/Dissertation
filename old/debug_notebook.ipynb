{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1f44d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n",
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.599, 0.623, 1.697]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "# Your custom timing function (unchanged)\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "def mini_urd(inputs, weights_1):\n",
    "    n_input  = 2\n",
    "    n_hidden = 1\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    neurons = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v > 1',\n",
    "        reset='v = 0',\n",
    "        method='exact'\n",
    "    )\n",
    "    neurons.v = 0\n",
    "    neurons.sum = 0\n",
    "    neurons.spikes_received = 0\n",
    "    neurons.global_clock = 0\n",
    "    neurons.scheduled_time = 1e9*second\n",
    "\n",
    "    # 2‐neuron stimulus\n",
    "    stim = SpikeGeneratorGroup(2,\n",
    "        indices=[0,1],\n",
    "        times=inputs*ms\n",
    "    )\n",
    "\n",
    "    # stim → “input” neurons\n",
    "    syn_input = Synapses(\n",
    "        stim, neurons[0:n_input],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "spikes_received += 1\n",
    "sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "\n",
    "scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "'''\n",
    "    )\n",
    "    syn_input.connect(j='i')\n",
    "    # syn_input.w = weights_1\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    # “input” → hidden\n",
    "    syn_hidden = Synapses(\n",
    "        neurons[0:n_input], neurons[n_input:],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "spikes_received += 1\n",
    "sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "'''\n",
    "    )\n",
    "    syn_hidden.connect()\n",
    "    syn_hidden.w = weights_1\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "    # fire at scheduled_time\n",
    "    neurons.run_regularly('''\n",
    "v = int(abs(t - scheduled_time) < 0.0005*ms)*1.2\n",
    "global_clock += 0.001\n",
    "''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(neurons)\n",
    "    run(5*ms)\n",
    "\n",
    "    # collect first spikes\n",
    "    result = []\n",
    "    for idx in range(n_total):\n",
    "        ts = mon.spike_trains()[idx]\n",
    "        result.append(round(float(ts[0]/ms),3) if len(ts) else None)\n",
    "    return result\n",
    "\n",
    "# Example call (2 inputs, 2→1 weights)\n",
    "mini_urd_inputs    = np.array([0.4, 0.5]) # bias of .123\n",
    "#mini_urd_weights_1 = np.array([1, 1])\n",
    "mini_urd_weights_1 = np.array([.2, 1])\n",
    "print(mini_urd(mini_urd_inputs, mini_urd_weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47afa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.622ms, target=1.000ms, loss=0.1934\n",
      " ∇w = [0.11762763 0.05065946]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.728ms, target=1.200ms, loss=0.1394\n",
      " ∇w = [0.13643476 0.10232718]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.493ms, target=1.800ms, loss=0.0471\n",
      " ∇w = [-0.0126795  -0.01305307]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.726ms, target=1.500ms, loss=0.0255\n",
      " ∇w = [0.02357803 0.06605228]\n",
      " Updated w after epoch: [ 0.94700782 -1.04119717]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.614ms, target=1.000ms, loss=0.1885\n",
      " ∇w = [0.10799947 0.0521212 ]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.722ms, target=1.200ms, loss=0.1362\n",
      " ∇w = [0.12212377 0.11013685]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.484ms, target=1.800ms, loss=0.0499\n",
      " ∇w = [-0.0128775  -0.01369549]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.722ms, target=1.500ms, loss=0.0246\n",
      " ∇w = [0.02250171 0.0727253 ]\n",
      " Updated w after epoch: [ 0.89905832 -1.08545474]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.605ms, target=1.000ms, loss=0.1830\n",
      " ∇w = [0.09953924 0.0535088 ]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.716ms, target=1.200ms, loss=0.1331\n",
      " ∇w = [0.11003231 0.11858747]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.473ms, target=1.800ms, loss=0.0535\n",
      " ∇w = [-0.01316674 -0.01446025]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.717ms, target=1.500ms, loss=0.0235\n",
      " ∇w = [0.02141926 0.08012517]\n",
      " Updated w after epoch: [ 0.85549351 -1.13300698]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.596ms, target=1.000ms, loss=0.1776\n",
      " ∇w = [0.09220302 0.05494664]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.711ms, target=1.200ms, loss=0.1306\n",
      " ∇w = [0.09992519 0.12815328]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.461ms, target=1.800ms, loss=0.0575\n",
      " ∇w = [-0.0135037  -0.01531316]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.713ms, target=1.500ms, loss=0.0227\n",
      " ∇w = [0.02049658 0.08915577]\n",
      " Updated w after epoch: [ 0.81566929 -1.18439548]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Sample 0: inp=[0.2 0.5], t_h=1.586ms, target=1.000ms, loss=0.1717\n",
      " ∇w = [0.08564047 0.0563872 ]\n",
      "Sample 1: inp=[0.1 0.8], t_h=1.707ms, target=1.200ms, loss=0.1285\n",
      " ∇w = [0.0913818 0.1391703]\n",
      "Sample 2: inp=[0.7 0.3], t_h=1.447ms, target=1.800ms, loss=0.0623\n",
      " ∇w = [-0.01392621 -0.01631143]\n",
      "Sample 3: inp=[0.4 0.9], t_h=1.710ms, target=1.500ms, loss=0.0221\n",
      " ∇w = [0.01968529 0.10020789]\n",
      " Updated w after epoch: [ 0.77911302 -1.24028627]\n",
      "\n",
      "Final learned weights: [ 0.77911302 -1.24028627]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Your original spike_timing + its ∂/∂w\n",
    "# ---------------------------------------------------------------------\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    # avoid log(0)\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - x**(1 - w) * np.log(x + eps)\n",
    "    else:\n",
    "        return - (1 - x)**(1 + w) * np.log(1 - x + eps)\n",
    "\n",
    "# sigmoid derivative\n",
    "def dsigmoid(z):\n",
    "    s = 1.0/(1 + np.exp(-z))\n",
    "    return s*(1 - s)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Your mini_urd forward pass (unchanged)\n",
    "# ---------------------------------------------------------------------\n",
    "def mini_urd(inputs, weights_1):\n",
    "    n_input  = 2\n",
    "    n_hidden = 1\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    neurons = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v > 1',\n",
    "        reset='v = 0',\n",
    "        method='exact'\n",
    "    )\n",
    "    neurons.v = 0\n",
    "    neurons.sum = 0\n",
    "    neurons.spikes_received = 0\n",
    "    neurons.global_clock = 0\n",
    "    neurons.scheduled_time = 1e9*second\n",
    "\n",
    "    stim = SpikeGeneratorGroup(2,\n",
    "        indices=[0,1],\n",
    "        times=inputs*ms\n",
    "    )\n",
    "\n",
    "    # stim → input‐layer (no weights here)\n",
    "    syn_input = Synapses(\n",
    "        stim, neurons[0:n_input],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "        scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    syn_input.connect(j='i')\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    # input → hidden (this is the only trainable W)\n",
    "    syn_hidden = Synapses(\n",
    "        neurons[0:n_input], neurons[n_input:],\n",
    "        'w : 1\\nlayer : 1',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, spikes_received)\n",
    "        scheduled_time = (1/(1 + exp(-(sum/spikes_received))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    syn_hidden.connect()\n",
    "    syn_hidden.w = weights_1\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "    # run neurons at their scheduled_time\n",
    "    neurons.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time) < 0.0005*ms)*1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(neurons)\n",
    "    run(5*ms)\n",
    "\n",
    "    result = []\n",
    "    for idx in range(n_total):\n",
    "        ts = mon.spike_trains()[idx]\n",
    "        if len(ts):\n",
    "            result.append(float(ts[0]/ms))\n",
    "        else:\n",
    "            result.append(None)\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Training function for the 1‐layer mini model\n",
    "# ---------------------------------------------------------------------\n",
    "def train_mini_urd(training_data, target_times, w_init,\n",
    "                   epochs=5, lr=0.1):\n",
    "    \"\"\"\n",
    "    training_data: list of length-N samples, each is array([t0, t1])\n",
    "    target_times:  list of length-N floats (desired hidden spike time in ms)\n",
    "    w_init:        np.array of shape (2,)\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n=== Epoch {ep+1}/{epochs} ===\")\n",
    "        for i, (inp, t_target) in enumerate(zip(training_data, target_times)):\n",
    "            # ---- forward ----\n",
    "            outs = mini_urd(inp, w)\n",
    "            t_hidden = outs[-1]\n",
    "            if t_hidden is None:\n",
    "                # no spike → treat as very late\n",
    "                t_hidden = 5.0\n",
    "\n",
    "            # ---- loss & dL/dt ----\n",
    "            # use ½*(t_h - t*)²\n",
    "            loss = 0.5 * (t_hidden - t_target)**2\n",
    "            dL_dt = (t_hidden - t_target)\n",
    "\n",
    "            # ---- analytic backprop to w[0], w[1] ----\n",
    "            # recompute sum and sr for hidden\n",
    "            layer_h = 1\n",
    "            # each input‐neuron spikes once, so sr = 2\n",
    "            sr_h = 2.0\n",
    "            # global_clock at arrival: use inp[i] ms for each\n",
    "            s0 = spike_timing(w[0], inp[0], layer_h, 0, 1)\n",
    "            s1 = spike_timing(w[1], inp[1], layer_h, 0, 1)\n",
    "            sum_h = s0 + s1\n",
    "            z = sum_h/sr_h\n",
    "\n",
    "            # d t_sched / d sum_h\n",
    "            # scheduled_time = (sigmoid(z) + layer_h) * ms\n",
    "            # dt/dsum = ms * dsigmoid(z) * (1/sr_h)\n",
    "            dt_dsum = dsigmoid(z) * (1/sr_h)\n",
    "\n",
    "            # dsum/dw_i = d_spike_timing_dw(w_i, global_clock=inp[i], ...)\n",
    "            dsum_dw = np.array([\n",
    "                d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1),\n",
    "                d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1),\n",
    "            ])\n",
    "\n",
    "            # full gradient: dL/dw_i = dL/dt * dt/dsum * dsum/dw_i\n",
    "            grads = dL_dt * dt_dsum * dsum_dw\n",
    "\n",
    "            # ---- print & update ----\n",
    "            print(f\"Sample {i}: inp={inp}, t_h={t_hidden:.3f}ms, target={t_target:.3f}ms, loss={loss:.4f}\")\n",
    "            print(\" ∇w =\", grads)\n",
    "            w -= lr * grads\n",
    "\n",
    "        print(\" Updated w after epoch:\", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Quick demo\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # four toy samples: two input‐spike times, and a target hidden time\n",
    "    X = [np.array([0.2,0.5]),\n",
    "         np.array([0.1,0.8]),\n",
    "         np.array([0.7,0.3]),\n",
    "         np.array([0.4,0.9])]\n",
    "    Y = [1.0, 1.2, 1.8, 1.5]   # desired hidden spike times in ms\n",
    "\n",
    "    w0 = np.array([1.0, -1.0])\n",
    "    w_final = train_mini_urd(X, Y, w0, epochs=5, lr=0.2)\n",
    "\n",
    "    print(\"\\nFinal learned weights:\", w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d37fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.276, s1=1.000, t_h=1.709\n",
      "  L0=0.0003, L1=0.0450, Lf=0.2513, L=0.1710\n",
      "  ∇w = [0.00712, 0.23573]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.158, s1=0.995, t_h=1.685\n",
      "  L0=0.0009, L1=0.0190, Lf=0.1176, L=0.0786\n",
      "  ∇w = [-0.00504, 0.04943]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.752, s1=0.966, t_h=1.708\n",
      "  L0=0.0115, L1=0.1603, Lf=0.0042, L=0.1739\n",
      "  ∇w = [0.03938, 0.65315]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.479, s1=0.990, t_h=1.685\n",
      "  L0=0.0002, L1=0.0761, Lf=0.0171, L=0.0849\n",
      "  ∇w = [-0.00493, 0.04176]\n",
      " Updated w: [0.19634647 0.90199285]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.274, s1=0.934, t_h=1.706\n",
      "  L0=0.0003, L1=0.0275, Lf=0.2492, L=0.1524\n",
      "  ∇w = [0.00647, 0.17787]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.157, s1=0.974, t_h=1.685\n",
      "  L0=0.0009, L1=0.0152, Lf=0.1176, L=0.0750\n",
      "  ∇w = [-0.00544, 0.04403]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.751, s1=0.865, t_h=1.705\n",
      "  L0=0.0114, L1=0.1082, Lf=0.0045, L=0.1219\n",
      "  ∇w = [0.03901, 0.47942]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.477, s1=0.982, t_h=1.685\n",
      "  L0=0.0003, L1=0.0731, Lf=0.0171, L=0.0819\n",
      "  ∇w = [-0.00558, 0.04064]\n",
      " Updated w: [0.1929002 0.8277963]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.273, s1=0.887, t_h=1.703\n",
      "  L0=0.0004, L1=0.0176, Lf=0.2471, L=0.1415\n",
      "  ∇w = [0.00582, 0.14021]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.156, s1=0.959, t_h=1.685\n",
      "  L0=0.0010, L1=0.0127, Lf=0.1176, L=0.0725\n",
      "  ∇w = [-0.00581, 0.04011]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.750, s1=0.795, t_h=1.703\n",
      "  L0=0.0112, L1=0.0781, Lf=0.0047, L=0.0917\n",
      "  ∇w = [0.03868, 0.37349]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.476, s1=0.976, t_h=1.685\n",
      "  L0=0.0003, L1=0.0708, Lf=0.0171, L=0.0797\n",
      "  ∇w = [-0.00618, 0.03975]\n",
      " Updated w: [0.18965049 0.76844043]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.271, s1=0.852, t_h=1.701\n",
      "  L0=0.0004, L1=0.0115, Lf=0.2457, L=0.1348\n",
      "  ∇w = [0.00521, 0.11349]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.155, s1=0.947, t_h=1.685\n",
      "  L0=0.0010, L1=0.0108, Lf=0.1176, L=0.0707\n",
      "  ∇w = [-0.00616, 0.03707]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.749, s1=0.743, t_h=1.701\n",
      "  L0=0.0111, L1=0.0589, Lf=0.0049, L=0.0724\n",
      "  ∇w = [0.03837, 0.30214]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.474, s1=0.971, t_h=1.685\n",
      "  L0=0.0003, L1=0.0689, Lf=0.0171, L=0.0778\n",
      "  ∇w = [-0.00675, 0.03903]\n",
      " Updated w: [0.18658432 0.71926742]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Sample 0: inp=[0.2 0.5], s0=0.270, s1=0.823, t_h=1.700\n",
      "  L0=0.0004, L1=0.0076, Lf=0.2450, L=0.1305\n",
      "  ∇w = [0.00465, 0.09347]\n",
      "Sample 1: inp=[0.1 0.8], s0=0.154, s1=0.937, t_h=1.685\n",
      "  L0=0.0011, L1=0.0094, Lf=0.1176, L=0.0693\n",
      "  ∇w = [-0.00648, 0.03461]\n",
      "Sample 2: inp=[0.7 0.3], s0=0.748, s1=0.702, t_h=1.699\n",
      "  L0=0.0110, L1=0.0457, Lf=0.0051, L=0.0592\n",
      "  ∇w = [0.03807, 0.25090]\n",
      "Sample 3: inp=[0.4 0.9], s0=0.473, s1=0.967, t_h=1.685\n",
      "  L0=0.0004, L1=0.0673, Lf=0.0171, L=0.0763\n",
      "  ∇w = [-0.00729, 0.03843]\n",
      " Updated w: [0.18368871 0.67752675]\n",
      "\n",
      "Final weights: [0.18368871 0.67752675]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) spike_timing + its derivative\n",
    "# -----------------------------------------------------------------------------\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - x**(1 - w) * np.log(x + eps)\n",
    "    else:\n",
    "        return - (1 - x)**(1 + w) * np.log(1 - x + eps)\n",
    "\n",
    "def dsigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s*(1 - s)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) mini_urd forward: returns hidden‐spike‐time only\n",
    "# -----------------------------------------------------------------------------\n",
    "def mini_urd(inputs, w):\n",
    "    n_input  = 2\n",
    "    n_hidden = 1\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    G = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        sr              : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v>1', reset='v=0', method='exact'\n",
    "    )\n",
    "    G.v = 0; G.sum = 0; G.sr = 0\n",
    "    G.global_clock = 0\n",
    "    G.scheduled_time = 1e9*second\n",
    "\n",
    "    stim = SpikeGeneratorGroup(2, indices=[0,1], times=inputs*ms)\n",
    "\n",
    "    # first layer has fixed identity weights\n",
    "    S1 = Synapses(stim, G[:2],\n",
    "        'layer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(1, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S1.connect(j='i')\n",
    "    S1.layer = 0\n",
    "\n",
    "    # trainable synapse 2→hidden\n",
    "    S2 = Synapses(G[:2], G[2:],\n",
    "        'w : 1\\nlayer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S2.connect()\n",
    "    S2.w = w\n",
    "    S2.layer = 1\n",
    "\n",
    "    # drive v when scheduled_time hits\n",
    "    G.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time)<0.0005*ms)*1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(G)\n",
    "    run(5*ms)\n",
    "\n",
    "    # return hidden spike time (or a large value if no spike)\n",
    "    ts = mon.spike_trains()[2]\n",
    "    return float(ts[0]/ms) if len(ts) else 5.0\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Training with multi‐loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_multi_loss(\n",
    "    X,                    # list of np.array([t0,t1])\n",
    "    t_hidden_targets,     # list of floats\n",
    "    t0_targets,           # list of floats for s0\n",
    "    t1_targets,           # list of floats for s1\n",
    "    w_init,\n",
    "    alpha=1.0, beta=1.0, gamma=1.0,\n",
    "    epochs=5, lr=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-loss:\n",
    "      L0 = ½ (s0 - t0)^2\n",
    "      L1 = ½ (s1 - t1)^2\n",
    "      Lf = ½ (t_h  - t_h*)^2\n",
    "      L = α L0 + β L1 + γ Lf\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n=== Epoch {ep+1}/{epochs} ===\")\n",
    "        for i, inp in enumerate(X):\n",
    "            # forward pass\n",
    "            t_h = mini_urd(inp, w)\n",
    "            # recompute s0,s1 exactly the same way Brian did\n",
    "            layer_h = 1\n",
    "            # each input is first spike, so sr_i=1, sum_i=0 → use that\n",
    "            s0 = spike_timing(w[0], inp[0], layer_h, 0, 1)\n",
    "            s1 = spike_timing(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # --- compute loss terms ---\n",
    "            t0_tgt = t0_targets[i]\n",
    "            t1_tgt = t1_targets[i]\n",
    "            th_tgt = t_hidden_targets[i]\n",
    "\n",
    "            L0 = 0.5*(s0 - t0_tgt)**2\n",
    "            L1 = 0.5*(s1 - t1_tgt)**2\n",
    "            Lf = 0.5*(t_h - th_tgt)**2\n",
    "            L  = alpha*L0 + beta*L1 + gamma*Lf\n",
    "\n",
    "            # --- gradients ---\n",
    "            # ∂L0/∂w0 = (s0 - t0)*∂s0/∂w0\n",
    "            dL0_dw0 = (s0 - t0_tgt) * d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1)\n",
    "            # ∂L1/∂w1\n",
    "            dL1_dw1 = (s1 - t1_tgt) * d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # ∂Lf/∂w0,w1 = ∂Lf/∂t_h × ∂t_h/∂sum × ∂sum/∂w_i\n",
    "            dLf_dt  = (t_h - th_tgt)\n",
    "            sum_tot = s0 + s1\n",
    "            sr = 2.0\n",
    "            z = sum_tot/sr\n",
    "            dt_dsum = dsigmoid(z)*(1/sr)\n",
    "            dsum_dw0 = d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1)\n",
    "            dsum_dw1 = d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1)\n",
    "            dLf_dw0 = dLf_dt * dt_dsum * dsum_dw0\n",
    "            dLf_dw1 = dLf_dt * dt_dsum * dsum_dw1\n",
    "\n",
    "            # combine\n",
    "            grad0 = alpha*dL0_dw0 + gamma*dLf_dw0\n",
    "            grad1 = beta *dL1_dw1 + gamma*dLf_dw1\n",
    "\n",
    "            # print & update\n",
    "            print(f\"Sample {i}: inp={inp}, s0={s0:.3f}, s1={s1:.3f}, t_h={t_h:.3f}\")\n",
    "            print(f\"  L0={L0:.4f}, L1={L1:.4f}, Lf={Lf:.4f}, L={L:.4f}\")\n",
    "            print(f\"  ∇w = [{grad0:.5f}, {grad1:.5f}]\")\n",
    "            w[0] -= lr * grad0\n",
    "            w[1] -= lr * grad1\n",
    "\n",
    "        print(\" Updated w:\", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Demo\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # toy data: 4 samples\n",
    "    X = [np.array([0.2,0.5]),\n",
    "         np.array([0.1,0.8]),\n",
    "         np.array([0.7,0.3]),\n",
    "         np.array([0.4,0.9])]\n",
    "\n",
    "    # main target: hidden spike at these ms\n",
    "    T_hidden = [1.0, 1.2, 1.8, 1.5]\n",
    "    # aux targets for each synapse\n",
    "    T0 = [0.3, 0.2, 0.6, 0.5]\n",
    "    T1 = [0.7, 0.8, 0.4, 0.6]\n",
    "\n",
    "    w0 = np.array([0.2, 1.0])\n",
    "    w_final = train_multi_loss(X, T_hidden, T0, T1, w0,\n",
    "                               alpha=1.0, beta=1.0, gamma=0.5,\n",
    "                               epochs=5, lr=0.1)\n",
    "\n",
    "    print(\"\\nFinal weights:\", w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea9011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fd3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c3410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6f7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module='brian2.codegen.generators.base')\n",
    "\n",
    "start_scope()\n",
    "\n",
    "defaultclock.dt = 0.0001*ms  \n",
    "\n",
    "# Custom timing function\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, result=1, sum=1, spikes_received=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received): \n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return (x ** (1 - w)) \n",
    "    else:\n",
    "        return (1 - (1 - x) ** (1 + w)) \n",
    "    \n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(layer=1, result=1, sum=1, spikes_received=1)\n",
    "def math1(layer, sum, spikes_received): \n",
    "    return (sum/spikes_received )+ layer\n",
    "\n",
    "def run_Urd(inputs, weights_1, weights_2, weights_3):\n",
    "    '''4-10-3 SNN'''\n",
    "    # will add check of weights # so it all works\n",
    "    n_input = 4 \n",
    "    n_hidden = 10\n",
    "    n_output = 3\n",
    "    n_total = n_input + n_hidden + n_output\n",
    "\n",
    "    neurons = NeuronGroup(n_total, '''\n",
    "        v : 1\n",
    "        sum : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time : second\n",
    "        global_clock : 1\n",
    "    ''', threshold='v > 1', reset='v = 0', method='exact')\n",
    "    neurons.v = 0\n",
    "    neurons.scheduled_time = 1e9 * second\n",
    "    neurons.global_clock = 0.0\n",
    "    neurons.sum = 0.0\n",
    "    neurons.spikes_received = 0.0\n",
    "\n",
    "\n",
    "    indicess = [i for i in range(n_input)]\n",
    "    stim = SpikeGeneratorGroup(n_input, indices=indicess, times=(inputs*ms))\n",
    "\n",
    "    syn_input = Synapses(stim, neurons[0:n_input], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    syn_input.connect(j='i')\n",
    "    syn_input.w = weights_1\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    syn_hidden = Synapses(neurons[0:n_input], neurons[n_input:n_input+n_hidden], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    for inp in range(n_input):\n",
    "        for hid in range(n_hidden):\n",
    "            syn_hidden.connect(i=inp, j=hid)\n",
    "\n",
    "    syn_hidden.w = weights_2\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "\n",
    "    syn_output = Synapses(\n",
    "        neurons[n_input:n_input+n_hidden], \n",
    "        neurons[n_input+n_hidden:n_total], \n",
    "        '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "        ''',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "        '''\n",
    "    )\n",
    "\n",
    "    for hid in range(n_hidden):\n",
    "        for out in range(n_output):\n",
    "            syn_output.connect(i=hid, j=out)\n",
    "\n",
    "    # Set weights in correct order\n",
    "    syn_output.w[:] = weights_3\n",
    "    syn_output.layer = 2\n",
    "\n",
    "    #print(syn_output.i[:], syn_output.j[:])\n",
    "    #weights_into_output_1 = weights_3[1::3]\n",
    "\n",
    "\n",
    "\n",
    "    neurons.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "\n",
    "    spikemon = SpikeMonitor(neurons)\n",
    "    \n",
    "    # neurons.v = 0\n",
    "    # neurons.scheduled_time = 1e9 * second\n",
    "    # neurons.global_clock = 0.0\n",
    "    # neurons.sum = 0.0\n",
    "    # neurons.spikes_received = 0.0\n",
    "\n",
    "    run(5*ms)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(n_total):\n",
    "        times = spikemon.spike_trains()[i]\n",
    "        if len(times) > 0:\n",
    "            result.append(round(times[0]/ms, 3))\n",
    "        else:\n",
    "            result.append(None)  # or some other placeholder like float('nan')\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ccc62bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14345227 0.86066371 0.88725555 0.85814195] [0.44777422 0.76353581 0.48259652 0.3623195  0.99852955 0.99073865\n",
      " 0.90408036 0.39032022 0.29293302 0.22087558 0.15663    0.32492888\n",
      " 0.56082809 0.25731051 0.04950147 0.28284291 0.23587225 0.44035401\n",
      " 0.39615996 0.35721367 0.89914801 0.44307031 0.63992119 0.52551554\n",
      " 0.98827489 0.05136685 0.22214933 0.74473358 0.12725664 0.83123581\n",
      " 0.98797027 0.54554983 0.49111424 0.37928825 0.2765512  0.63761703\n",
      " 0.15451435 0.66322082 0.94605562 0.18985174] [0.22203751 0.43385482 0.91329917 0.06779644 0.40236334 0.81143013\n",
      " 0.4022748  0.1185883  0.60056945 0.44718627 0.4049495  0.8794631\n",
      " 0.85964283 0.19174956 0.60335014 0.80864427 0.46067967 0.14843805\n",
      " 0.04586898 0.5629214  0.93889325 0.55204879 0.64219171 0.40427682\n",
      " 0.87504725 0.92320919 0.09731941 0.36464712 0.61217678 0.59806614]\n",
      "Epoch  0  Loss=0.2828  Outputs=[2.948, 2.955, 2.962]\n",
      "[0.13953711 0.86182532 0.88831144 0.85949145] [0.44743883 0.76336731 0.48216986 0.36189828 0.99909788 0.9911931\n",
      " 0.90395764 0.39029905 0.29326618 0.22012886 0.15599928 0.32419638\n",
      " 0.56023825 0.25677451 0.05653258 0.29145169 0.23518641 0.43993118\n",
      " 0.39599046 0.3567034  0.91026207 0.45410064 0.63883158 0.52466321\n",
      " 0.99408617 0.05810146 0.23281233 0.74405953 0.12696208 0.83034703\n",
      " 0.98733955 0.54481733 0.49048446 0.37874531 0.28337626 0.64570119\n",
      " 0.15382852 0.66279799 0.94588612 0.18934146] [0.21876973 0.4324101  0.91297917 0.06540616 0.40134529 0.81122318\n",
      " 0.39871252 0.11714358 0.6002385  0.4425356  0.40297812 0.87897976\n",
      " 0.85879033 0.19139106 0.60328364 0.80721844 0.46007717 0.14833071\n",
      " 0.04376921 0.56198734 0.93871481 0.54819101 0.64048565 0.40392365\n",
      " 0.86989742 0.92092319 0.09687291 0.35969729 0.60998122 0.5975577 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_loss(predicted, desired):\n",
    "    \"\"\"\n",
    "    MSE loss on the 3 output spike-times.\n",
    "    predicted: full list of 17 times (None → we treat as large penalty)\n",
    "    desired: array-like of length 3\n",
    "    \"\"\"\n",
    "    # pull out last 3 spikes\n",
    "    out_t = np.array([t if t is not None else 10.0 for t in predicted[-3:]])\n",
    "    d = np.array(desired)\n",
    "    return np.mean((out_t - d)**2)\n",
    "\n",
    "def finite_difference_grads(inputs, w1, w2, w3, desired, eps=3):\n",
    "    \"\"\"\n",
    "    Returns numerical gradients of loss wrt each weight array.\n",
    "    \"\"\"\n",
    "    # baseline loss\n",
    "    base_out = run_Urd(inputs, w1, w2, w3)\n",
    "    L0 = compute_loss(base_out, desired)\n",
    "\n",
    "    # grad for layer1\n",
    "    gw1 = np.zeros_like(w1)\n",
    "    for idx in np.ndindex(w1.shape):\n",
    "        w1p = w1.copy()\n",
    "        w1p[idx] += eps\n",
    "        Lp = compute_loss(run_Urd(inputs, w1p, w2, w3), desired)\n",
    "        gw1[idx] = (Lp - L0)/eps\n",
    "\n",
    "    # grad for layer2\n",
    "    gw2 = np.zeros_like(w2)\n",
    "    for idx in np.ndindex(w2.shape):\n",
    "        w2p = w2.copy()\n",
    "        w2p[idx] += eps\n",
    "        Lp = compute_loss(run_Urd(inputs, w1, w2p, w3), desired)\n",
    "        gw2[idx] = (Lp - L0)/eps\n",
    "\n",
    "    # grad for layer3\n",
    "    gw3 = np.zeros_like(w3)\n",
    "    for idx in np.ndindex(w3.shape):\n",
    "        w3p = w3.copy()\n",
    "        w3p[idx] += eps\n",
    "        Lp = compute_loss(run_Urd(inputs, w1, w2, w3p), desired)\n",
    "        gw3[idx] = (Lp - L0)/eps\n",
    "\n",
    "    return gw1, gw2, gw3, L0\n",
    "\n",
    "def backprop_snn_fd(inputs, w1, w2, w3,\n",
    "                    desired=[2.1, 2.6, 2.9],\n",
    "                    lr=0.1, eps=3):\n",
    "    \"\"\"\n",
    "    One gradient‐step on the SNN via finite‐difference.\n",
    "    Returns updated (w1, w2, w3) and the loss before update.\n",
    "    \"\"\"\n",
    "    gw1, gw2, gw3, loss = finite_difference_grads(inputs, w1, w2, w3, desired, eps)\n",
    "    # gradient descent\n",
    "    w1 -= lr * gw1\n",
    "    w2 -= lr * gw2\n",
    "    w3 -= lr * gw3\n",
    "    return w1, w2, w3, loss\n",
    "\n",
    "# — example usage —\n",
    "# random init\n",
    "inputs    = np.random.uniform(0, 1, 4)\n",
    "weights_1 = np.random.uniform(0, 1, 4)\n",
    "weights_2 = np.random.uniform(0, 1, 40)\n",
    "weights_3 = np.random.uniform(0, 1, 30)\n",
    "\n",
    "print(weights_1, weights_2, weights_3)\n",
    "\n",
    "desired = [2.1, 2.6, 2.9]\n",
    "for epoch in range(1):\n",
    "    w1, w2, w3, loss = backprop_snn_fd(\n",
    "        inputs, weights_1, weights_2, weights_3,\n",
    "        desired=desired, lr=0.5, eps=3\n",
    "    )\n",
    "    weights_1, weights_2, weights_3 = w1, w2, w3\n",
    "    out = run_Urd(inputs, w1, w2, w3)[-3:]\n",
    "    print(f\"Epoch {epoch:2d}  Loss={loss:.4f}  Outputs={out}\")\n",
    "\n",
    "print(weights_1, weights_2, weights_3)\n",
    "\n",
    "# After a few epochs you should see the 3 output times marching\n",
    "# closer to [2.1, 2.6, 2.9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 – Avg spikes: 0.00\n",
      "Epoch 2/20 – Avg spikes: 0.00\n",
      "Epoch 3/20 – Avg spikes: 0.00\n",
      "Epoch 4/20 – Avg spikes: 0.00\n",
      "Epoch 5/20 – Avg spikes: 0.00\n",
      "Epoch 6/20 – Avg spikes: 0.00\n",
      "Epoch 7/20 – Avg spikes: 0.00\n",
      "Epoch 8/20 – Avg spikes: 0.00\n",
      "Epoch 9/20 – Avg spikes: 0.00\n",
      "Epoch 10/20 – Avg spikes: 0.00\n",
      "Epoch 11/20 – Avg spikes: 0.00\n",
      "Epoch 12/20 – Avg spikes: 0.00\n",
      "Epoch 13/20 – Avg spikes: 0.00\n",
      "Epoch 14/20 – Avg spikes: 0.00\n",
      "Epoch 15/20 – Avg spikes: 0.00\n",
      "Epoch 16/20 – Avg spikes: 0.00\n",
      "Epoch 17/20 – Avg spikes: 0.00\n",
      "Epoch 18/20 – Avg spikes: 0.00\n",
      "Epoch 19/20 – Avg spikes: 0.00\n",
      "Epoch 20/20 – Avg spikes: 0.00\n",
      "Final first-spike times (ms): [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
