{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b7c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module='brian2.codegen.generators.base')\n",
    "\n",
    "start_scope()\n",
    "\n",
    "defaultclock.dt = 0.0001*ms  \n",
    "\n",
    "# Custom timing function\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, result=1, sum=1, spikes_received=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received): \n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return (x ** (1 - w)) \n",
    "    else:\n",
    "        return (1 - (1 - x) ** (1 + w)) \n",
    "    \n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(layer=1, result=1, sum=1, spikes_received=1)\n",
    "def math1(layer, sum, spikes_received): \n",
    "    return (sum/spikes_received )+ layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cdc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved gradient descent for SNN: ensure gradients are calculated and weights updated correctly\n",
    "\n",
    "def improved_update_last_layer_weights(spike_times, desired_outputs, w_3, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Update output weights using softmax + cross-entropy.\n",
    "    w_3: shape 30 (flattened 10 hidden * 3 outputs)\n",
    "    \"\"\"\n",
    "    hidden_activations = np.array(spike_times[4:14])   # 10 hidden neurons\n",
    "    # Build weight matrix (3 classes x 10 hidden units)\n",
    "    weights = np.zeros((3, 10))\n",
    "    for out_idx in range(3):\n",
    "        weights[out_idx] = w_3[out_idx::3]\n",
    "\n",
    "    # Forward Pass: logits and softmax\n",
    "    logits = weights @ hidden_activations\n",
    "    probs = np.exp(logits - np.max(logits))\n",
    "    probs /= np.sum(probs)\n",
    "\n",
    "    # Loss (for monitoring)\n",
    "    loss = -np.sum(np.array(desired_outputs) * np.log(probs + 1e-12))\n",
    "\n",
    "    # Gradient: dL/dz = softmax_output - target\n",
    "    grad_logits = probs - np.array(desired_outputs)\n",
    "\n",
    "    # Gradient: dL/dW = outer(grad_logits, hidden_activations)\n",
    "    grad_weights = np.outer(grad_logits, hidden_activations)\n",
    "\n",
    "    # Weight Update\n",
    "    weights -= learning_rate * grad_weights\n",
    "\n",
    "    # Clip (optional)\n",
    "    weights = np.clip(weights, -5.0, 5.0)\n",
    "\n",
    "    # Flatten back into w_3 list (stride write-back)\n",
    "    for out_idx in range(3):\n",
    "        for i in range(10):\n",
    "            w_3[out_idx + i * 3] = weights[out_idx, i]\n",
    "\n",
    "    # Diagnostics\n",
    "    print(\"Updated weights (first 6):\", w_3[:6])\n",
    "    print(\"Gradient signs:\", np.sign(grad_weights))\n",
    "    print(\"Softmax probabilities:\", probs)\n",
    "    print(\"Cross-entropy loss:\", loss)\n",
    "    return w_3\n",
    "\n",
    "# Example usage:\n",
    "# w_3 = improved_update_last_layer_weights(var, wanted_output, w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d72e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Urd, Verdande, Skuld \n",
    "\n",
    "# did not apply the specficyed weifhts to the first layer yet will maybe do after\n",
    "def run_Urd(inputs, weights_1, weights_2, weights_3):\n",
    "    '''4-10-3 SNN'''\n",
    "    # will add check of weights # so it all works\n",
    "    n_input = 4 \n",
    "    n_hidden = 10\n",
    "    n_output = 3\n",
    "    n_total = n_input + n_hidden + n_output\n",
    "\n",
    "    neurons = NeuronGroup(n_total, '''\n",
    "        v : 1\n",
    "        sum : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time : second\n",
    "        global_clock : 1\n",
    "    ''', threshold='v > 1', reset='v = 0', method='exact')\n",
    "    neurons.v = 0\n",
    "    neurons.scheduled_time = 1e9 * second\n",
    "    neurons.global_clock = 0.0\n",
    "    neurons.sum = 0.0\n",
    "    neurons.spikes_received = 0.0\n",
    "\n",
    "\n",
    "    indicess = [i for i in range(n_input)]\n",
    "    stim = SpikeGeneratorGroup(n_input, indices=indicess, times=(inputs*ms))\n",
    "\n",
    "    syn_input = Synapses(stim, neurons[0:n_input], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    syn_input.connect(j='i')\n",
    "    syn_input.w = weights_1\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    syn_hidden = Synapses(neurons[0:n_input], neurons[n_input:n_input+n_hidden], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    for inp in range(n_input):\n",
    "        for hid in range(n_hidden):\n",
    "            syn_hidden.connect(i=inp, j=hid)\n",
    "\n",
    "    syn_hidden.w = weights_2\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "\n",
    "    syn_output = Synapses(\n",
    "        neurons[n_input:n_input+n_hidden], \n",
    "        neurons[n_input+n_hidden:n_total], \n",
    "        '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "        ''',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "        '''\n",
    "    )\n",
    "\n",
    "    for hid in range(n_hidden):\n",
    "        for out in range(n_output):\n",
    "            syn_output.connect(i=hid, j=out)\n",
    "\n",
    "    # Set weights in correct order\n",
    "    syn_output.w[:] = weights_3\n",
    "    syn_output.layer = 2\n",
    "\n",
    "    #print(syn_output.i[:], syn_output.j[:])\n",
    "    #weights_into_output_1 = weights_3[1::3]\n",
    "\n",
    "\n",
    "\n",
    "    neurons.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "\n",
    "    spikemon = SpikeMonitor(neurons)\n",
    "    \n",
    "    # neurons.v = 0\n",
    "    # neurons.scheduled_time = 1e9 * second\n",
    "    # neurons.global_clock = 0.0\n",
    "    # neurons.sum = 0.0\n",
    "    # neurons.spikes_received = 0.0\n",
    "\n",
    "    run(5*ms)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(n_total):\n",
    "        times = spikemon.spike_trains()[i]\n",
    "        if len(times) > 0:\n",
    "            result.append(round(times[0]/ms, 3))\n",
    "        else:\n",
    "            result.append(None)  # or some other placeholder like float('nan')\n",
    "            \n",
    "    return result\n",
    "\n",
    "def real_outputs(var):\n",
    "    return [round(var[-3 + i] - 2, 4) if var[-3 + i] is not None else 0.0 for i in range(3)]\n",
    "\n",
    "# def calc_cost(outputs, desired_outputs):\n",
    "#     return 0.5 * ((outputs - desired_outputs) ** 2)\n",
    "\n",
    "\n",
    "# # --- Initialization ---\n",
    "# w_1 = np.random.uniform(0.05, 0.95, size=4).tolist()      # input to hidden\n",
    "# w_2 = np.random.uniform(-0.95, 0.95, size=40).tolist()     # hidden to next layer (10 hidden * 4 inputs)\n",
    "# w_3 = np.random.uniform(-0.95, 0.95, size=30).tolist()     # next layer to output (10 hidden * 3 outputs)\n",
    "\n",
    "\n",
    "# def softmax(x):\n",
    "#     exps = np.exp(x - np.max(x))  # stability fix\n",
    "#     return exps / np.sum(exps)\n",
    "\n",
    "# def cross_entropy_loss(predicted_probs, target_one_hot):\n",
    "#     return -np.sum(target_one_hot * np.log(predicted_probs + 1e-12))  # avoid log(0)\n",
    "\n",
    "# def update_output_weights_softmax(hidden_activations, weights, target_one_hot, learning_rate=0.1):\n",
    "#     \"\"\"\n",
    "#     hidden_activations: shape (n_hidden,)\n",
    "#     weights: shape (n_classes, n_hidden)\n",
    "#     target_one_hot: shape (n_classes,) - one-hot encoded desired output\n",
    "#     \"\"\"\n",
    "#     # Forward pass\n",
    "#     logits = weights @ hidden_activations  # shape (n_classes,)\n",
    "#     probs = softmax(logits)                # shape (n_classes,)\n",
    "\n",
    "#     # Compute loss (optional, for monitoring)\n",
    "#     loss = cross_entropy_loss(probs, target_one_hot)\n",
    "\n",
    "#     # Compute gradient of loss w.r.t. logits\n",
    "#     grad_logits = probs - target_one_hot  # shape (n_classes,)\n",
    "\n",
    "#     # Compute gradient of loss w.r.t. weights\n",
    "#     grad_weights = np.outer(grad_logits, hidden_activations)  # shape (n_classes, n_hidden)\n",
    "\n",
    "#     # Update weights\n",
    "#     weights -= learning_rate * grad_weights\n",
    "\n",
    "#     # Optional: clip to avoid explosion\n",
    "#     weights = np.clip(weights, -5.0, 5.0)\n",
    "\n",
    "#     return weights, probs, loss\n",
    "\n",
    "# # # --- Target Output Generator ---\n",
    "# # def make_target_output(label, high=5.0, low=0.05):\n",
    "# #     target = [low, low, low]\n",
    "# #     target[label] = high\n",
    "# #     return target\n",
    "\n",
    "# # # --- Softmax Function \n",
    "# # def softmax(x):\n",
    "# #     e_x = np.exp(x - np.max(x))\n",
    "# #     return e_x / np.sum(e_x)\n",
    "\n",
    "\n",
    "# # def cross_entropy_loss(predicted_probs, target_one_hot):\n",
    "# #     return -np.sum(target_one_hot * np.log(predicted_probs + 1e-12))  # avoid log(0)\n",
    "\n",
    "\n",
    "# # # --- Update Weights into Output Neuron ---\n",
    "# # def update_weights_input_neuron(hidden_activations, weights, actual_output, desired_output, learning_rate=0.1):\n",
    "# #     error = actual_output - desired_output\n",
    "# #     gradients = error * hidden_activations\n",
    "\n",
    "# #     signs = np.sign(gradients)\n",
    "# #     sign_str = ', '.join([f\"{g:+.3f}\" for g in gradients])\n",
    "# #     print(f\"Gradient signs: [{sign_str}] (Error: {error:+.3f})\")\n",
    "# #     print(\"Hidden activations:\", hidden_activations)\n",
    "# #     print(\"Errors:\", actual_output - desired_output)\n",
    "\n",
    "    \n",
    "# #     updated_weights = weights - learning_rate * gradients\n",
    "# #     updated_weights = np.clip(updated_weights, -5.0, 5.0)\n",
    "# #     return updated_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # --- Update Hidden Neuron Weights ---\n",
    "# # def update_single_hidden_neuron_weights(input_activations, weights_to_hidden, downstream_weights, output_errors, hidden_activation, learning_rate=0.1):\n",
    "# #     propagated_error = np.dot(output_errors, downstream_weights)\n",
    "# #     derivative = hidden_activation * (1 - hidden_activation)\n",
    "# #     local_gradient = propagated_error * derivative\n",
    "# #     gradients = local_gradient * input_activations\n",
    "# #     updated_weights = weights_to_hidden - learning_rate * gradients\n",
    "# #     updated_weights = np.clip(updated_weights, -0.95, 0.95)\n",
    "# #     return updated_weights\n",
    "\n",
    "# # # --- Update Hidden Layer ---\n",
    "# # def update_hidden_layer_weights(spike_times, output_errors):\n",
    "# #     lr = 0.1\n",
    "# #     input_activations = np.array(spike_times[:4])\n",
    "# #     hidden_activations = np.array(spike_times[4:14])\n",
    "\n",
    "# #     for i in range(10):\n",
    "# #         start_idx = i * 4\n",
    "# #         end_idx = (i + 1) * 4\n",
    "\n",
    "# #         weights_to_hidden_i = w_2[start_idx:end_idx]\n",
    "# #         hidden_activation = hidden_activations[i]\n",
    "\n",
    "# #         downstream_weights = np.array([\n",
    "# #             w_3[i * 3 + 0],\n",
    "# #             w_3[i * 3 + 1],\n",
    "# #             w_3[i * 3 + 2],\n",
    "# #         ])\n",
    "\n",
    "# #         updated_weights = update_single_hidden_neuron_weights(\n",
    "# #             input_activations,\n",
    "# #             np.array(weights_to_hidden_i),\n",
    "# #             downstream_weights,\n",
    "# #             output_errors,\n",
    "# #             hidden_activation,\n",
    "# #             learning_rate=lr\n",
    "# #         ) \n",
    "\n",
    "# #         w_2[start_idx:end_idx] = updated_weights\n",
    "\n",
    "\n",
    "# def update_last_layer_weights(spike_times, desired_outputs, learning_rate=0.1):\n",
    "#     \"\"\"\n",
    "#     Update output weights using softmax + cross-entropy.\n",
    "#     `w_3`: shape 30 (flattened 10 hidden * 3 outputs)\n",
    "#     \"\"\"\n",
    "#     hidden_activations = np.array(spike_times[4:14])   # 10 hidden neurons\n",
    "#     actual_outputs = np.array(spike_times[-3:])        # old output activations (can ignore for this method)\n",
    "\n",
    "#     # --- 1. Build weight matrix (3 classes x 10 hidden units)\n",
    "#     weights = np.zeros((3, 10))\n",
    "#     for out_idx in range(3):\n",
    "#         weights[out_idx] = w_3[out_idx::3]\n",
    "\n",
    "#     # --- 2. Forward Pass: logits and softmax\n",
    "#     logits = weights @ hidden_activations               # shape (3,)\n",
    "#     probs = softmax(logits)\n",
    "\n",
    "#     # --- 3. Loss (optional: monitor training)\n",
    "#     loss = cross_entropy_loss(probs, desired_outputs)\n",
    "\n",
    "#     # --- 4. Gradient: dL/dz = softmax_output - target\n",
    "#     grad_logits = probs - np.array(desired_outputs)     # shape (3,)\n",
    "\n",
    "#     # --- 5. Gradient: dL/dW = outer(grad_logits, hidden_activations)\n",
    "#     grad_weights = np.outer(grad_logits, hidden_activations)  # shape (3, 10)\n",
    "\n",
    "#     # --- 6. Weight Update\n",
    "#     weights -= learning_rate * grad_weights\n",
    "\n",
    "#     # --- 7. Clip (optional)\n",
    "#     weights = np.clip(weights, -5.0, 5.0)\n",
    "\n",
    "#     # --- 8. Flatten back into w_3 list (stride write-back)\n",
    "#     for out_idx in range(3):\n",
    "#         for i in range(10):\n",
    "#             w_3[out_idx + i * 3] = weights[out_idx, i]\n",
    "\n",
    "#     # --- 9. Optional: Debug prints\n",
    "#     grad_signs = np.sign(grad_weights)\n",
    "#     for i in range(3):\n",
    "#         sign_str = ', '.join(f\"{v:+.3f}\" for v in grad_weights[i])\n",
    "#         print(f\"Gradient signs (class {i}): [{sign_str}] (Error: {grad_logits[i]:+.3f})\")\n",
    "#     print(\"Hidden activations:\", hidden_activations)\n",
    "#     print(\"Softmax probabilities:\", probs)\n",
    "#     print(\"Cross-entropy loss:\", loss)\n",
    "\n",
    "\n",
    "# # --- Update Output Layer ---\n",
    "# # def update_last_layer_weights(spike_times, desired_outputs, learning_rate=0.1):\n",
    "# #     hidden_activations = np.array(spike_times[4:14])\n",
    "# #     actual_outputs = np.array(spike_times[-3:])\n",
    "\n",
    "# #     weights_0 = np.array(w_3[0::3])\n",
    "\n",
    "\n",
    "    \n",
    "# #     weights_1 = np.array(w_3[1::3])\n",
    "# #     weights_2 = np.array(w_3[2::3])\n",
    "\n",
    "# #     updated_0 = update_weights_input_neuron(hidden_activations, weights_0, actual_outputs[0], desired_outputs[0], learning_rate)\n",
    "# #     updated_1 = update_weights_input_neuron(hidden_activations, weights_1, actual_outputs[1], desired_outputs[1], learning_rate)\n",
    "# #     updated_2 = update_weights_input_neuron(hidden_activations, weights_2, actual_outputs[2], desired_outputs[2], learning_rate)\n",
    "\n",
    "# #     w_3[0::3] = updated_0\n",
    "# #     w_3[1::3] = updated_1\n",
    "# #     w_3[2::3] = updated_2\n",
    "\n",
    "\n",
    "# # def update_weights_input_neuron(hidden_activations, weights, actual_output, desired_output, learning_rate=0.1):\n",
    "\n",
    "# #     \"\"\"\n",
    "# #     Update all weights going into a single output neuron using gradient descent.\n",
    "\n",
    "# #     Args:\n",
    "# #         hidden_activations (np.ndarray): Activations from hidden neurons (shape: [n_hidden]).\n",
    "# #         weights (np.ndarray): Current weights into the output neuron (shape: [n_hidden]).\n",
    "# #         actual_output (float): Current output of this neuron.\n",
    "# #         desired_output (float): Target output for this neuron.\n",
    "# #         learning_rate (float): Learning rate.\n",
    "\n",
    "# #     Returns:\n",
    "# #         np.ndarray: Updated weights (shape: [n_hidden]).\n",
    "# #     \"\"\"\n",
    "# #     error = actual_output - desired_output\n",
    "# #     gradients = error * hidden_activations\n",
    "# #     updated_weights = weights - learning_rate * gradients\n",
    "# #     updated_weights = np.clip(updated_weights, -0.95, 0.95)\n",
    "# #     return updated_weights\n",
    "\n",
    "\n",
    "# # def update_single_hidden_neuron_weights(input_activations, weights_to_hidden, downstream_weights, output_errors, hidden_activation, learning_rate=0.1):\n",
    "# #     \"\"\"\n",
    "# #     Updates the weights into a single hidden neuron.\n",
    "\n",
    "# #     Args:\n",
    "# #         input_activations (np.ndarray): Input activations (shape: [n_input]).\n",
    "# #         weights_to_hidden (np.ndarray): Weights into this hidden neuron (shape: [n_input]).\n",
    "# #         downstream_weights (np.ndarray): Weights from this hidden neuron to each output neuron (shape: [n_output]).\n",
    "# #         output_errors (np.ndarray): Errors at each output neuron (shape: [n_output]).\n",
    "# #         hidden_activation (float): Activation value of this hidden neuron.\n",
    "# #         learning_rate (float): Learning rate.\n",
    "\n",
    "# #     Returns:\n",
    "# #         np.ndarray: Updated weights for this hidden neuron (shape: [n_input]).\n",
    "# #     \"\"\"\n",
    "# #     propagated_error = np.dot(output_errors, downstream_weights)\n",
    "# #     derivative = hidden_activation * (1 - hidden_activation)\n",
    "# #     local_gradient = propagated_error * derivative\n",
    "# #     gradients = local_gradient * input_activations\n",
    "# #     updated_weights = weights_to_hidden - learning_rate * gradients\n",
    "# #     updated_weights = np.clip(updated_weights, -0.95, 0.95)\n",
    "# #     return updated_weights\n",
    "\n",
    "\n",
    "# # def update_hidden_layer_weights(spike_times, output_errors):\n",
    "# #     lr = 0.1 # learning rate will need to universalize later\n",
    "\n",
    "# #     input_activations = np.array(spike_times[:4])\n",
    "# #     hidden_activations = np.array(spike_times[4:14])\n",
    "\n",
    "# #     for i in range(10):  # 10 hidden neurons\n",
    "# #         start_idx = i * 4\n",
    "# #         end_idx = (i + 1) * 4\n",
    "\n",
    "# #         weights_to_hidden_i = w_2[start_idx:end_idx]\n",
    "# #         hidden_activation = hidden_activations[i]\n",
    "\n",
    "# #         # FIXED: Correct indexing for downstream weights\n",
    "# #         # Each hidden neuron i connects to all 3 outputs with weights at positions:\n",
    "# #         # i*3, i*3+1, i*3+2\n",
    "# #         downstream_weights = np.array([\n",
    "# #             w_3[i * 3 + 0],  # weight from hidden neuron i to output 0\n",
    "# #             w_3[i * 3 + 1],  # weight from hidden neuron i to output 1  \n",
    "# #             w_3[i * 3 + 2],  # weight from hidden neuron i to output 2\n",
    "# #         ])\n",
    "        \n",
    "# #         # Debug: Print the indices being used\n",
    "# #         #print(f\"  Weight indices for neuron {i}: [{i*3}, {i*3+1}, {i*3+2}]\")\n",
    "# #         #print(f\"  w_3 values at those indices: {w_3[i*3:i*3+3]}\")\n",
    "\n",
    "# #         propagated_error = np.dot(output_errors, downstream_weights)\n",
    "# #         derivative = hidden_activation * (1 - hidden_activation)\n",
    "# #         local_gradient = propagated_error * derivative\n",
    "# #         gradients = local_gradient * input_activations\n",
    "\n",
    "# #         # print(f\"\\nHidden Neuron {i}\")\n",
    "# #         # print(f\"  Hidden Activation: {hidden_activation}\")\n",
    "# #         # print(f\"  Downstream Weights: {downstream_weights}\")\n",
    "# #         # print(f\"  Propagated Error: {propagated_error}\")\n",
    "# #         # print(f\"  Local Gradient: {local_gradient}\")\n",
    "# #         # print(f\"  Gradients: {gradients}\")\n",
    "\n",
    "# #         updated_weights = weights_to_hidden_i - lr * gradients\n",
    "# #         updated_weights = np.clip(updated_weights, -0.95, 0.95)\n",
    "# #         w_2[start_idx:end_idx] = updated_weights\n",
    "\n",
    "# #     # print(\"hidden layer weights updated\")\n",
    "\n",
    "\n",
    "# # def update_last_layer_weights(spike_times, desired_outputs):\n",
    "\n",
    "# #     lr = 0.1 # will make more global later\n",
    "    \n",
    "# #     var = spike_times\n",
    "# #     hidden_activations = np.array(var[4:14])\n",
    "\n",
    "\n",
    "# #     weights_to_output_0 = w_3[0::3]\n",
    "# #     weights_to_output_1 = w_3[1::3]\n",
    "# #     weights_to_output_2 = w_3[2::3]\n",
    "\n",
    "# #     actual_output_0 = var[-3]\n",
    "# #     actual_output_1 = var[-2]\n",
    "# #     actual_output_2 = var[-1]\n",
    "\n",
    "\n",
    "# #     desired_output_0 = desired_outputs[0]\n",
    "# #     desired_output_1 = desired_outputs[1]\n",
    "# #     desired_output_2 = desired_outputs[2]\n",
    "\n",
    "\n",
    "# #     new_weights_0 = update_weights_input_neuron(\n",
    "# #         hidden_activations,\n",
    "# #         weights_to_output_0,\n",
    "# #         actual_output_0,\n",
    "# #         desired_output_0,\n",
    "# #         learning_rate=lr\n",
    "# #     )\n",
    "\n",
    "# #     new_weights_1 = update_weights_input_neuron(\n",
    "# #         hidden_activations,\n",
    "# #         weights_to_output_1,\n",
    "# #         actual_output_1,\n",
    "# #         desired_output_1,\n",
    "# #         learning_rate=lr\n",
    "# #     )\n",
    "\n",
    "# #     new_weights_2 = update_weights_input_neuron(\n",
    "# #         hidden_activations,\n",
    "# #         weights_to_output_2,\n",
    "# #         actual_output_2,\n",
    "# #         desired_output_2,\n",
    "# #         learning_rate=lr\n",
    "# #     )\n",
    "\n",
    "# #     w_3[0::3] = new_weights_0\n",
    "# #     w_3[1::3] = new_weights_1\n",
    "# #     w_3[2::3] = new_weights_2\n",
    "\n",
    "# #     print(\"weights updated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410facb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46936208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# inputs = [0.1, 0.2, 0.5, 0.9]\n",
    "# wanted_output = [2.1, 2.1, 2.9]\n",
    "\n",
    "# Initialize weights if not already\n",
    "# w_1 = np.random.uniform(0.05, 0.95, size=4)\n",
    "# w_2 = np.random.uniform(-0.95, 0.95, size=40)\n",
    "# w_3 = np.random.uniform(-0.95, 0.95, size=30)\n",
    "\n",
    "def real_outputs(var):\n",
    "    return [round(var[-3 + i] - 2, 4) if var[-3 + i] is not None else 0.0 for i in range(3)]\n",
    "\n",
    "def get_output_errors(var, wanted_output):\n",
    "    return np.array([var[-3 + i] - wanted_output[i] for i in range(3)])\n",
    "\n",
    "\n",
    "# print(w_2)\n",
    "# for i in range(5):\n",
    "#     try:\n",
    "#         var = run_Urd(inputs, w_1, w_2, w_3)\n",
    "\n",
    "#         print(f\"Iter {i} Output:\", real_outputs(var))\n",
    "\n",
    "#         update_last_layer_weights(var, wanted_output)\n",
    "\n",
    "#         # Compute error for hidden layer backprop\n",
    "#         output_errors = get_output_errors(var, wanted_output)\n",
    "\n",
    "#         update_hidden_layer_weights(var, output_errors)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in iteration {i}: {e}\")\n",
    "\n",
    "# # Final output\n",
    "# print(\"Final Output:\", real_outputs(var))\n",
    "# print(w_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5659360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy_loss(predicted_probs, target_one_hot):\n",
    "    \"\"\"Cross-entropy loss with numerical stability\"\"\"\n",
    "    return -np.sum(target_one_hot * np.log(predicted_probs + 1e-12))\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class SNNBackpropagation:\n",
    "    def __init__(self, w_1, w_2, w_3, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize backpropagation for SNN\n",
    "        w_1: input weights (4,) - weights from input to first layer\n",
    "        w_2: hidden weights (40,) - weights from input (4) to hidden (10) - flattened\n",
    "        w_3: output weights (30,) - weights from hidden (10) to output (3) - flattened\n",
    "        \"\"\"\n",
    "        self.w_1 = np.array(w_1)\n",
    "        self.w_2 = np.array(w_2).reshape(10, 4)  # 10 hidden neurons, 4 inputs each\n",
    "        self.w_3 = np.array(w_3).reshape(3, 10)  # 3 output neurons, 10 hidden inputs each\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def forward_pass(self, spike_times):\n",
    "        \"\"\"\n",
    "        Forward pass to compute activations\n",
    "        spike_times: [input_spikes(4), hidden_spikes(10), output_spikes(3)]\n",
    "        \"\"\"\n",
    "        self.input_activations = np.array(spike_times[:4])\n",
    "        self.hidden_activations = np.array(spike_times[4:14])\n",
    "        self.output_activations = np.array(spike_times[-3:])\n",
    "        \n",
    "        # Compute logits and probabilities for output layer\n",
    "        self.output_logits = self.w_3 @ self.hidden_activations\n",
    "        self.output_probs = softmax(self.output_logits)\n",
    "        \n",
    "        return self.output_probs\n",
    "    \n",
    "    def backward_pass(self, target_one_hot):\n",
    "        \"\"\"\n",
    "        Backward pass to compute gradients and update weights\n",
    "        target_one_hot: one-hot encoded target output [0, 1, 0] format\n",
    "        \"\"\"\n",
    "        # === OUTPUT LAYER GRADIENTS ===\n",
    "        # Gradient of loss w.r.t. output logits (softmax + cross-entropy)\n",
    "        output_grad = self.output_probs - target_one_hot  # shape (3,)\n",
    "        \n",
    "        # Gradient of loss w.r.t. output weights\n",
    "        grad_w3 = np.outer(output_grad, self.hidden_activations)  # shape (3, 10)\n",
    "        \n",
    "        # === HIDDEN LAYER GRADIENTS ===\n",
    "        # Backpropagate error to hidden layer\n",
    "        hidden_error = self.w_3.T @ output_grad  # shape (10,)\n",
    "        \n",
    "        # For spike-based networks, we need to approximate the derivative\n",
    "        # Using sigmoid approximation for spike timing derivative\n",
    "        hidden_grad = hidden_error * sigmoid_derivative(self.hidden_activations)  # shape (10,)\n",
    "        \n",
    "        # Gradient of loss w.r.t. hidden weights\n",
    "        grad_w2 = np.outer(hidden_grad, self.input_activations)  # shape (10, 4)\n",
    "        \n",
    "        # === INPUT LAYER GRADIENTS (if needed) ===\n",
    "        # Backpropagate to input layer (usually not updated in classification)\n",
    "        input_error = self.w_2.T @ hidden_grad  # shape (4,)\n",
    "        input_grad = input_error * sigmoid_derivative(self.input_activations)\n",
    "        \n",
    "        # === WEIGHT UPDATES ===\n",
    "        self.w_3 -= self.lr * grad_w3\n",
    "        self.w_2 -= self.lr * grad_w2\n",
    "        # self.w_1 -= self.lr * input_grad  # Uncomment if you want to update input weights\n",
    "        \n",
    "        # Clip weights to prevent explosion\n",
    "        self.w_3 = np.clip(self.w_3, -5.0, 5.0)\n",
    "        self.w_2 = np.clip(self.w_2, -5.0, 5.0)\n",
    "        self.w_1 = np.clip(self.w_1, -5.0, 5.0)\n",
    "        \n",
    "        return {\n",
    "            'output_grad': output_grad,\n",
    "            'hidden_grad': hidden_grad,\n",
    "            'input_grad': input_grad,\n",
    "            'grad_w3': grad_w3,\n",
    "            'grad_w2': grad_w2\n",
    "        }\n",
    "    \n",
    "    def get_flattened_weights(self):\n",
    "        \"\"\"Return weights in original flattened format for compatibility\"\"\"\n",
    "        return {\n",
    "            'w_1': self.w_1.tolist(),\n",
    "            'w_2': self.w_2.flatten().tolist(),\n",
    "            'w_3': self.w_3.flatten().tolist()\n",
    "        }\n",
    "    \n",
    "    def train_step(self, spike_times, target_one_hot, verbose=True):\n",
    "        \"\"\"Complete training step: forward + backward + diagnostics\"\"\"\n",
    "        # Forward pass\n",
    "        probs = self.forward_pass(spike_times)\n",
    "        loss = cross_entropy_loss(probs, target_one_hot)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = self.backward_pass(target_one_hot)\n",
    "        \n",
    "        # Diagnostics\n",
    "        if verbose:\n",
    "            print(f\"Input activations: {self.input_activations}\")\n",
    "            print(f\"Hidden activations: {self.hidden_activations}\")\n",
    "            print(f\"Output probabilities: {probs}\")\n",
    "            print(f\"Cross-entropy loss: {loss}\")\n",
    "            \n",
    "            # Print gradient information\n",
    "            print(f\"Output gradients: {gradients['output_grad']}\")\n",
    "            print(f\"Hidden gradients mean: {np.mean(np.abs(gradients['hidden_grad'])):.4f}\")\n",
    "            print(f\"Weight update magnitudes:\")\n",
    "            print(f\"  W3: {np.mean(np.abs(gradients['grad_w3'])):.4f}\")\n",
    "            print(f\"  W2: {np.mean(np.abs(gradients['grad_w2'])):.4f}\")\n",
    "        \n",
    "        return loss, probs\n",
    "\n",
    "# Enhanced training function\n",
    "def train_snn_with_full_backprop(w_1, w_2, w_3, training_data, epochs=100, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Train SNN with full backpropagation\n",
    "    \n",
    "    training_data: list of (spike_times, target_label) tuples\n",
    "    spike_times: result from run_Urd function\n",
    "    target_label: integer class label (0, 1, or 2)\n",
    "    \"\"\"\n",
    "    # Initialize backprop trainer\n",
    "    trainer = SNNBackpropagation(w_1, w_2, w_3, learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        print(f\"\\n=== Epoch {epoch} ===\")\n",
    "        \n",
    "        for i, (spike_times, target_label) in enumerate(training_data):\n",
    "            # Convert label to one-hot\n",
    "            target_one_hot = np.zeros(3)\n",
    "            target_one_hot[target_label] = 1.0\n",
    "            \n",
    "            # Handle None values in spike_times (neurons that didn't spike)\n",
    "            processed_spike_times = []\n",
    "            for st in spike_times:\n",
    "                if st is None:\n",
    "                    processed_spike_times.append(5.0)  # Late spike time for non-spiking neurons\n",
    "                else:\n",
    "                    processed_spike_times.append(st)\n",
    "            \n",
    "            print(f\"\\nSample {i}, Target class: {target_label}\")\n",
    "            print(f\"Spike times: {processed_spike_times}\")\n",
    "            \n",
    "            # Train on this sample\n",
    "            loss, probs = trainer.train_step(processed_spike_times, target_one_hot, verbose=True)\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss\n",
    "            predicted_class = np.argmax(probs)\n",
    "            if predicted_class == target_label:\n",
    "                correct_predictions += 1\n",
    "                \n",
    "            print(f\"Predicted class: {predicted_class}, Correct: {predicted_class == target_label}\")\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = epoch_loss / len(training_data)\n",
    "        accuracy = correct_predictions / len(training_data)\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.2%}\")\n",
    "        \n",
    "        # Early stopping if loss is very low\n",
    "        if avg_loss < 0.01:\n",
    "            print(\"Early stopping - loss converged\")\n",
    "            break\n",
    "    \n",
    "    # Return updated weights\n",
    "    updated_weights = trainer.get_flattened_weights()\n",
    "    return updated_weights, loss_history\n",
    "\n",
    "# Example usage function\n",
    "def example_training_loop():\n",
    "    \"\"\"Example of how to use the enhanced training\"\"\"\n",
    "    \n",
    "    # Your existing weight initialization\n",
    "    w_1 = np.random.uniform(0.05, 0.95, size=4).tolist()\n",
    "    w_2 = np.random.uniform(-0.95, 0.95, size=40).tolist()\n",
    "    w_3 = np.random.uniform(-0.95, 0.95, size=30).tolist()\n",
    "    \n",
    "    # Example training data (you'll need to generate this with your run_Urd function)\n",
    "    # training_data = [\n",
    "    #     (spike_times_sample_1, target_class_1),\n",
    "    #     (spike_times_sample_2, target_class_2),\n",
    "    #     ...\n",
    "    # ]\n",
    "    \n",
    "    # For demonstration, using dummy data\n",
    "    training_data = [\n",
    "        ([1.5, 2.0, 2.5, 3.0, 1.8, 2.2, 2.7, 3.2, 1.9, 2.3, 2.8, 3.3, 2.0, 2.4, 2.9], 0),\n",
    "        ([2.0, 2.5, 3.0, 3.5, 2.3, 2.7, 3.2, 3.7, 2.4, 2.8, 3.3, 3.8, 2.5, 2.9, 3.4], 1),\n",
    "        ([1.0, 1.5, 2.0, 2.5, 1.3, 1.7, 2.2, 2.7, 1.4, 1.8, 2.3, 2.8, 1.5, 1.9, 2.4], 2),\n",
    "    ]\n",
    "    \n",
    "    # Train the network\n",
    "    updated_weights, loss_history = train_snn_with_full_backprop(\n",
    "        w_1, w_2, w_3, \n",
    "        training_data, \n",
    "        epochs=50, \n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal updated weights:\")\n",
    "    print(f\"w_1: {len(updated_weights['w_1'])} elements\")\n",
    "    print(f\"w_2: {len(updated_weights['w_2'])} elements\") \n",
    "    print(f\"w_3: {len(updated_weights['w_3'])} elements\")\n",
    "    \n",
    "    return updated_weights, loss_history\n",
    "\n",
    "# Integration with your existing code\n",
    "def update_your_weights(spike_times, target_label, w_1, w_2, w_3, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for your current weight update function\n",
    "    \"\"\"\n",
    "    # Convert target to one-hot\n",
    "    target_one_hot = np.zeros(3)\n",
    "    target_one_hot[target_label] = 1.0\n",
    "    \n",
    "    # Handle None values\n",
    "    processed_spike_times = []\n",
    "    for st in spike_times:\n",
    "        if st is None:\n",
    "            processed_spike_times.append(5.0)\n",
    "        else:\n",
    "            processed_spike_times.append(st)\n",
    "    \n",
    "    # Create trainer and perform one update step\n",
    "    trainer = SNNBackpropagation(w_1, w_2, w_3, learning_rate)\n",
    "    loss, probs = trainer.train_step(processed_spike_times, target_one_hot)\n",
    "    \n",
    "    # Return updated weights in original format\n",
    "    updated_weights = trainer.get_flattened_weights()\n",
    "    return updated_weights['w_1'], updated_weights['w_2'], updated_weights['w_3'], loss, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70996c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c1a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 120 samples for 1 epochs\n",
      "\n",
      "=== Epoch 1/1 ===\n",
      "Iter 0 Output: [0.354, 0.5, 0.533]\n",
      "Error in iteration 0: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 1 Output: [0.483, 0.615, 0.663]\n",
      "Error in iteration 1: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 2 Output: [0.344, 0.49, 0.522]\n",
      "Error in iteration 2: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 3 Output: [0.314, 0.461, 0.488]\n",
      "Error in iteration 3: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 4 Output: [0.481, 0.615, 0.659]\n",
      "Error in iteration 4: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 5 Output: [0.417, 0.561, 0.596]\n",
      "Error in iteration 5: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 6 Output: [0.559, 0.677, 0.729]\n",
      "Error in iteration 6: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 7 Output: [0.541, 0.664, 0.719]\n",
      "Error in iteration 7: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 8 Output: [0.469, 0.606, 0.646]\n",
      "Error in iteration 8: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 9 Output: [0.473, 0.608, 0.651]\n",
      "Error in iteration 9: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 10 Output: [0.466, 0.603, 0.648]\n",
      "Error in iteration 10: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 11 Output: [0.329, 0.476, 0.505]\n",
      "Error in iteration 11: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 12 Output: [0.494, 0.628, 0.676]\n",
      "Error in iteration 12: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 13 Output: [0.518, 0.646, 0.697]\n",
      "Error in iteration 13: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 14 Output: [0.507, 0.636, 0.687]\n",
      "Error in iteration 14: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 15 Output: [0.335, 0.482, 0.513]\n",
      "Error in iteration 15: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 16 Output: [0.536, 0.661, 0.715]\n",
      "Error in iteration 16: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 17 Output: [0.488, 0.621, 0.666]\n",
      "Error in iteration 17: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 18 Output: [0.443, 0.582, 0.623]\n",
      "Error in iteration 18: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 19 Output: [0.455, 0.593, 0.634]\n",
      "Error in iteration 19: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 20 Output: [0.3, 0.449, 0.474]\n",
      "Error in iteration 20: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 21 Output: [0.331, 0.478, 0.508]\n",
      "Error in iteration 21: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 22 Output: [0.435, 0.575, 0.614]\n",
      "Error in iteration 22: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 23 Output: [0.443, 0.583, 0.621]\n",
      "Error in iteration 23: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 24 Output: [0.462, 0.599, 0.639]\n",
      "Error in iteration 24: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 25 Output: [0.606, 0.708, 0.773]\n",
      "Error in iteration 25: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 26 Output: [0.325, 0.473, 0.501]\n",
      "Error in iteration 26: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 27 Output: [0.465, 0.603, 0.646]\n",
      "Error in iteration 27: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 28 Output: [0.361, 0.506, 0.541]\n",
      "Error in iteration 28: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 29 Output: [0.47, 0.607, 0.65]\n",
      "Error in iteration 29: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 30 Output: [0.418, 0.56, 0.596]\n",
      "Error in iteration 30: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 31 Output: [0.601, 0.702, 0.765]\n",
      "Error in iteration 31: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 32 Output: [0.521, 0.648, 0.7]\n",
      "Error in iteration 32: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 33 Output: [0.5, 0.632, 0.679]\n",
      "Error in iteration 33: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 34 Output: [0.566, 0.682, 0.737]\n",
      "Error in iteration 34: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 35 Output: [0.362, 0.507, 0.544]\n",
      "Error in iteration 35: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 36 Output: [0.412, 0.556, 0.591]\n",
      "Error in iteration 36: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 37 Output: [0.5, 0.631, 0.68]\n",
      "Error in iteration 37: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 38 Output: [0.496, 0.628, 0.675]\n",
      "Error in iteration 38: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 39 Output: [0.28, 0.43, 0.453]\n",
      "Error in iteration 39: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 40 Output: [0.522, 0.648, 0.702]\n",
      "Error in iteration 40: 'SNNBackpropagation' object has no attribute 'train_step'\n",
      "Iter 41 Output: [0.338, 0.484, 0.515]\n",
      "Error in iteration 41: 'SNNBackpropagation' object has no attribute 'train_step'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 349\u001b[0m\n\u001b[0;32m    345\u001b[0m w_1, w_2, w_3 \u001b[38;5;241m=\u001b[39m initialize_weights()\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# or load existing: weights = np.load('weights_checkpoint.npz'); w_1, w_2, w_3 = weights['w1'], weights['w2'], weights['w3']\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# 3. Train with enhanced method\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m w_1, w_2, w_3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_Urd_enhanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# 4. Evaluate\u001b[39;00m\n\u001b[0;32m    352\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(X_test, y_test, w_1, w_2, w_3)\n",
      "Cell \u001b[1;32mIn[5], line 199\u001b[0m, in \u001b[0;36mtrain_Urd_enhanced\u001b[1;34m(training_data, result_data, w_1, w_2, w_3, loops, learning_rate)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(samples):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# Run SNN forward pass\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m         spike_times \u001b[38;5;241m=\u001b[39m \u001b[43mrun_Urd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;66;03m# Handle None values in spike_times (neurons that didn't spike)\u001b[39;00m\n\u001b[0;32m    205\u001b[0m         processed_spike_times \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36mrun_Urd\u001b[1;34m(inputs, weights_1, weights_2, weights_3)\u001b[0m\n\u001b[0;32m     90\u001b[0m spikemon \u001b[38;5;241m=\u001b[39m SpikeMonitor(neurons)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# neurons.v = 0\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# neurons.scheduled_time = 1e9 * second\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# neurons.global_clock = 0.0\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# neurons.sum = 0.0\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# neurons.spikes_received = 0.0\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_total):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1230\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m active_objects:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_clock \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[1;32m-> 1230\u001b[0m             \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[0;32m   1233\u001b[0m     timestep, t, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_variables[c]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:236\u001b[0m, in \u001b[0;36mBrianObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m codeobj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_objects:\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mcodeobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:131\u001b[0m, in \u001b[0;36mCodeObject.__call__\u001b[1;34m(self, **kwds)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_namespace()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:159\u001b[0m, in \u001b[0;36mCodeObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Runs the main code in the namespace.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        defined during the call of `CodeGenerator.code_object`.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\runtime\\numpy_rt\\numpy_rt.py:281\u001b[0m, in \u001b[0;36mNumpyCodeObject.run_block\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     exec(compiled_code, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    283\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode, block)\n",
      "File \u001b[1;32m(string):15\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\generators\\numpy_generator.py:401\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    397\u001b[0m clip_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m array, a_min, a_max: np\u001b[38;5;241m.\u001b[39mclip(array, a_min, a_max)\n\u001b[0;32m    398\u001b[0m DEFAULT_FUNCTIONS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mimplementations\u001b[38;5;241m.\u001b[39madd_implementation(\n\u001b[0;32m    399\u001b[0m     NumpyCodeGenerator, code\u001b[38;5;241m=\u001b[39mclip_func\n\u001b[0;32m    400\u001b[0m )\n\u001b[1;32m--> 401\u001b[0m int_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m value: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m DEFAULT_FUNCTIONS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mimplementations\u001b[38;5;241m.\u001b[39madd_implementation(\n\u001b[0;32m    403\u001b[0m     NumpyCodeGenerator, code\u001b[38;5;241m=\u001b[39mint_func\n\u001b[0;32m    404\u001b[0m )\n\u001b[0;32m    405\u001b[0m ceil_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m value: np\u001b[38;5;241m.\u001b[39mint32(np\u001b[38;5;241m.\u001b[39mceil(value))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Your enhanced backpropagation class (from previous artifact)\n",
    "class SNNBackpropagation:\n",
    "    def __init__(self, w_1, w_2, w_3, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize backpropagation for SNN\n",
    "        w_1: input weights (4,) - weights from input to first layer\n",
    "        w_2: hidden weights (40,) - weights from input (4) to hidden (10) - flattened\n",
    "        w_3: output weights (30,) - weights from hidden (10) to output (3) - flattened\n",
    "        \"\"\"\n",
    "        self.w_1 = np.array(w_1)\n",
    "        self.w_2 = np.array(w_2).reshape(10, 4)  # 10 hidden neurons, 4 inputs each\n",
    "        self.w_3 = np.array(w_3).reshape(3, 10)  # 3 output neurons, 10 hidden inputs each\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def cross_entropy_loss(self, predicted_probs, target_one_hot):\n",
    "        \"\"\"Cross-entropy loss with numerical stability\"\"\"\n",
    "        return -np.sum(target_one_hot * np.log(predicted_probs + 1e-12))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "        \n",
    "    def forward_pass(self, spike_times):\n",
    "        \"\"\"\n",
    "        Forward pass to compute activations\n",
    "        spike_times: [input_spikes(4), hidden_spikes(10), output_spikes(3)]\n",
    "        \"\"\"\n",
    "        self.input_activations = np.array(spike_times[:4])\n",
    "        self.hidden_activations = np.array(spike_times[4:14])\n",
    "        self.output_activations = np.array(spike_times[-3:])\n",
    "        \n",
    "        # Compute logits and probabilities for output layer\n",
    "        self.output_logits = self.w_3 @ self.hidden_activations\n",
    "        self.output_probs = self.softmax(self.output_logits)\n",
    "        \n",
    "        return self.output_probs\n",
    "    \n",
    "def spike_timing_derivative(x, w):\n",
    "    \"\"\"Derivative of the spike timing function\"\"\"\n",
    "    if w >= 0:\n",
    "        return (1 - w) * (x ** (-w))\n",
    "    else:\n",
    "        return (1 + w) * ((1 - x) ** (-w - 1))\n",
    "\n",
    "def backward_pass(self, target_one_hot):\n",
    "    \"\"\"\n",
    "    Backward pass to compute gradients and update weights\n",
    "    \"\"\"\n",
    "    # === OUTPUT LAYER GRADIENTS ===\n",
    "    # Gradient of loss w.r.t. output logits (softmax + cross-entropy)\n",
    "    output_grad = self.output_probs - target_one_hot  # shape (3,)\n",
    "    \n",
    "    # === HIDDEN LAYER GRADIENTS ===\n",
    "    # Backpropagate error to hidden layer\n",
    "    hidden_error = self.w_3.T @ output_grad  # shape (10,)\n",
    "    \n",
    "    # For spike-based networks, use spike timing derivative\n",
    "    hidden_grad = np.zeros_like(hidden_error)\n",
    "    for i in range(len(hidden_error)):\n",
    "        # Use spike timing derivative instead of sigmoid\n",
    "        hidden_grad[i] = hidden_error[i] * spike_timing_derivative(\n",
    "            self.hidden_activations[i], self.w_3[:, i]\n",
    "        )\n",
    "    \n",
    "    # === INPUT LAYER GRADIENTS ===\n",
    "    input_grad = np.zeros_like(self.input_activations)\n",
    "    for i in range(len(self.input_activations)):\n",
    "        input_grad[i] = (self.w_2.T @ hidden_grad)[i] * spike_timing_derivative(\n",
    "            self.input_activations[i], self.w_2[:, i]\n",
    "        )\n",
    "    \n",
    "    # === WEIGHT UPDATES ===\n",
    "    # Update output weights\n",
    "    grad_w3 = np.outer(output_grad, self.hidden_activations)\n",
    "    self.w_3 -= self.lr * grad_w3\n",
    "    \n",
    "    # Update hidden weights\n",
    "    grad_w2 = np.outer(hidden_grad, self.input_activations)\n",
    "    self.w_2 -= self.lr * grad_w2\n",
    "    \n",
    "    # Clip weights\n",
    "    self.w_3 = np.clip(self.w_3, -5.0, 5.0)\n",
    "    self.w_2 = np.clip(self.w_2, -5.0, 5.0)\n",
    "    \n",
    "    return {\n",
    "        'output_grad': output_grad,\n",
    "        'hidden_grad': hidden_grad,\n",
    "        'input_grad': input_grad,\n",
    "        'grad_w3': grad_w3,\n",
    "        'grad_w2': grad_w2\n",
    "    }\n",
    "    \n",
    "    def get_flattened_weights(self):\n",
    "        \"\"\"Return weights in original flattened format for compatibility\"\"\"\n",
    "        return {\n",
    "            'w_1': self.w_1.tolist(),\n",
    "            'w_2': self.w_2.flatten().tolist(),\n",
    "            'w_3': self.w_3.flatten().tolist()\n",
    "        }\n",
    "    \n",
    "    def train_step(self, spike_times, target_one_hot, verbose=False):\n",
    "        \"\"\"Complete training step: forward + backward + diagnostics\"\"\"\n",
    "        # Forward pass\n",
    "        probs = self.forward_pass(spike_times)\n",
    "        loss = self.cross_entropy_loss(probs, target_one_hot)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = self.backward_pass(target_one_hot)\n",
    "        \n",
    "        # Diagnostics\n",
    "        if verbose:\n",
    "            print(f\"Hidden activations: {self.hidden_activations}\")\n",
    "            print(f\"Softmax probabilities: {probs}\")\n",
    "            print(f\"Cross-entropy loss: {loss}\")\n",
    "            \n",
    "            # Print gradient information for output layer (matching your original format)\n",
    "            for i in range(3):\n",
    "                grad_signs = gradients['grad_w3'][i]\n",
    "                sign_str = ', '.join(f\"{v:+.3f}\" for v in grad_signs)\n",
    "                print(f\"Gradient signs (class {i}): [{sign_str}] (Error: {gradients['output_grad'][i]:+.3f})\")\n",
    "        \n",
    "        return loss, probs\n",
    "\n",
    "# Load and prepare Iris dataset\n",
    "def prepare_iris_data():\n",
    "    \"\"\"Load and prepare Iris dataset\"\"\"\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    def custom_scale(X, min_val=0.05, max_val=0.95):\n",
    "        X_min = np.min(X, axis=0)\n",
    "        X_max = np.max(X, axis=0)\n",
    "        return min_val + (X - X_min) * (max_val - min_val) / (X_max - X_min)\n",
    "\n",
    "    X_scaled = custom_scale(X)\n",
    "\n",
    "    # Combine and shuffle (important!)\n",
    "    combined = list(zip(X_scaled, y))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(combined)\n",
    "\n",
    "    X_shuffled, y_shuffled = zip(*combined)\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_shuffled, y_shuffled, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    training_data = [list(x) for x in X_train]\n",
    "    result_data = list(y_train)\n",
    "    \n",
    "    return training_data, result_data, X_test, y_test\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights():\n",
    "    \"\"\"Initialize network weights\"\"\"\n",
    "    w_1 = np.random.uniform(0.05, 0.95, size=4).tolist()\n",
    "    w_2 = np.random.uniform(-0.95, 0.95, size=40).tolist()\n",
    "    w_3 = np.random.uniform(-0.95, 0.95, size=30).tolist()\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Enhanced training function\n",
    "def train_Urd_enhanced(training_data, result_data, w_1, w_2, w_3, loops=5, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Enhanced training function with full backpropagation\n",
    "    \"\"\"\n",
    "    samples = len(training_data)\n",
    "    print(f\"Training on {samples} samples for {loops} epochs\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SNNBackpropagation(w_1, w_2, w_3, learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(loops):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{loops} ===\")\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for i in range(samples):\n",
    "            try:\n",
    "                # Run SNN forward pass\n",
    "                spike_times = run_Urd(training_data[i], \n",
    "                                    trainer.w_1.tolist(), \n",
    "                                    trainer.w_2.flatten().tolist(), \n",
    "                                    trainer.w_3.flatten().tolist())\n",
    "                \n",
    "                # Handle None values in spike_times (neurons that didn't spike)\n",
    "                processed_spike_times = []\n",
    "                for st in spike_times:\n",
    "                    if st is None:\n",
    "                        processed_spike_times.append(5.0)  # Late spike time for non-spiking neurons\n",
    "                    else:\n",
    "                        processed_spike_times.append(st)\n",
    "                \n",
    "                # Convert target to one-hot\n",
    "                target_one_hot = np.zeros(3)\n",
    "                target_one_hot[result_data[i]] = 1.0\n",
    "                \n",
    "                # Print current output (matching your original format)\n",
    "                outputs = real_outputs(spike_times)\n",
    "                print(f\"Iter {i} Output: {outputs}\")\n",
    "                \n",
    "                # Train step\n",
    "                loss, probs = trainer.train_step(processed_spike_times, target_one_hot, verbose=True)\n",
    "                \n",
    "                # Track metrics\n",
    "                epoch_loss += loss\n",
    "                predicted_class = np.argmax(probs)\n",
    "                if predicted_class == result_data[i]:\n",
    "                    correct_predictions += 1\n",
    "                \n",
    "                print(f\"Target: Class {result_data[i]}, Predicted: Class {predicted_class}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in iteration {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = epoch_loss / samples\n",
    "        accuracy = correct_predictions / samples\n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Training Accuracy: {accuracy:.2%}\")\n",
    "        print(f\"Epoch {epoch + 1} completed\")\n",
    "        \n",
    "        # Early stopping if accuracy is very high\n",
    "        if accuracy > 0.95:\n",
    "            print(\"Early stopping - high accuracy achieved\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTraining completed after {min(epoch + 1, loops)} epochs\")\n",
    "    \n",
    "    # Return updated weights in original format\n",
    "    final_weights = trainer.get_flattened_weights()\n",
    "    return final_weights['w_1'], final_weights['w_2'], final_weights['w_3']\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(X_test, y_test, w_1, w_2, w_3):\n",
    "    \"\"\"Evaluate the trained model\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    print(\"\\n--- Evaluation on Test Data ---\")\n",
    "    for i, sample in enumerate(X_test):\n",
    "        try:\n",
    "            pred = run_Urd(sample.tolist(), w_1, w_2, w_3)\n",
    "            outputs = real_outputs(pred)\n",
    "            predicted_class = int(np.argmax(outputs))\n",
    "\n",
    "            actual_class = int(y_test[i]) if isinstance(y_test[i], (int, np.integer)) else int(np.argmax(y_test[i]))\n",
    "\n",
    "            if predicted_class == actual_class:\n",
    "                correct += 1\n",
    "                class_correct[actual_class] += 1\n",
    "            class_total[actual_class] += 1\n",
    "            total += 1\n",
    "\n",
    "            print(f\"Sample {i}: Predicted Class: {predicted_class}, Actual Class: {actual_class}, Outputs: {outputs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating sample {i}: {e}\")\n",
    "            total += 1  # Still count it as a sample\n",
    "            actual_class = int(y_test[i])\n",
    "            class_total[actual_class] += 1\n",
    "\n",
    "    # Final results\n",
    "    print(f\"\\nOverall Accuracy: {correct}/{total} = {correct / total:.2%}\")\n",
    "\n",
    "    for cls in range(3):\n",
    "        if class_total[cls] > 0:\n",
    "            acc = class_correct[cls] / class_total[cls]\n",
    "            print(f\"Accuracy for Class {cls}: {class_correct[cls]}/{class_total[cls]} = {acc:.2%}\")\n",
    "        else:\n",
    "            print(f\"Class {cls} has no samples in test set.\")\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Main training and evaluation pipeline\n",
    "def main_training_pipeline():\n",
    "    \"\"\"Complete training and evaluation pipeline\"\"\"\n",
    "    print(\"=== SNN Training with Enhanced Backpropagation ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"Loading and preparing Iris dataset...\")\n",
    "    training_data, result_data, X_test, y_test = prepare_iris_data()\n",
    "    print(f\"Training samples: {len(training_data)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Initialize weights\n",
    "    print(\"\\nInitializing weights...\")\n",
    "    w_1, w_2, w_3 = initialize_weights()\n",
    "    print(f\"Initial weights - w_1: {len(w_1)}, w_2: {len(w_2)}, w_3: {len(w_3)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    w_1_trained, w_2_trained, w_3_trained = train_Urd_enhanced(\n",
    "        training_data, result_data, w_1, w_2, w_3, \n",
    "        loops=3,  # Reduced for initial testing\n",
    "        learning_rate=0.05  # Conservative learning rate\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating trained model...\")\n",
    "    final_accuracy = evaluate_model(X_test, y_test, w_1_trained, w_2_trained, w_3_trained)\n",
    "    \n",
    "    # Save weights\n",
    "    print(\"\\nSaving trained weights...\")\n",
    "    np.savez('weights_checkpoint_enhanced.npz', \n",
    "             w1=w_1_trained, w2=w_2_trained, w3=w_3_trained)\n",
    "    print(\"Weights saved to 'weights_checkpoint_enhanced.npz'\")\n",
    "    \n",
    "    return w_1_trained, w_2_trained, w_3_trained, final_accuracy\n",
    "\n",
    "# Usage example:\n",
    "# Assuming you have run_Urd and real_outputs functions defined elsewhere\n",
    "# w_1_final, w_2_final, w_3_final, accuracy = main_training_pipeline()\n",
    "\n",
    "# For integration with your existing code, you can also use individual functions:\n",
    "\n",
    "# Example usage with your existing setup:\n",
    "\n",
    "# 1. Prepare data\n",
    "training_data, result_data, X_test, y_test = prepare_iris_data()\n",
    "\n",
    "# 2. Initialize or load weights\n",
    "w_1, w_2, w_3 = initialize_weights()\n",
    "# or load existing: weights = np.load('weights_checkpoint.npz'); w_1, w_2, w_3 = weights['w1'], weights['w2'], weights['w3']\n",
    "\n",
    "# 3. Train with enhanced method\n",
    "w_1, w_2, w_3 = train_Urd_enhanced(training_data, result_data, w_1, w_2, w_3, loops=1)\n",
    "\n",
    "# 4. Evaluate\n",
    "accuracy = evaluate_model(X_test, y_test, w_1, w_2, w_3)\n",
    "\n",
    "# 5. Save\n",
    "np.savez('weights_checkpoint_enhancedv_2cleaningrequired.npz', w1=w_1, w2=w_2, w3=w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f10925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 Output: [1.168, 1.167, 1.171]\n",
      "Gradient signs (class 0): [-5.521, -5.521, -5.521, -5.521, -5.521, -5.521, -5.521, -5.521, -5.521, -5.521] (Error: -2.820)\n",
      "Gradient signs (class 1): [-3.764, -3.764, -3.764, -3.764, -3.764, -3.764, -3.764, -3.764, -3.764, -3.764] (Error: -1.922)\n",
      "Gradient signs (class 2): [-2.659, -2.659, -2.659, -2.659, -2.659, -2.659, -2.659, -2.659, -2.659, -2.659] (Error: -1.358)\n",
      "Hidden activations: [1.958 1.958 1.958 1.958 1.958 1.958 1.958 1.958 1.958 1.958]\n",
      "Softmax probabilities: [0.18016129 0.12773607 0.69210263]\n",
      "Cross-entropy loss: 10.114618924065724\n",
      "Iter 1 Output: [1.065, 1.065, 1.065]\n",
      "Gradient signs (class 0): [-3.451, -3.451, -3.451, -3.451, -3.451, -3.451, -3.451, -3.451, -3.451, -3.451] (Error: -1.740)\n",
      "Gradient signs (class 1): [-5.382, -5.382, -5.382, -5.382, -5.382, -5.382, -5.382, -5.382, -5.382, -5.382] (Error: -2.714)\n",
      "Gradient signs (class 2): [-3.264, -3.264, -3.264, -3.264, -3.264, -3.264, -3.264, -3.264, -3.264, -3.264] (Error: -1.646)\n",
      "Hidden activations: [1.983 1.983 1.983 1.983 1.983 1.983 1.983 1.983 1.983 1.983]\n",
      "Softmax probabilities: [0.30991895 0.28607084 0.40401021]\n",
      "Cross-entropy loss: 8.013954600440112\n",
      "Iter 2 Output: [1.185, 1.187, 1.186]\n",
      "Gradient signs (class 0): [-5.450, -5.450, -5.450, -5.450, -5.450, -5.450, -5.450, -5.450, -5.450, -5.450] (Error: -2.785)\n",
      "Gradient signs (class 1): [-3.130, -3.130, -3.130, -3.130, -3.130, -3.130, -3.130, -3.130, -3.130, -3.130] (Error: -1.599)\n",
      "Gradient signs (class 2): [-3.358, -3.358, -3.358, -3.358, -3.358, -3.358, -3.358, -3.358, -3.358, -3.358] (Error: -1.716)\n",
      "Hidden activations: [1.957 1.957 1.957 1.957 1.957 1.957 1.957 1.957 1.957 1.957]\n",
      "Softmax probabilities: [0.21533905 0.45054493 0.33411602]\n",
      "Cross-entropy loss: 8.488431697137685\n",
      "Iter 3 Output: [1.223, 1.223, 1.223]\n",
      "Gradient signs (class 0): [-5.200, -5.200, -5.200, -5.200, -5.200, -5.200, -5.200, -5.200, -5.200, -5.200] (Error: -2.667)\n",
      "Gradient signs (class 1): [-3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347] (Error: -1.717)\n",
      "Gradient signs (class 2): [-3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347, -3.347] (Error: -1.717)\n",
      "Hidden activations: [1.95 1.95 1.95 1.95 1.95 1.95 1.95 1.95 1.95 1.95]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 4 Output: [1.062, 1.062, 1.062]\n",
      "Gradient signs (class 0): [-3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406] (Error: -1.717)\n",
      "Gradient signs (class 1): [-5.291, -5.291, -5.291, -5.291, -5.291, -5.291, -5.291, -5.291, -5.291, -5.291] (Error: -2.667)\n",
      "Gradient signs (class 2): [-3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406, -3.406] (Error: -1.717)\n",
      "Hidden activations: [1.984 1.984 1.984 1.984 1.984 1.984 1.984 1.984 1.984 1.984]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 5 Output: [1.139, 1.139, 1.139]\n",
      "Gradient signs (class 0): [-3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377] (Error: -1.717)\n",
      "Gradient signs (class 1): [-3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377, -3.377] (Error: -1.717)\n",
      "Gradient signs (class 2): [-5.245, -5.245, -5.245, -5.245, -5.245, -5.245, -5.245, -5.245, -5.245, -5.245] (Error: -2.667)\n",
      "Hidden activations: [1.967 1.967 1.967 1.967 1.967 1.967 1.967 1.967 1.967 1.967]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 6 Output: [1.033, 1.033, 1.033]\n",
      "Gradient signs (class 0): [-3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418] (Error: -1.717)\n",
      "Gradient signs (class 1): [-3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418, -3.418] (Error: -1.717)\n",
      "Gradient signs (class 2): [-5.309, -5.309, -5.309, -5.309, -5.309, -5.309, -5.309, -5.309, -5.309, -5.309] (Error: -2.667)\n",
      "Hidden activations: [1.991 1.991 1.991 1.991 1.991 1.991 1.991 1.991 1.991 1.991]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 7 Output: [1.045, 1.045, 1.045]\n",
      "Gradient signs (class 0): [-3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413] (Error: -1.717)\n",
      "Gradient signs (class 1): [-3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413, -3.413] (Error: -1.717)\n",
      "Gradient signs (class 2): [-5.301, -5.301, -5.301, -5.301, -5.301, -5.301, -5.301, -5.301, -5.301, -5.301] (Error: -2.667)\n",
      "Hidden activations: [1.988 1.988 1.988 1.988 1.988 1.988 1.988 1.988 1.988 1.988]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 8 Output: [1.071, 1.071, 1.071]\n",
      "Gradient signs (class 0): [-3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402] (Error: -1.717)\n",
      "Gradient signs (class 1): [-5.285, -5.285, -5.285, -5.285, -5.285, -5.285, -5.285, -5.285, -5.285, -5.285] (Error: -2.667)\n",
      "Gradient signs (class 2): [-3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402, -3.402] (Error: -1.717)\n",
      "Hidden activations: [1.982 1.982 1.982 1.982 1.982 1.982 1.982 1.982 1.982 1.982]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 9 Output: [1.067, 1.067, 1.067]\n",
      "Gradient signs (class 0): [-3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404] (Error: -1.717)\n",
      "Gradient signs (class 1): [-5.288, -5.288, -5.288, -5.288, -5.288, -5.288, -5.288, -5.288, -5.288, -5.288] (Error: -2.667)\n",
      "Gradient signs (class 2): [-3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404, -3.404] (Error: -1.717)\n",
      "Hidden activations: [1.983 1.983 1.983 1.983 1.983 1.983 1.983 1.983 1.983 1.983]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 10 Output: [1.093, 1.093, 1.093]\n",
      "Gradient signs (class 0): [-3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394] (Error: -1.717)\n",
      "Gradient signs (class 1): [-3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394, -3.394] (Error: -1.717)\n",
      "Gradient signs (class 2): [-5.272, -5.272, -5.272, -5.272, -5.272, -5.272, -5.272, -5.272, -5.272, -5.272] (Error: -2.667)\n",
      "Hidden activations: [1.977 1.977 1.977 1.977 1.977 1.977 1.977 1.977 1.977 1.977]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n",
      "Iter 11 Output: [1.202, 1.202, 1.202]\n",
      "Gradient signs (class 0): [-5.211, -5.211, -5.211, -5.211, -5.211, -5.211, -5.211, -5.211, -5.211, -5.211] (Error: -2.667)\n",
      "Gradient signs (class 1): [-3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354] (Error: -1.717)\n",
      "Gradient signs (class 2): [-3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354, -3.354] (Error: -1.717)\n",
      "Hidden activations: [1.954 1.954 1.954 1.954 1.954 1.954 1.954 1.954 1.954 1.954]\n",
      "Softmax probabilities: [0.33333333 0.33333333 0.33333333]\n",
      "Cross-entropy loss: 7.800147249522278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 76\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloops\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loops completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Step 4: Train your model\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mtrain_Urd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     80\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 56\u001b[0m, in \u001b[0;36mtrain_Urd\u001b[1;34m(training_data, result_data, loops)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(samples):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         var \u001b[38;5;241m=\u001b[39m \u001b[43mrun_Urd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, real_outputs(var))\n\u001b[0;32m     60\u001b[0m         update_last_layer_weights(var, wanted_output[i])\n",
      "Cell \u001b[1;32mIn[15], line 98\u001b[0m, in \u001b[0;36mrun_Urd\u001b[1;34m(inputs, weights_1, weights_2, weights_3)\u001b[0m\n\u001b[0;32m     90\u001b[0m spikemon \u001b[38;5;241m=\u001b[39m SpikeMonitor(neurons)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# neurons.v = 0\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# neurons.scheduled_time = 1e9 * second\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# neurons.global_clock = 0.0\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# neurons.sum = 0.0\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# neurons.spikes_received = 0.0\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_total):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1230\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m active_objects:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_clock \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[1;32m-> 1230\u001b[0m             \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[0;32m   1233\u001b[0m     timestep, t, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_variables[c]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:236\u001b[0m, in \u001b[0;36mBrianObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m codeobj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_objects:\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mcodeobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:131\u001b[0m, in \u001b[0;36mCodeObject.__call__\u001b[1;34m(self, **kwds)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_namespace()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:159\u001b[0m, in \u001b[0;36mCodeObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Runs the main code in the namespace.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        defined during the call of `CodeGenerator.code_object`.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\runtime\\numpy_rt\\numpy_rt.py:281\u001b[0m, in \u001b[0;36mNumpyCodeObject.run_block\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     exec(compiled_code, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    283\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode, block)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "def custom_scale(X, min_val=0.05, max_val=0.95):  # arelayd scaled\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "    return min_val + (X - X_min) * (max_val - min_val) / (X_max - X_min)\n",
    "\n",
    "X_scaled = custom_scale(X)\n",
    "\n",
    "# Normalize to [0.05, 0.95]\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_scaled = 0.05 + ((X - X_min) / (X_max - X_min)) * (0.95 - 0.05)\n",
    "\n",
    "# Combine and shuffle (important!)\n",
    "combined = list(zip(X_scaled, y))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "X_shuffled, y_shuffled = zip(*combined)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_shuffled, y_shuffled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "training_data = [list(x) for x in X_train]\n",
    "result_data = list(y_train)\n",
    "\n",
    "# Step 3: Define training function (your original code)\n",
    "def train_Urd(training_data, result_data, loops):\n",
    "    samples = len(training_data)\n",
    "\n",
    "    wanted_output = [[-.1, -.1, -.1] for _ in range(samples)]\n",
    "    \n",
    "    for i in range(samples):\n",
    "        if result_data[i] == 0:\n",
    "            wanted_output[i] = [3.0, 2.05, 2.05]\n",
    "        elif result_data[i] == 1:\n",
    "            wanted_output[i] = [2.05, 3.0, 2.05]\n",
    "        elif result_data[i] == 2:\n",
    "            wanted_output[i] = [2.05, 2.05, 3.0]\n",
    "        else:\n",
    "            print(\"PRoblemmm\")\n",
    "\n",
    "    for j in range(loops):\n",
    "        for i in range(samples):\n",
    "            try:\n",
    "                var = run_Urd(training_data[i], w_1, w_2, w_3)\n",
    "\n",
    "                print(f\"Iter {i} Output:\", real_outputs(var))\n",
    "\n",
    "                update_last_layer_weights(var, wanted_output[i])\n",
    "\n",
    "                # calc error for hidden layer backprop\n",
    "                output_errors = get_output_errors(var, wanted_output[i])\n",
    "\n",
    "                update_hidden_layer_weights(var, output_errors)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in iteration {i}: {e}\")\n",
    "\n",
    "        print(f\"{j + 1} loop of training data completed\")\n",
    "    \n",
    "    print(f\"{loops} loops completed\")\n",
    "\n",
    "\n",
    "# Step 4: Train your model\n",
    "train_Urd(training_data, result_data, loops=1)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "print(\"\\n--- Evaluation on Test Data ---\")\n",
    "for i, sample in enumerate(X_test):\n",
    "    pred = run_Urd(sample.tolist(), w_1, w_2, w_3)\n",
    "    outputs = real_outputs(pred)\n",
    "    predicted_class = int(np.argmax(outputs))\n",
    "\n",
    "    actual_class = int(y_test[i]) if isinstance(y_test[i], (int, np.integer)) else int(np.argmax(y_test[i]))\n",
    "\n",
    "    if predicted_class == actual_class:\n",
    "        correct += 1\n",
    "        class_correct[actual_class] += 1\n",
    "    class_total[actual_class] += 1\n",
    "    total += 1\n",
    "\n",
    "    print(f\"Sample {i}: Predicted Class: {predicted_class}, Actual Class: {actual_class}, Outputs: {outputs}\")\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nOverall Accuracy: {correct}/{total} = {correct / total:.2%}\")\n",
    "\n",
    "for cls in range(3):\n",
    "    if class_total[cls] > 0:\n",
    "        acc = class_correct[cls] / class_total[cls]\n",
    "        print(f\"Accuracy for Class {cls}: {class_correct[cls]}/{class_total[cls]} = {acc:.2%}\")\n",
    "    else:\n",
    "        print(f\"Class {cls} has no samples in test set.\")\n",
    "\n",
    "np.savez('weights_checkpoint.npz', w1=w_1, w2=w_2, w3=w_3)\n",
    "# when weights was all 2.05 and 2.95 all values seemed give out that was either all for A or B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = [[0.1, 0.5, 0.1, 0.75], [0.3, 0.5, 0.21, 0.65]]\n",
    "# result_data = [0, 1]\n",
    "\n",
    "# def train_Urd(training_data, result_data, loops):\n",
    "#     samples = len(training_data)\n",
    "\n",
    "#     wanted_output = [[-.1, -.1, -.1] for _ in range(samples)]\n",
    "    \n",
    "#     for i in range(samples):\n",
    "#         if result_data[i] == 0:\n",
    "#             wanted_output[i] = [0.95, 0.05, 0.05]\n",
    "#         elif result_data[i] == 1:\n",
    "#             wanted_output[i] = [0.05, 0.95, 0.05]\n",
    "#         elif result_data[i] == 2:\n",
    "#             wanted_output[i] = [0.05, 0.05, 0.95]\n",
    "\n",
    "#     for j in range(loops):\n",
    "#         for i in range(samples):\n",
    "#             try:\n",
    "#                 var = run_Urd(training_data[i], w_1, w_2, w_3)\n",
    "\n",
    "#                 print(f\"Iter {i} Output:\", real_outputs(var))\n",
    "\n",
    "#                 update_last_layer_weights(var, wanted_output[i])\n",
    "\n",
    "#                 # calc error for hidden layer backprop\n",
    "#                 output_errors = get_output_errors(var, wanted_output[i])\n",
    "\n",
    "#                 update_hidden_layer_weights(var, output_errors)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in iteration {i}: {e}\")\n",
    "\n",
    "#         print(f\"{j + 1} loop of training data completed\")\n",
    "    \n",
    "#     print(f\"{loops} loops completed\")\n",
    "\n",
    "\n",
    "\n",
    "# train_Urd(training_data, result_data, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
