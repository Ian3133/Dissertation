{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b7c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "\n",
    "start_scope()\n",
    "\n",
    "defaultclock.dt = 0.0001*ms  \n",
    "\n",
    "# Custom timing function\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, result=1, sum=1, spikes_received=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received): \n",
    "    #print(global_clock)\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return (x ** (1 - w)) \n",
    "    else:\n",
    "        return (1 - (1 - x) ** (1 + w)) \n",
    "    \n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(layer=1, result=1, sum=1, spikes_received=1)\n",
    "def math1(layer, sum, spikes_received): \n",
    "    return (sum/spikes_received )+ layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urd, Verdande, Skuld \n",
    "\n",
    "# did not apply the specficyed weifhts to the first layer yet will maybe do after\n",
    "def run_Urd(inputs, weights_1, weights_2, weights_3):\n",
    "    '''4-10-3 SNN'''\n",
    "    # will add check of weights # so it all works\n",
    "    n_input = 4 \n",
    "    n_hidden = 10\n",
    "    n_output = 3\n",
    "    n_total = n_input + n_hidden + n_output\n",
    "\n",
    "    neurons = NeuronGroup(n_total, '''\n",
    "        v : 1\n",
    "        sum : 1\n",
    "        spikes_received : 1\n",
    "        scheduled_time : second\n",
    "        global_clock : 1\n",
    "    ''', threshold='v > 1', reset='v = 0', method='exact')\n",
    "    neurons.v = 0\n",
    "    neurons.scheduled_time = 1e9 * second\n",
    "    neurons.global_clock = 0.0\n",
    "    neurons.sum = 0.0\n",
    "    neurons.spikes_received = 0.0\n",
    "\n",
    "\n",
    "    indices = list(range(n_input))\n",
    "    stim = SpikeGeneratorGroup(n_input, indices=indices, times=(inputs*ms))\n",
    "\n",
    "    syn_input = Synapses(stim, neurons[0:n_input], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    syn_input.connect(j='i')\n",
    "    syn_input.w = weights_1\n",
    "    syn_input.layer = 0\n",
    "\n",
    "    syn_hidden = Synapses(neurons[0:n_input], neurons[n_input:n_input+n_hidden], '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "    ''', on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "    ''')\n",
    "    for inp in range(n_input):\n",
    "        for hid in range(n_hidden):\n",
    "            syn_hidden.connect(i=inp, j=hid)\n",
    "\n",
    "    syn_hidden.w = weights_2\n",
    "    syn_hidden.layer = 1\n",
    "\n",
    "\n",
    "    syn_output = Synapses(\n",
    "        neurons[n_input:n_input+n_hidden], \n",
    "        neurons[n_input+n_hidden:n_total], \n",
    "        '''\n",
    "        w : 1\n",
    "        layer : 1\n",
    "        ''',\n",
    "        on_pre='''\n",
    "        spikes_received += 1\n",
    "        sum += spike_timing(w, global_clock, layer, spikes_received, sum)\n",
    "        scheduled_time = ((sum/spikes_received) + layer) * ms \n",
    "        '''\n",
    "    )\n",
    "\n",
    "    for hid in range(n_hidden):\n",
    "        for out in range(n_output):\n",
    "            syn_output.connect(i=hid, j=out)\n",
    "\n",
    "    # Set weights in correct order\n",
    "    syn_output.w[:] = weights_3\n",
    "    syn_output.layer = 2\n",
    "\n",
    "    #print(syn_output.i[:], syn_output.j[:])\n",
    "    #weights_into_output_1 = weights_3[1::3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    neurons.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "\n",
    "    spikemon = SpikeMonitor(neurons)\n",
    "\n",
    "    run(5*ms)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(n_total):\n",
    "        times = spikemon.spike_trains()[i]\n",
    "        if len(times) > 0:\n",
    "            result.append(round(times[0]/ms, 3))\n",
    "        else:\n",
    "            result.append(None)  # or some other placeholder like float('nan')\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def calc_cost(outputs, desired_outputs):\n",
    "    return 0.5 * ((outputs - desired_outputs) ** 2)\n",
    "\n",
    "def shift_weights(cost):\n",
    "\n",
    "    return 0 \n",
    "    \n",
    "w_1 = [0] * 4 #np.random.uniform(  0.05, .95, size=4) #[0.5] * 4\n",
    "w_2 = [0] * 40 #np.random.uniform( -.95, .95, size=40) #[0.5] * 40\n",
    "w_3 = [0] * 30 #np.random.uniform( -.95, .95, size=30) #[0.5] * 30 \n",
    "\n",
    "def update_weights_input_neuron(hidden_activations, weights, actual_output, desired_output, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Update all weights going into a single output neuron using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        hidden_activations (np.ndarray): Activations from hidden neurons (shape: [n_hidden]).\n",
    "        weights (np.ndarray): Current weights into the output neuron (shape: [n_hidden]).\n",
    "        actual_output (float): Current output of this neuron.\n",
    "        desired_output (float): Target output for this neuron.\n",
    "        learning_rate (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Updated weights (shape: [n_hidden]).\n",
    "    \"\"\"\n",
    "    error = actual_output - desired_output\n",
    "    gradients = error * hidden_activations\n",
    "    updated_weights = weights - learning_rate * gradients\n",
    "    updated_weights = np.clip(updated_weights, -0.999999, 0.999999)\n",
    "    return updated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46936208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n",
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n",
      "WARNING    Came across an abstract code block that may not be well-defined: the outcome may depend on the order of execution. You can ignore this warning if you are sure that the order of operations does not matter. 3 lines of abstract code, first line is: 'spikes_received += 1 (in-place)'\n",
      " [brian2.codegen.generators.base]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.503, 0.503, 0.503]\n",
      "[0.0788045 0.0788045 0.0788045]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0.5] * 4 #np.random.uniform(0.1, .95, size=4) #[0.5]* 4\n",
    "desired_outputs = np.array([.9] * 3) # np.random.uniform(0.1, .95, size=3)\n",
    "\n",
    "# w_1 = [0] * 4 #np.random.uniform(  0.05, .95, size=4) #[0.5] * 4\n",
    "# w_2 = [0] * 40 #np.random.uniform( -.95, .95, size=40) #[0.5] * 40\n",
    "# w_3 = [0] * 30 #np.random.uniform( -.95, .95, size=30) #[0.5] * 30\n",
    "\n",
    "var = run_Urd(inputs, w_1, w_2, w_3)\n",
    "\n",
    "\n",
    "real_output = [-1]*3\n",
    "for i in range(3):\n",
    "    real_output[i] = round(var[-3+i] - 2, 4)\n",
    "\n",
    "print(real_output)\n",
    "#print(calc_cost(real_output, desired_outputs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "36d6f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_single_output_neuron_weights(hidden_activations, weights, actual_output, desired_output, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Update all weights going into a single output neuron using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        hidden_activations (np.ndarray): Activations from hidden neurons (shape: [n_hidden]).\n",
    "        weights (np.ndarray): Current weights into the output neuron (shape: [n_hidden]).\n",
    "        actual_output (float): Current output of this neuron.\n",
    "        desired_output (float): Target output for this neuron.\n",
    "        learning_rate (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Updated weights (shape: [n_hidden]).\n",
    "    \"\"\"\n",
    "    error = actual_output - desired_output\n",
    "    gradients = error * hidden_activations\n",
    "    updated_weights = weights - learning_rate * gradients\n",
    "    updated_weights = np.clip(updated_weights, -0.999999, 0.999999)\n",
    "    return updated_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b9283ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old weights: [ 0.39496386  0.22768375  0.06844447  0.3543251  -0.24470463  0.05841081\n",
      " -0.36690612 -0.28705187 -0.20666003 -0.15582938]\n",
      "New weights: [ 0.4167412   0.23201014  0.10793045  0.35611423 -0.19730257  0.07679181\n",
      " -0.31910129 -0.28643604 -0.18191923 -0.15314195]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 10\n",
    "\n",
    "# Fake data\n",
    "hidden_activations = np.random.uniform(0, 1, size=n_hidden)\n",
    "weights_to_output_0 = np.random.uniform(-0.5, 0.5, size=n_hidden)\n",
    "actual_output = 2.2\n",
    "desired_output = 2.7\n",
    "\n",
    "# Update weights\n",
    "new_weights = update_single_output_neuron_weights(\n",
    "    hidden_activations,\n",
    "    weights_to_output_0,\n",
    "    actual_output,\n",
    "    desired_output,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "print(\"Old weights:\", weights_to_output_0)\n",
    "print(\"New weights:\", new_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
