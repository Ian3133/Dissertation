{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after one update: [0.37626079]\n"
     ]
    }
   ],
   "source": [
    "# Math Section for Skuld model v2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "a = 1 # the main part of what i will be changing for weights changes the size of the sigmoid without changing near zero\n",
    "b = 0  # bias will add to updating with ^ later\n",
    "m = 0.5 # the rate of change of the sigmoid \n",
    "c = 1 # a constant I used, same as but but shift the sigmod to a place where i 0 is close to 0 (actuyll 0.04 but whatever)\n",
    "layer_bias = -0.5\n",
    "\n",
    "\n",
    "def ians_sigmoid(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "    # width = max(width, 0.01)  # prevent division by 0 or instability\n",
    "    # layer_shift = layer * 0.1  # optional small bias per layer\n",
    "    # return scale * np.exp(-((x - center - layer_shift) ** 2) / (2 * width ** 2))\n",
    "    return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "    #return 1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias)))\n",
    "#print(ians_sigmoid(0.8, a, b, m, c))\n",
    "\n",
    "# def ians_sigmoid_new(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "#     return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "\n",
    "def calc_loss(output, desired):\n",
    "    # can change but not a requirement yet\n",
    "    return (0.5 * ((output - desired) ** 2))\n",
    "\n",
    "\n",
    "def d_loss_d_weight_2(n1, n2, desired):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1) # removedall layer_bias can check layer if correct call\n",
    "\n",
    "def d_loss_d_weight_1(n1, n2, desired, w2, input):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1 * w2 * (1- n1) * input)\n",
    "\n",
    "\n",
    "def d_loss_d_bias_2(n2, desired):\n",
    "    return (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "\n",
    "def d_loss_d_bias_1(n1, n2, delta2, desired, a2):\n",
    "    #delta2 = (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "    return delta2 * a2 * (n1 - layer_bias) * (1 - (n1 - layer_bias))\n",
    "\n",
    "\n",
    "def update_param(lr, grad, w):\n",
    "    w -= lr * grad\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cd955e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820821300824607 1.1543436937742046 1.4860303144480338\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_pass(inputs, a, b):\n",
    "    global m, c\n",
    "    sig_r1 = ians_sigmoid(inputs[0], a[0], b[0], m, c)\n",
    "    delay_1 = sig_r1 + inputs[0]\n",
    "    sig_r2 = ians_sigmoid(inputs[1], a[1], b[1], m, c)\n",
    "    delay_2 = sig_r2 + inputs[1]\n",
    "    avg_delay = (delay_1 + delay_2) / 2\n",
    "\n",
    "    sig_r2 = ians_sigmoid(avg_delay, a[2], b[2], m, c, layer=1)\n",
    "    delay_3 = sig_r2 + avg_delay\n",
    "\n",
    "    return ([delay_1, delay_2, delay_3])\n",
    "\n",
    "\n",
    "def run_basic(input, a, b, desired, lr):   # will need to check work on bias/b and make sure everything is correct in this\n",
    "\n",
    "    result = forward_pass(inputs, a, b)\n",
    "   \n",
    "    n1 = result[0]\n",
    "    n2 = result[1]\n",
    "    n3 = result[2]\n",
    "    print(n1, n2, n3)\n",
    "\n",
    "    #loss = calc_loss(output, desired)\n",
    "\n",
    "    # delta_a2 = - d_loss_d_weight_2(n1, n2, desired)\n",
    "\n",
    "    # delta_a1 = - d_loss_d_weight_1(n1, n2, desired, a2, input) # trying to just input a as a the weight directly\n",
    "\n",
    "    # new_a1 = update_param(lr, delta_a1, a1)\n",
    "    # new_a2 = update_param(lr, delta_a2, a2)\n",
    "\n",
    "\n",
    "    # delta_b2 = d_loss_d_bias_2(n2, desired)\n",
    "    # delta_b1 = d_loss_d_bias_1(n1, n2, delta_b2, desired, b2) # trying to just input a as a the weight directly\n",
    "\n",
    "\n",
    "    # new_b1 = update_param(lr, delta_b1, b1)\n",
    "    # new_b2 = update_param(lr, delta_b2, b2)\n",
    "\n",
    "    # return new_a1, new_a2, new_b1, new_b2, result\n",
    "    return\n",
    "\n",
    "m = 0.5\n",
    "c = 1 \n",
    "inputs = [0.5, 0.8]\n",
    "a = [1.0]*3\n",
    "b = [0.0]*3\n",
    "desired = 1.8\n",
    "\n",
    "run_basic(inputs, a, b, desired, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9bad6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820821300824607 1.1543436937742046 1.4860303144480338\n"
     ]
    }
   ],
   "source": [
    "s1 = ians_sigmoid(inputs[0], 1, 0, 0.5, 1, layer=0)  + inputs[0]\n",
    "\n",
    "s2 = ians_sigmoid(inputs[1], 1, 0, 0.5, 1, layer=0) + inputs[1]\n",
    "\n",
    "s3 = ians_sigmoid((s1 + s2)/2, 1, 0, 0.5, 1, layer=1) + (s1 + s2)/2\n",
    "\n",
    "print(s1, s2, s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f499576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — Loss: 1.38932 — Predicted: [0.05066474 0.12981888]\n",
      "Epoch  2 — Loss: 1.38057 — Predicted: [0.05147972 0.13564856]\n",
      "Epoch  3 — Loss: 1.37130 — Predicted: [0.05231687 0.14187985]\n",
      "Epoch  4 — Loss: 1.36145 — Predicted: [0.05317686 0.14854387]\n",
      "Epoch  5 — Loss: 1.35101 — Predicted: [0.05406037 0.15567347]\n",
      "Epoch  6 — Loss: 1.33992 — Predicted: [0.05496807 0.16330303]\n",
      "Epoch  7 — Loss: 1.32816 — Predicted: [0.05590063 0.17146811]\n",
      "Epoch  8 — Loss: 1.31569 — Predicted: [0.05685871 0.18020503]\n",
      "Epoch  9 — Loss: 1.30247 — Predicted: [0.05784296 0.18955019]\n",
      "Epoch 10 — Loss: 1.28848 — Predicted: [0.05885402 0.19953927]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Standard sigmoid + derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(hidden_size, input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        self.z1 = self.W1 @ x + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        # Output layer\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        return 0.5 * np.sum((y_hat - y) ** 2)\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # 1) Output error\n",
    "        delta2 = (self.a2 - y) * sigmoid_deriv(self.z2)\n",
    "        # 2) Hidden error\n",
    "        delta1 = (self.W2.T @ delta2) * sigmoid_deriv(self.z1)\n",
    "\n",
    "        # Parameter gradients\n",
    "        self.dW2 = delta2 @ self.a1.T\n",
    "        self.db2 = delta2\n",
    "        self.dW1 = delta1 @ x.T\n",
    "        self.db1 = delta1\n",
    "\n",
    "    def update_params(self, lr):\n",
    "        # Gradient descent step\n",
    "        self.W2 -= lr * self.dW2\n",
    "        self.b2 -= lr * self.db2\n",
    "        self.W1 -= lr * self.dW1\n",
    "        self.b1 -= lr * self.db1\n",
    "\n",
    "    def train_step(self, x, y, lr=0.1):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.compute_loss(y_hat, y)\n",
    "        self.backward(x, y)\n",
    "        self.update_params(lr)\n",
    "        return loss, y_hat\n",
    "\n",
    "# Example: X=3 inputs, Y=4 hidden, Z=2 outputs\n",
    "np.random.seed(42)\n",
    "model = NeuralNetwork(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Dummy data (column vectors)\n",
    "x = np.array([[0.8], [0.1], [0.3]])\n",
    "y = np.array([[1.0], [1.5]])\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss, y_hat = model.train_step(x, y, lr)\n",
    "    # Flatten prediction for readability\n",
    "    prediction = y_hat.flatten()\n",
    "    print(f\"Epoch {epoch:2d} — Loss: {loss:.5f} — Predicted: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55198d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — Loss: 49.64083 — Predicted (delayed): [7.52598245 7.52949007]\n",
      "Epoch  2 — Loss: 49.64068 — Predicted (delayed): [7.52597187 7.52947909]\n",
      "Epoch  3 — Loss: 49.64053 — Predicted (delayed): [7.52596128 7.52946809]\n",
      "Epoch  4 — Loss: 49.64038 — Predicted (delayed): [7.52595069 7.52945709]\n",
      "Epoch  5 — Loss: 49.64023 — Predicted (delayed): [7.52594009 7.52944608]\n",
      "Epoch  6 — Loss: 49.64007 — Predicted (delayed): [7.52592948 7.52943506]\n",
      "Epoch  7 — Loss: 49.63992 — Predicted (delayed): [7.52591887 7.52942404]\n",
      "Epoch  8 — Loss: 49.63977 — Predicted (delayed): [7.52590824 7.529413  ]\n",
      "Epoch  9 — Loss: 49.63962 — Predicted (delayed): [7.52589762 7.52940197]\n",
      "Epoch 10 — Loss: 49.63946 — Predicted (delayed): [7.52588698 7.52939092]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Standard sigmoid + derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class SNN_Network:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(hidden_size, input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sum of raw inputs (scalar)\n",
    "        sum_in = np.sum(x)\n",
    "        # Hidden layer pre-activation\n",
    "        self.z1 = self.W1 @ x + self.b1\n",
    "        # Hidden activation (\"spike response\")\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        # Delay accumulator: add summed raw input to each hidden neuron\n",
    "        self.d1 = self.a1 + sum_in\n",
    "        \n",
    "        # Sum of delayed-hidden (scalar)\n",
    "        sum_h = np.sum(self.d1)\n",
    "        # Output layer pre-activation\n",
    "        self.z2 = self.W2 @ self.d1 + self.b2\n",
    "        # Output activation\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        # Delay accumulator for output\n",
    "        self.d2 = self.a2 + sum_h\n",
    "        \n",
    "        return self.d2  # use the delayed output as the final \"guess\"\n",
    "\n",
    "    def compute_loss(self, y_pred, y):\n",
    "        return 0.5 * np.sum((y_pred - y) ** 2)\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # Backprop through the final sigmoid\n",
    "        delta2 = (self.a2 - y) * sigmoid_deriv(self.z2)\n",
    "        # Gradients for W2 and b2\n",
    "        self.dW2 = delta2 @ self.d1.T\n",
    "        self.db2 = delta2\n",
    "        \n",
    "        # Backprop into hidden (through W2 and sigmoid)\n",
    "        delta1 = (self.W2.T @ delta2) * sigmoid_deriv(self.z1)\n",
    "        # Gradients for W1 and b1\n",
    "        self.dW1 = delta1 @ x.T\n",
    "        self.db1 = delta1\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.W2 -= lr * self.dW2\n",
    "        self.b2 -= lr * self.db2\n",
    "        self.W1 -= lr * self.dW1\n",
    "        self.b1 -= lr * self.db1\n",
    "\n",
    "    def train_step(self, x, y, lr=0.1):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        self.backward(x, y)\n",
    "        self.update(lr)\n",
    "        return loss, y_pred\n",
    "\n",
    "# Example: X=3 inputs, Y=4 hidden, Z=2 outputs, summing inputs at each layer\n",
    "np.random.seed(42)\n",
    "model = SNN_Network(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Dummy data (column vectors)\n",
    "x = np.array([[0.8], [0.1], [0.3]])\n",
    "y = np.array([[1.0], [0.0]])\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss, y_pred = model.train_step(x, y, lr)\n",
    "    print(f\"Epoch {epoch:2d} — Loss: {loss:.5f} — Predicted (delayed): {y_pred.flatten()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
