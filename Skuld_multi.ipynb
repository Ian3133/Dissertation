{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be99b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imports and black box of Functions\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "a = 1 \n",
    "b = 0  \n",
    "m = 0.5 \n",
    "c = 1 \n",
    "layer_bias = -0.5\n",
    "\n",
    "\n",
    "def ians_sigmoid(x, a_a, b_b, m_m, c_c, layer=0):\n",
    "    return (1/ ((1/a_a) + np.exp(((-x*m_m) + b_b + c_c + layer * layer_bias))))* a_a\n",
    "\n",
    "\n",
    "def calc_loss(output, desired):\n",
    "    # can change but not a requirement yet\n",
    "    return (0.5 * ((output - desired) ** 2))\n",
    "\n",
    "\n",
    "def d_loss_d_weight_2(n1, n2, desired):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1) # removedall layer_bias can check layer if correct call\n",
    "\n",
    "def d_loss_d_weight_1(n1, n2, desired, w2, input):\n",
    "    # check the removal of the layer bias fo rthis one and go back to check the others as well\n",
    "    return ((n2 - desired) * (n2) * (1-(n2)) * n1 * w2 * (1- n1) * input)\n",
    "\n",
    "\n",
    "def d_loss_d_bias_2(n2, desired):\n",
    "    return (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "\n",
    "def d_loss_d_bias_1(n1, n2, delta2, desired, a2):\n",
    "    #delta2 = (n2 - desired) * (n2 - layer_bias) * (1 - (n2 - layer_bias))\n",
    "    return delta2 * a2 * (n1 - layer_bias) * (1 - (n1 - layer_bias))\n",
    "\n",
    "\n",
    "def update_param(lr, grad, w):\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def forward_pass(inputs, x,y,z, a, b):\n",
    "    global m, c\n",
    "    total = x + y + z\n",
    "    delays = []\n",
    "    total_time_input = 0\n",
    "    for i in range(0, x):\n",
    "        sig_r = ians_sigmoid(inputs[i], a[i], b[i], m, c)\n",
    "        delays.append(sig_r + inputs[i])\n",
    "        total_time_input += delays[i]\n",
    "\n",
    "    average_input = total_time_input / x\n",
    "    total_time_hidden= 0\n",
    "        \n",
    "    for i in range(x,(y+x)):\n",
    "        \n",
    "        sig_r = ians_sigmoid(average_input, a[i], b[i], m, c, layer=1)\n",
    "        delays.append(sig_r + average_input)\n",
    "        print(delays[i])\n",
    "        total_time_hidden += delays[i]\n",
    "\n",
    "    average_hidden = total_time_hidden / y\n",
    "    total_time_output= 0\n",
    "\n",
    "\n",
    "    for i in range((y+x),(z+y+x)):\n",
    "        \n",
    "\n",
    "        sig_r = ians_sigmoid(average_hidden, a[i], b[i], m, c, layer=2)\n",
    "        delays.append(sig_r + average_hidden)\n",
    "        total_time_output += delays[i]\n",
    "\n",
    "    average_output = total_time_output / (z)\n",
    "\n",
    "    return (delays, average_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd955e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.09416450476605823, 2.233968903876898)\n",
      "(0.08297514494611541, 2.2073699668510565)\n",
      "(0.07332311540797724, 2.182944161485659)\n",
      "(0.06496207998138327, 2.1604499409942615)\n",
      "(0.057691481644167306, 2.139680678414794)\n",
      "(0.05134681461492055, 2.1204584672462894)\n",
      "(0.04579222122685662, 2.102629216127117)\n",
      "(0.040914798360642705, 2.0860587294967337)\n",
      "(0.03662017611845823, 2.0706295479745633)\n",
      "(0.03282905373554481, 2.05623838016794)\n",
      "(0.029474463045470727, 2.0427939992894006)\n",
      "(0.02649959012932553, 2.0302155082931015)\n",
      "(0.023856028945076418, 2.01843089957731)\n",
      "(0.021502371982137652, 2.0073758519314033)\n",
      "(0.019403065849803086, 1.996992719915245)\n",
      "(0.017527476603360032, 1.9872296803573624)\n",
      "(0.015849122212711845, 1.9780400079348002)\n",
      "(0.014345039058539334, 1.9693814574180972)\n",
      "(0.01299525653167644, 1.9612157345402517)\n",
      "(0.011782359307364489, 1.9535080408797174)\n",
      "(0.01069112109718754, 1.946226680856727)\n",
      "(0.009708196961144052, 1.9393427210954635)\n",
      "(0.008821863820870236, 1.9328296941265035)\n",
      "(0.008021800823235272, 1.9266633397888693)\n",
      "(0.007298902788919668, 1.9208213788111994)\n",
      "(0.0066451212390569465, 1.9152833139622292)\n",
      "(0.006053328497220851, 1.9100302549049202)\n",
      "(0.005517201169330447, 1.9050447634994763)\n",
      "(0.005031119952896263, 1.9003107168043003)\n",
      "(0.004590083252195431, 1.8958131854412057)\n",
      "(0.00418963250286614, 1.8915383253382554)\n",
      "(0.0038257874578631152, 1.8874732811533113)\n",
      "(0.003494989972269485, 1.883606099924222)\n",
      "(0.0031940550593522986, 1.8799256536958229)\n",
      "(0.002920128184169429, 1.8764215700462825)\n",
      "(0.002670647921695166, 1.8730841695813145)\n",
      "(0.002443313239977584, 1.8699044095887747)\n",
      "(0.0022360547802083844, 1.8668738331518149)\n",
      "(0.0020470095987474633, 1.8639845231090686)\n",
      "(0.0018744989143087746, 1.8612290603277362)\n",
      "(0.0017170084692824591, 1.858600485821919)\n",
      "(0.001573171169665269, 1.856092266305887)\n",
      "(0.0014417517150188378, 1.8536982628214143)\n",
      "(0.001321632969702265, 1.8514127021212126)\n",
      "(0.001211803860488654, 1.8492301505276727)\n",
      "(0.0011113486145417177, 1.847145490018489)\n",
      "(0.00101943717639094, 1.8451538963189433)\n",
      "(0.0009353166636645045, 1.8432508188053014)\n",
      "(0.0008583037394624859, 1.8414319620453217)\n",
      "(0.0007877777948414175, 1.839693268820832)\n",
      "(0.0007231748483145969, 1.8380309044939664)\n",
      "(0.0006639820808714525, 1.8364412425932886)\n",
      "(0.0006097329350565593, 1.8349208515089928)\n",
      "(0.0005600027153462091, 1.8334664821977515)\n",
      "(0.0005144046346161888, 1.832075056807937)\n",
      "(0.0004725862580645721, 1.8307436581448784)\n",
      "(0.00043422630168018573, 1.8294695199037985)\n",
      "(0.00039903174734597183, 1.8282500176051617)\n",
      "(0.00036673524103559896, 1.8270826601734615)\n",
      "(0.00033709274438914916, 1.8259650821061344)\n",
      "(0.00030988141330846874, 1.8248950361842866)\n",
      "(0.00028489768016132325, 1.8238703866814647)\n",
      "(0.0002619555187747387, 1.8228891030306886)\n",
      "(0.00024088487368261073, 1.821949253913635)\n",
      "(0.00022153023710582348, 1.8210490017390766)\n",
      "(0.00020374935892225808, 1.8201865974806186)\n",
      "(0.00018741207645731346, 1.8193603758464196)\n",
      "(0.0001723992523182225, 1.818568750755946)\n",
      "(0.00015860180973035833, 1.8178102111009589)\n",
      "(0.00014591985593002892, 1.8170833167698799)\n",
      "(0.00013426188514196671, 1.8163866949164233)\n",
      "(0.00012354405353646658, 1.8157190364549782)\n",
      "(0.00011368951933293263, 1.815079092766671)\n",
      "(0.00010462784190452664, 1.8144656726013364)\n",
      "(9.629443435280383e-05, 1.8138776391618174)\n",
      "(8.863006457013972e-05, 1.813313907358108)\n",
      "(8.158040029804078e-05, 1.8127734412198155)\n",
      "(7.509559412902496e-05, 1.812255251456337)\n",
      "(6.912990479325901e-05, 1.8117583931549561)\n",
      "(6.364135142390043e-05, 1.8112819636078035)\n",
      "(5.859139781232887e-05, 1.8108251002593352)\n",
      "(5.394466394865512e-05, 1.8103869787665765)\n",
      "(4.9668662399129606e-05, 1.8099668111649745)\n",
      "(4.5733557302227e-05, 1.8095638441332162)\n",
      "(4.211194397266967e-05, 1.8091773573508576)\n",
      "(3.877864728953194e-05, 1.8088066619430443)\n",
      "(3.571053721318135e-05, 1.8084510990070146)\n",
      "(3.288635992801332e-05, 1.808110038215448)\n",
      "(3.0286583245399196e-05, 1.807782876492069)\n",
      "(2.789325502542969e-05, 1.8074690367552222)\n",
      "(2.568987348854938e-05, 1.8071679667254459)\n",
      "(2.3661268389724598e-05, 1.8068791377933175)\n",
      "(2.1793492119986454e-05, 1.806602043944111)\n",
      "(2.0073719883613338e-05, 1.8063362007360269)\n",
      "(1.8490158174882674e-05, 1.8060811443289702)\n",
      "(1.7031960846957754e-05, 1.8058364305610464)\n",
      "(1.5689152127840068e-05, 1.8056016340701335)\n",
      "(1.4452555994837611e-05, 1.8053763474580495)\n",
      "(1.3313731370434795e-05, 1.8051601804949895)\n",
      "(1.2264912649272614e-05, 1.8049527593620673)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def run_basic(inputs, x, y, z, a, b, desired, lr):\n",
    "    \"\"\"\n",
    "    inputs: list of length x\n",
    "    a, b: lists of length x+y+z (weights & biases for each neuron in order: input-layer, hidden-layer, output-layer)\n",
    "    desired: scalar target\n",
    "    lr: learning rate\n",
    "    Mutates a and b in-place.\n",
    "    Returns: (loss, avg_output)\n",
    "    \"\"\"\n",
    "    global m, c, layer_bias\n",
    "\n",
    "    # === Forward pass ===\n",
    "    delays = []\n",
    "    # input layer\n",
    "    total_in = 0.0\n",
    "    for k in range(x):\n",
    "        E = -m * inputs[k] + b[k] + c + 0 * layer_bias\n",
    "        u = (1 / a[k]) + np.exp(E)\n",
    "        f = a[k] / u\n",
    "        δ = f + inputs[k]\n",
    "        delays.append(δ)\n",
    "        total_in += δ\n",
    "    avg_input = total_in / x\n",
    "\n",
    "    # hidden layer\n",
    "    total_h = 0.0\n",
    "    for j in range(y):\n",
    "        idx = x + j\n",
    "        E = -m * avg_input + b[idx] + c + 1 * layer_bias\n",
    "        u = (1 / a[idx]) + np.exp(E)\n",
    "        f = a[idx] / u\n",
    "        δ = f + avg_input\n",
    "        delays.append(δ)\n",
    "        total_h += δ\n",
    "    avg_hidden = total_h / y\n",
    "\n",
    "    # output layer\n",
    "    total_o = 0.0\n",
    "    for ℓ in range(z):\n",
    "        idx = x + y + ℓ\n",
    "        E = -m * avg_hidden + b[idx] + c + 2 * layer_bias\n",
    "        u = (1 / a[idx]) + np.exp(E)\n",
    "        f = a[idx] / u\n",
    "        δ = f + avg_hidden\n",
    "        delays.append(δ)\n",
    "        total_o += δ\n",
    "    avg_output = total_o / z\n",
    "\n",
    "    # compute loss\n",
    "    loss = calc_loss(avg_output, desired)\n",
    "\n",
    "    # === Backward pass ===\n",
    "    N = x + y + z\n",
    "    grads_a = [0.0] * N\n",
    "    grads_b = [0.0] * N\n",
    "\n",
    "    # dL/dO and delta for each output neuron\n",
    "    dL_dO = avg_output - desired\n",
    "    for ℓ in range(z):\n",
    "        idx = x + y + ℓ\n",
    "        # chain through average\n",
    "        dL_dδ = dL_dO * (1 / z)\n",
    "\n",
    "        # derivative of f wrt a[idx], b[idx]\n",
    "        E = -m * avg_hidden + b[idx] + c + 2 * layer_bias\n",
    "        expE = np.exp(E)\n",
    "        u = (1 / a[idx]) + expE\n",
    "\n",
    "        # ∂f/∂a and ∂f/∂b for f = a/u\n",
    "        df_da = 1/u + 1/(u * u * a[idx])\n",
    "        df_db = -a[idx] * expE / (u * u)\n",
    "\n",
    "        grads_a[idx] = dL_dδ * df_da\n",
    "        grads_b[idx] = dL_dδ * df_db\n",
    "\n",
    "    # backprop into avg_hidden\n",
    "    dL_dh = dL_dO  # because each output δ adds directly to avg_hidden with weight 1/z, summed z times → cancels\n",
    "\n",
    "    # hidden layer\n",
    "    for j in range(y):\n",
    "        idx = x + j\n",
    "        # each hidden δ contributes equally to avg_hidden\n",
    "        dL_dδ = dL_dh * (1 / y)\n",
    "\n",
    "        E = -m * avg_input + b[idx] + c + 1 * layer_bias\n",
    "        expE = np.exp(E)\n",
    "        u = (1 / a[idx]) + expE\n",
    "\n",
    "        df_da = 1/u + 1/(u * u * a[idx])\n",
    "        df_db = -a[idx] * expE / (u * u)\n",
    "\n",
    "        grads_a[idx] = dL_dδ * df_da\n",
    "        grads_b[idx] = dL_dδ * df_db\n",
    "\n",
    "    # backprop into avg_input\n",
    "    dL_di = dL_dh  # because sum over hidden then 1/y cancels\n",
    "\n",
    "    # input layer\n",
    "    for k in range(x):\n",
    "        idx = k\n",
    "        dL_dδ = dL_di * (1 / x)\n",
    "\n",
    "        E = -m * inputs[k] + b[idx] + c + 0 * layer_bias\n",
    "        expE = np.exp(E)\n",
    "        u = (1 / a[idx]) + expE\n",
    "\n",
    "        df_da = 1/u + 1/(u * u * a[idx])\n",
    "        df_db = -a[idx] * expE / (u * u)\n",
    "\n",
    "        grads_a[idx] = dL_dδ * df_da\n",
    "        grads_b[idx] = dL_dδ * df_db\n",
    "\n",
    "    # === Parameter update ===\n",
    "    for i in range(N):\n",
    "        a[i] -= lr * grads_a[i]\n",
    "        b[i] -= lr * grads_b[i]\n",
    "\n",
    "    return loss, avg_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(14)\n",
    "x = 4  # l;1 \n",
    "inputs = [0.5, 0.8, 0.6, 0.9]\n",
    "y = 10 # #l2 \n",
    "z = 3   # l3\n",
    "\n",
    "a = []\n",
    "for i in range(x+y+z):\n",
    "    a.append(random.uniform(0.05, -0.05) + 1.0) \n",
    "\n",
    "b = []\n",
    "for i in range(x+y+z):\n",
    "    b.append(random.uniform(0.05, -0.05))\n",
    "\n",
    "\n",
    "m = 0.5\n",
    "c = 1 \n",
    "\n",
    "desired = 1.8\n",
    "\n",
    "for i in range(100):\n",
    "    print(run_basic(inputs, x,y,z, a, b, desired, 0.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bad6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820821300824607 1.1543436937742046 1.4860303144480338 2.163685150377593\n"
     ]
    }
   ],
   "source": [
    "s1 = ians_sigmoid(inputs[0], 1, 0, 0.5, 1, layer=0)  + inputs[0]\n",
    "\n",
    "s2 = ians_sigmoid(inputs[1], 1, 0, 0.5, 1, layer=0) + inputs[1]\n",
    "\n",
    "s3 = ians_sigmoid((s1 + s2)/2, 1, 0, 0.5, 1, layer=1) + (s1 + s2)/2\n",
    "\n",
    "s4 = ians_sigmoid(s3, 1, 0, 0.5, 1, layer=2) + s3\n",
    "\n",
    "print(s1, s2, s3, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc72e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f499576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — Loss: 1.38932 — Predicted: [0.05066474 0.12981888]\n",
      "Epoch  2 — Loss: 1.38057 — Predicted: [0.05147972 0.13564856]\n",
      "Epoch  3 — Loss: 1.37130 — Predicted: [0.05231687 0.14187985]\n",
      "Epoch  4 — Loss: 1.36145 — Predicted: [0.05317686 0.14854387]\n",
      "Epoch  5 — Loss: 1.35101 — Predicted: [0.05406037 0.15567347]\n",
      "Epoch  6 — Loss: 1.33992 — Predicted: [0.05496807 0.16330303]\n",
      "Epoch  7 — Loss: 1.32816 — Predicted: [0.05590063 0.17146811]\n",
      "Epoch  8 — Loss: 1.31569 — Predicted: [0.05685871 0.18020503]\n",
      "Epoch  9 — Loss: 1.30247 — Predicted: [0.05784296 0.18955019]\n",
      "Epoch 10 — Loss: 1.28848 — Predicted: [0.05885402 0.19953927]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Standard sigmoid + derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(hidden_size, input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        self.z1 = self.W1 @ x + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        # Output layer\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        return 0.5 * np.sum((y_hat - y) ** 2)\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # 1) Output error\n",
    "        delta2 = (self.a2 - y) * sigmoid_deriv(self.z2)\n",
    "        # 2) Hidden error\n",
    "        delta1 = (self.W2.T @ delta2) * sigmoid_deriv(self.z1)\n",
    "\n",
    "        # Parameter gradients\n",
    "        self.dW2 = delta2 @ self.a1.T\n",
    "        self.db2 = delta2\n",
    "        self.dW1 = delta1 @ x.T\n",
    "        self.db1 = delta1\n",
    "\n",
    "    def update_params(self, lr):\n",
    "        # Gradient descent step\n",
    "        self.W2 -= lr * self.dW2\n",
    "        self.b2 -= lr * self.db2\n",
    "        self.W1 -= lr * self.dW1\n",
    "        self.b1 -= lr * self.db1\n",
    "\n",
    "    def train_step(self, x, y, lr=0.1):\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.compute_loss(y_hat, y)\n",
    "        self.backward(x, y)\n",
    "        self.update_params(lr)\n",
    "        return loss, y_hat\n",
    "\n",
    "# Example: X=3 inputs, Y=4 hidden, Z=2 outputs\n",
    "np.random.seed(42)\n",
    "model = NeuralNetwork(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Dummy data (column vectors)\n",
    "x = np.array([[0.8], [0.1], [0.3]])\n",
    "y = np.array([[1.0], [1.5]])\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss, y_hat = model.train_step(x, y, lr)\n",
    "    # Flatten prediction for readability\n",
    "    prediction = y_hat.flatten()\n",
    "    print(f\"Epoch {epoch:2d} — Loss: {loss:.5f} — Predicted: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e55198d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — Loss: 49.64083 — Predicted (delayed): [7.52598245 7.52949007]\n",
      "Epoch  2 — Loss: 49.64068 — Predicted (delayed): [7.52597187 7.52947909]\n",
      "Epoch  3 — Loss: 49.64053 — Predicted (delayed): [7.52596128 7.52946809]\n",
      "Epoch  4 — Loss: 49.64038 — Predicted (delayed): [7.52595069 7.52945709]\n",
      "Epoch  5 — Loss: 49.64023 — Predicted (delayed): [7.52594009 7.52944608]\n",
      "Epoch  6 — Loss: 49.64007 — Predicted (delayed): [7.52592948 7.52943506]\n",
      "Epoch  7 — Loss: 49.63992 — Predicted (delayed): [7.52591887 7.52942404]\n",
      "Epoch  8 — Loss: 49.63977 — Predicted (delayed): [7.52590824 7.529413  ]\n",
      "Epoch  9 — Loss: 49.63962 — Predicted (delayed): [7.52589762 7.52940197]\n",
      "Epoch 10 — Loss: 49.63946 — Predicted (delayed): [7.52588698 7.52939092]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Standard sigmoid + derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class SNN_Network:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(hidden_size, input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sum of raw inputs (scalar)\n",
    "        sum_in = np.sum(x)\n",
    "        # Hidden layer pre-activation\n",
    "        self.z1 = self.W1 @ x + self.b1\n",
    "        # Hidden activation (\"spike response\")\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        # Delay accumulator: add summed raw input to each hidden neuron\n",
    "        self.d1 = self.a1 + sum_in\n",
    "        \n",
    "        # Sum of delayed-hidden (scalar)\n",
    "        sum_h = np.sum(self.d1)\n",
    "        # Output layer pre-activation\n",
    "        self.z2 = self.W2 @ self.d1 + self.b2\n",
    "        # Output activation\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        # Delay accumulator for output\n",
    "        self.d2 = self.a2 + sum_h\n",
    "        \n",
    "        return self.d2  # use the delayed output as the final \"guess\"\n",
    "\n",
    "    def compute_loss(self, y_pred, y):\n",
    "        return 0.5 * np.sum((y_pred - y) ** 2)\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # Backprop through the final sigmoid\n",
    "        delta2 = (self.a2 - y) * sigmoid_deriv(self.z2)\n",
    "        # Gradients for W2 and b2\n",
    "        self.dW2 = delta2 @ self.d1.T\n",
    "        self.db2 = delta2\n",
    "        \n",
    "        # Backprop into hidden (through W2 and sigmoid)\n",
    "        delta1 = (self.W2.T @ delta2) * sigmoid_deriv(self.z1)\n",
    "        # Gradients for W1 and b1\n",
    "        self.dW1 = delta1 @ x.T\n",
    "        self.db1 = delta1\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.W2 -= lr * self.dW2\n",
    "        self.b2 -= lr * self.db2\n",
    "        self.W1 -= lr * self.dW1\n",
    "        self.b1 -= lr * self.db1\n",
    "\n",
    "    def train_step(self, x, y, lr=0.1):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        self.backward(x, y)\n",
    "        self.update(lr)\n",
    "        return loss, y_pred\n",
    "\n",
    "# Example: X=3 inputs, Y=4 hidden, Z=2 outputs, summing inputs at each layer\n",
    "np.random.seed(42)\n",
    "model = SNN_Network(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Dummy data (column vectors)\n",
    "x = np.array([[0.8], [0.1], [0.3]])\n",
    "y = np.array([[1.0], [0.0]])\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss, y_pred = model.train_step(x, y, lr)\n",
    "    print(f\"Epoch {epoch:2d} — Loss: {loss:.5f} — Predicted (delayed): {y_pred.flatten()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
