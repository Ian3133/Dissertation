{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb51b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Weights Layer 1:\n",
      "[[0.38634416 0.90828617]\n",
      " [0.70844354 0.58685631]]\n",
      "Weights Layer 2:\n",
      "[[0.17116176 0.19497657]\n",
      " [0.07925506 0.83638947]]\n",
      "\n",
      "Epoch 2\n",
      "Weights Layer 1:\n",
      "[[0.38591538 0.91003255]\n",
      " [0.70793498 0.58247045]]\n",
      "Weights Layer 2:\n",
      "[[0.1662311  0.18651457]\n",
      " [0.07786964 0.82978903]]\n",
      "\n",
      "Epoch 3\n",
      "Weights Layer 1:\n",
      "[[0.38555726 0.91125248]\n",
      " [0.70739396 0.57692537]]\n",
      "Weights Layer 2:\n",
      "[[0.16831792 0.17169738]\n",
      " [0.08332401 0.81667725]]\n",
      "\n",
      "Epoch 4\n",
      "Weights Layer 1:\n",
      "[[0.38498439 0.91379412]\n",
      " [0.70707118 0.57444332]]\n",
      "Weights Layer 2:\n",
      "[[0.15295808 0.17333288]\n",
      " [0.06976976 0.82055254]]\n",
      "\n",
      "Epoch 5\n",
      "Weights Layer 1:\n",
      "[[0.38451942 0.91671065]\n",
      " [0.70676444 0.57252564]]\n",
      "Weights Layer 2:\n",
      "[[0.1344237  0.17815774]\n",
      " [0.06317202 0.82772231]]\n",
      "\n",
      "Epoch 6\n",
      "Weights Layer 1:\n",
      "[[0.38430123 0.91800031]\n",
      " [0.70615499 0.56692903]]\n",
      "Weights Layer 2:\n",
      "[[0.13703314 0.16341554]\n",
      " [0.07551434 0.81476102]]\n",
      "\n",
      "Epoch 7\n",
      "Weights Layer 1:\n",
      "[[0.38410007 0.91843664]\n",
      " [0.70555532 0.55972641]]\n",
      "Weights Layer 2:\n",
      "[[0.14990802 0.13911948]\n",
      " [0.09094776 0.79201896]]\n",
      "\n",
      "Epoch 8\n",
      "Weights Layer 1:\n",
      "[[0.38366436 0.92016211]\n",
      " [0.70529772 0.55567957]]\n",
      "Weights Layer 2:\n",
      "[[0.14508123 0.13148258]\n",
      " [0.08308061 0.78647936]]\n",
      "\n",
      "Epoch 9\n",
      "Weights Layer 1:\n",
      "[[0.38349057 0.92008328]\n",
      " [0.70504014 0.54770066]]\n",
      "Weights Layer 2:\n",
      "[[0.16472311 0.10101046]\n",
      " [0.10261319 0.75749185]]\n",
      "\n",
      "Epoch 10\n",
      "Weights Layer 1:\n",
      "[[0.38310113 0.92076345]\n",
      " [0.70515737 0.54181585]]\n",
      "Weights Layer 2:\n",
      "[[0.17348781 0.08079627]\n",
      " [0.10907122 0.73913008]]\n",
      "\n",
      "Epoch 11\n",
      "Weights Layer 1:\n",
      "[[0.38220086 0.9228952 ]\n",
      " [0.70517216 0.539209  ]]\n",
      "Weights Layer 2:\n",
      "[[0.16128984 0.08510123]\n",
      " [0.0933494  0.74158251]]\n",
      "\n",
      "Epoch 12\n",
      "Weights Layer 1:\n",
      "[[0.38173983 0.92386715]\n",
      " [0.70536939 0.53395079]]\n",
      "Weights Layer 2:\n",
      "[[0.16673816 0.07328573]\n",
      " [0.09624171 0.72694805]]\n",
      "\n",
      "Epoch 13\n",
      "Weights Layer 1:\n",
      "[[0.38101359 0.92557661]\n",
      " [0.70551743 0.5303642 ]]\n",
      "Weights Layer 2:\n",
      "[[0.16169146 0.07570278]\n",
      " [0.08802844 0.72282624]]\n",
      "\n",
      "Epoch 14\n",
      "Weights Layer 1:\n",
      "[[0.38039858 0.92706942]\n",
      " [0.7056923  0.52626314]]\n",
      "Weights Layer 2:\n",
      "[[0.16026344 0.07458494]\n",
      " [0.08510738 0.71533264]]\n",
      "\n",
      "Epoch 15\n",
      "Weights Layer 1:\n",
      "[[0.37972184 0.92881969]\n",
      " [0.70582069 0.5227084 ]]\n",
      "Weights Layer 2:\n",
      "[[0.15541689 0.07648857]\n",
      " [0.08028963 0.71136758]]\n",
      "\n",
      "Epoch 16\n",
      "Weights Layer 1:\n",
      "[[0.37908314 0.93058698]\n",
      " [0.7059275  0.51915947]]\n",
      "Weights Layer 2:\n",
      "[[0.15067925 0.07754916]\n",
      " [0.0781609  0.70744819]]\n",
      "\n",
      "Epoch 17\n",
      "Weights Layer 1:\n",
      "[[0.37859919 0.93185234]\n",
      " [0.70609995 0.51461544]]\n",
      "Weights Layer 2:\n",
      "[[0.15296571 0.07303289]\n",
      " [0.08327723 0.69675278]]\n",
      "\n",
      "Epoch 18\n",
      "Weights Layer 1:\n",
      "[[0.37796556 0.93358519]\n",
      " [0.70621824 0.51116618]]\n",
      "Weights Layer 2:\n",
      "[[0.14822883 0.07577398]\n",
      " [0.07947003 0.69307086]]\n",
      "\n",
      "Epoch 19\n",
      "Weights Layer 1:\n",
      "[[0.37749078 0.93483253]\n",
      " [0.70639357 0.5067376 ]]\n",
      "Weights Layer 2:\n",
      "[[0.1505345  0.07232638]\n",
      " [0.08396548 0.6825873 ]]\n",
      "\n",
      "Epoch 20\n",
      "Weights Layer 1:\n",
      "[[0.37674673 0.93702455]\n",
      " [0.70641832 0.50433866]]\n",
      "Weights Layer 2:\n",
      "[[0.13887963 0.08012434]\n",
      " [0.0731448  0.68595515]]\n",
      "\n",
      "Epoch 21\n",
      "Weights Layer 1:\n",
      "[[0.37612454 0.93928223]\n",
      " [0.70639883 0.50189625]]\n",
      "Weights Layer 2:\n",
      "[[0.12746554 0.0850984 ]\n",
      " [0.06944584 0.68921528]]\n",
      "\n",
      "Epoch 22\n",
      "Weights Layer 1:\n",
      "[[0.37569036 0.9410593 ]\n",
      " [0.70638473 0.49843282]]\n",
      "Weights Layer 2:\n",
      "[[0.12316789 0.08237386]\n",
      " [0.07341054 0.68557768]]\n",
      "\n",
      "Epoch 23\n",
      "Weights Layer 1:\n",
      "[[0.37531257 0.94256441]\n",
      " [0.70639134 0.49451504]]\n",
      "Weights Layer 2:\n",
      "[[0.1223524  0.07810668]\n",
      " [0.07798136 0.6786152 ]]\n",
      "\n",
      "Epoch 24\n",
      "Weights Layer 1:\n",
      "[[0.37496826 0.94379946]\n",
      " [0.70644278 0.4901837 ]]\n",
      "Weights Layer 2:\n",
      "[[0.12497422 0.07343899]\n",
      " [0.08324963 0.66837501]]\n",
      "\n",
      "Epoch 25\n",
      "Weights Layer 1:\n",
      "[[0.37460037 0.94500676]\n",
      " [0.70653651 0.48595893]]\n",
      "Weights Layer 2:\n",
      "[[0.12751004 0.07142423]\n",
      " [0.08607344 0.65830788]]\n",
      "\n",
      "Epoch 26\n",
      "Weights Layer 1:\n",
      "[[0.37407342 0.94689544]\n",
      " [0.7065607  0.48320907]]\n",
      "Weights Layer 2:\n",
      "[[0.11956305 0.07745087]\n",
      " [0.07732343 0.65865495]]\n",
      "\n",
      "Epoch 27\n",
      "Weights Layer 1:\n",
      "[[0.37370418 0.94798142]\n",
      " [0.70658545 0.47949269]]\n",
      "Weights Layer 2:\n",
      "[[0.11875302 0.07584816]\n",
      " [0.07986666 0.65211642]]\n",
      "\n",
      "Epoch 28\n",
      "Weights Layer 1:\n",
      "[[0.37325553 0.94882508]\n",
      " [0.70657216 0.47675378]]\n",
      "Weights Layer 2:\n",
      "[[0.11097125 0.08011874]\n",
      " [0.07488887 0.65252463]]\n",
      "\n",
      "Epoch 29\n",
      "Weights Layer 1:\n",
      "[[0.37284338 0.94926464]\n",
      " [0.706509   0.47445981]]\n",
      "Weights Layer 2:\n",
      "[[0.09984937 0.08536596]\n",
      " [0.07007979 0.65631509]]\n",
      "\n",
      "Epoch 30\n",
      "Weights Layer 1:\n",
      "[[0.3725575  0.94933022]\n",
      " [0.70641033 0.47166314]]\n",
      "Weights Layer 2:\n",
      "[[0.09237847 0.08572448]\n",
      " [0.07109175 0.65660476]]\n",
      "\n",
      "Epoch 31\n",
      "Weights Layer 1:\n",
      "[[0.3723762  0.94911541]\n",
      " [0.70628264 0.46745448]]\n",
      "Weights Layer 2:\n",
      "[[0.09542339 0.07695066]\n",
      " [0.07958567 0.64667665]]\n",
      "\n",
      "Epoch 32\n",
      "Weights Layer 1:\n",
      "[[0.37215891 0.94901254]\n",
      " [0.70622679 0.46337052]]\n",
      "Weights Layer 2:\n",
      "[[0.09834012 0.07310967]\n",
      " [0.08416112 0.63695479]]\n",
      "\n",
      "Epoch 33\n",
      "Weights Layer 1:\n",
      "[[0.37189134 0.94906185]\n",
      " [0.70619887 0.45983181]]\n",
      "Weights Layer 2:\n",
      "[[0.09769494 0.07384536]\n",
      " [0.08327966 0.63080953]]\n",
      "\n",
      "Epoch 34\n",
      "Weights Layer 1:\n",
      "[[0.37152347 0.94945454]\n",
      " [0.70612575 0.45809777]]\n",
      "Weights Layer 2:\n",
      "[[0.08314881 0.08395013]\n",
      " [0.06969838 0.63841198]]\n",
      "\n",
      "Epoch 35\n",
      "Weights Layer 1:\n",
      "[[0.37133214 0.94949032]\n",
      " [0.70600167 0.45585096]]\n",
      "Weights Layer 2:\n",
      "[[0.07368325 0.08794747]\n",
      " [0.06833688 0.64240658]]\n",
      "\n",
      "Epoch 36\n",
      "Weights Layer 1:\n",
      "[[0.37124971 0.94914232]\n",
      " [0.70575912 0.45126953]]\n",
      "Weights Layer 2:\n",
      "[[0.08500088 0.07491066]\n",
      " [0.08068093 0.6293053 ]]\n",
      "\n",
      "Epoch 37\n",
      "Weights Layer 1:\n",
      "[[0.3710415  0.94921739]\n",
      " [0.70567097 0.44820734]]\n",
      "Weights Layer 2:\n",
      "[[0.0812132  0.07726455]\n",
      " [0.0784221  0.62670978]]\n",
      "\n",
      "Epoch 38\n",
      "Weights Layer 1:\n",
      "[[0.37090822 0.94900218]\n",
      " [0.70554795 0.44382312]]\n",
      "Weights Layer 2:\n",
      "[[0.08938966 0.07077306]\n",
      " [0.08656506 0.61390784]]\n",
      "\n",
      "Epoch 39\n",
      "Weights Layer 1:\n",
      "[[0.37068395 0.94907677]\n",
      " [0.70550203 0.44045012]]\n",
      "Weights Layer 2:\n",
      "[[0.08882191 0.07284571]\n",
      " [0.08442628 0.60814819]]\n",
      "\n",
      "Epoch 40\n",
      "Weights Layer 1:\n",
      "[[0.37053411 0.94885248]\n",
      " [0.70545713 0.43583472]]\n",
      "Weights Layer 2:\n",
      "[[0.09872725 0.0667359 ]\n",
      " [0.09336253 0.5922065 ]]\n",
      "\n",
      "Epoch 41\n",
      "Weights Layer 1:\n",
      "[[0.37028051 0.94892647]\n",
      " [0.70548709 0.43223525]]\n",
      "Weights Layer 2:\n",
      "[[0.10146409 0.06889778]\n",
      " [0.09277022 0.58334317]]\n",
      "\n",
      "Epoch 42\n",
      "Weights Layer 1:\n",
      "[[0.3701143  0.94869488]\n",
      " [0.70556488 0.42750398]]\n",
      "Weights Layer 2:\n",
      "[[0.11461145 0.06327511]\n",
      " [0.10300242 0.56430869]]\n",
      "\n",
      "Epoch 43\n",
      "Weights Layer 1:\n",
      "[[0.36973694 0.94897407]\n",
      " [0.70565826 0.424538  ]]\n",
      "Weights Layer 2:\n",
      "[[0.11359673 0.06947571]\n",
      " [0.09847183 0.55924819]]\n",
      "\n",
      "Epoch 44\n",
      "Weights Layer 1:\n",
      "[[0.36942523 0.9490261 ]\n",
      " [0.70573789 0.42120736]]\n",
      "Weights Layer 2:\n",
      "[[0.11611981 0.07031529]\n",
      " [0.09760732 0.5507716 ]]\n",
      "\n",
      "Epoch 45\n",
      "Weights Layer 1:\n",
      "[[0.36906604 0.94914698]\n",
      " [0.70579928 0.41830377]]\n",
      "Weights Layer 2:\n",
      "[[0.1151533  0.07304392]\n",
      " [0.09309684 0.54582502]]\n",
      "\n",
      "Epoch 46\n",
      "Weights Layer 1:\n",
      "[[0.36879731 0.94905026]\n",
      " [0.70588787 0.41467691]]\n",
      "Weights Layer 2:\n",
      "[[0.12119797 0.0695397 ]\n",
      " [0.09586029 0.53406682]]\n",
      "\n",
      "Epoch 47\n",
      "Weights Layer 1:\n",
      "[[0.3684542  0.94908884]\n",
      " [0.70600376 0.41152171]]\n",
      "Weights Layer 2:\n",
      "[[0.12367351 0.07049481]\n",
      " [0.09489869 0.52591555]]\n",
      "\n",
      "Epoch 48\n",
      "Weights Layer 1:\n",
      "[[0.36814731 0.94903635]\n",
      " [0.70615537 0.40807124]]\n",
      "Weights Layer 2:\n",
      "[[0.12960738 0.06863846]\n",
      " [0.09751371 0.51442796]]\n",
      "\n",
      "Epoch 49\n",
      "Weights Layer 1:\n",
      "[[0.36732832 0.94983635]\n",
      " [0.70599706 0.40812529]]\n",
      "Weights Layer 2:\n",
      "[[0.10074058 0.09042359]\n",
      " [0.06397574 0.53745626]]\n",
      "\n",
      "Epoch 50\n",
      "Weights Layer 1:\n",
      "[[0.36716345 0.94944911]\n",
      " [0.70589458 0.40438912]]\n",
      "Weights Layer 2:\n",
      "[[0.10728139 0.07645091]\n",
      " [0.07761631 0.52569733]]\n",
      "\n",
      "Epoch 51\n",
      "Weights Layer 1:\n",
      "[[0.36687982 0.94936632]\n",
      " [0.70589636 0.40156486]]\n",
      "Weights Layer 2:\n",
      "[[0.10664458 0.07623883]\n",
      " [0.07970165 0.52106213]]\n",
      "\n",
      "Epoch 52\n",
      "Weights Layer 1:\n",
      "[[0.36677731 0.948928  ]\n",
      " [0.7059841  0.39663692]]\n",
      "Weights Layer 2:\n",
      "[[0.12685095 0.06046588]\n",
      " [0.09915618 0.49594158]]\n",
      "\n",
      "Epoch 53\n",
      "Weights Layer 1:\n",
      "[[0.36653706 0.94880183]\n",
      " [0.70630598 0.39281893]]\n",
      "Weights Layer 2:\n",
      "[[0.13965241 0.06112401]\n",
      " [0.1088113  0.47808233]]\n",
      "\n",
      "Epoch 54\n",
      "Weights Layer 1:\n",
      "[[0.36596972 0.94919111]\n",
      " [0.70646152 0.3907506 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13486556 0.07088311]\n",
      " [0.10022278 0.47762949]]\n",
      "\n",
      "Epoch 55\n",
      "Weights Layer 1:\n",
      "[[0.36552275 0.94929049]\n",
      " [0.70659569 0.38834092]]\n",
      "Weights Layer 2:\n",
      "[[0.13369394 0.07380808]\n",
      " [0.09536632 0.4736521 ]]\n",
      "\n",
      "Epoch 56\n",
      "Weights Layer 1:\n",
      "[[0.36504605 0.94940627]\n",
      " [0.70667695 0.38624375]]\n",
      "Weights Layer 2:\n",
      "[[0.12912171 0.07780664]\n",
      " [0.08697559 0.47314058]]\n",
      "\n",
      "Epoch 57\n",
      "Weights Layer 1:\n",
      "[[0.36465776 0.94939328]\n",
      " [0.70675605 0.38380429]]\n",
      "Weights Layer 2:\n",
      "[[0.12816234 0.07725659]\n",
      " [0.0841111  0.469169  ]]\n",
      "\n",
      "Epoch 58\n",
      "Weights Layer 1:\n",
      "[[0.3643165  0.94932714]\n",
      " [0.70686437 0.38106704]]\n",
      "Weights Layer 2:\n",
      "[[0.13071824 0.07430931]\n",
      " [0.08595716 0.46182553]]\n",
      "\n",
      "Epoch 59\n",
      "Weights Layer 1:\n",
      "[[0.36386435 0.94942626]\n",
      " [0.70693862 0.37900559]]\n",
      "Weights Layer 2:\n",
      "[[0.12628755 0.07816248]\n",
      " [0.08021536 0.4614811 ]]\n",
      "\n",
      "Epoch 60\n",
      "Weights Layer 1:\n",
      "[[0.36349615 0.9494086 ]\n",
      " [0.70700902 0.37661448]]\n",
      "Weights Layer 2:\n",
      "[[0.12544034 0.07750724]\n",
      " [0.08074928 0.45767625]]\n",
      "\n",
      "Epoch 61\n",
      "Weights Layer 1:\n",
      "[[0.36317103 0.94934355]\n",
      " [0.70710673 0.37394551]]\n",
      "Weights Layer 2:\n",
      "[[0.12807038 0.0744871 ]\n",
      " [0.0841072  0.4504989 ]]\n",
      "\n",
      "Epoch 62\n",
      "Weights Layer 1:\n",
      "[[0.3627381  0.94944089]\n",
      " [0.70717425 0.37194269]]\n",
      "Weights Layer 2:\n",
      "[[0.1236957  0.07833886]\n",
      " [0.07937466 0.45031843]]\n",
      "\n",
      "Epoch 63\n",
      "Weights Layer 1:\n",
      "[[0.36246372 0.94930933]\n",
      " [0.7072852  0.36901984]]\n",
      "Weights Layer 2:\n",
      "[[0.12983643 0.0721327 ]\n",
      " [0.08637673 0.43980689]]\n",
      "\n",
      "Epoch 64\n",
      "Weights Layer 1:\n",
      "[[0.36210333 0.94930902]\n",
      " [0.70743681 0.36651334]]\n",
      "Weights Layer 2:\n",
      "[[0.13236329 0.07220165]\n",
      " [0.08709891 0.4329213 ]]\n",
      "\n",
      "Epoch 65\n",
      "Weights Layer 1:\n",
      "[[0.36168492 0.94937768]\n",
      " [0.70756447 0.3643415 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13138798 0.07474744]\n",
      " [0.08405621 0.42956375]]\n",
      "\n",
      "Epoch 66\n",
      "Weights Layer 1:\n",
      "[[0.36123692 0.94947143]\n",
      " [0.70764332 0.36245754]]\n",
      "Weights Layer 2:\n",
      "[[0.12698959 0.07862923]\n",
      " [0.07929184 0.42966833]]\n",
      "\n",
      "Epoch 67\n",
      "Weights Layer 1:\n",
      "[[0.36087249 0.94945634]\n",
      " [0.70771844 0.36026976]]\n",
      "Weights Layer 2:\n",
      "[[0.12616258 0.07794277]\n",
      " [0.08020671 0.42630456]]\n",
      "\n",
      "Epoch 68\n",
      "Weights Layer 1:\n",
      "[[0.36047045 0.94950691]\n",
      " [0.70776793 0.35838898]]\n",
      "Weights Layer 2:\n",
      "[[0.12186639 0.08038735]\n",
      " [0.07758792 0.42643371]]\n",
      "\n",
      "Epoch 69\n",
      "Weights Layer 1:\n",
      "[[0.36003274 0.94963289]\n",
      " [0.70774948 0.35706224]]\n",
      "Weights Layer 2:\n",
      "[[0.11070366 0.0874801 ]\n",
      " [0.07056168 0.43341335]]\n",
      "\n",
      "Epoch 70\n",
      "Weights Layer 1:\n",
      "[[0.35976206 0.9495753 ]\n",
      " [0.70770751 0.35510073]]\n",
      "Weights Layer 2:\n",
      "[[0.10668064 0.08551687]\n",
      " [0.07336119 0.43337845]]\n",
      "\n",
      "Epoch 71\n",
      "Weights Layer 1:\n",
      "[[0.35960135 0.94935158]\n",
      " [0.70769374 0.35198522]]\n",
      "Weights Layer 2:\n",
      "[[0.11656874 0.07192219]\n",
      " [0.08556182 0.41966628]]\n",
      "\n",
      "Epoch 72\n",
      "Weights Layer 1:\n",
      "[[0.35926918 0.94940883]\n",
      " [0.7077676  0.3498946 ]]\n",
      "Weights Layer 2:\n",
      "[[0.11579594 0.07469002]\n",
      " [0.08331723 0.41654322]]\n",
      "\n",
      "Epoch 73\n",
      "Weights Layer 1:\n",
      "[[0.3589172  0.94949458]\n",
      " [0.70780199 0.3480833 ]]\n",
      "Weights Layer 2:\n",
      "[[0.11158646 0.07868533]\n",
      " [0.07898099 0.41687166]]\n",
      "\n",
      "Epoch 74\n",
      "Weights Layer 1:\n",
      "[[0.35875762 0.94926936]\n",
      " [0.70788797 0.3448822 ]]\n",
      "Weights Layer 2:\n",
      "[[0.12483345 0.06682925]\n",
      " [0.09210028 0.40000921]]\n",
      "\n",
      "Epoch 75\n",
      "Weights Layer 1:\n",
      "[[0.35836446 0.94938784]\n",
      " [0.7080209  0.34294864]]\n",
      "Weights Layer 2:\n",
      "[[0.12390687 0.07230085]\n",
      " [0.08727348 0.3971963 ]]\n",
      "\n",
      "Epoch 76\n",
      "Weights Layer 1:\n",
      "[[0.35807986 0.94933526]\n",
      " [0.70818125 0.34050469]]\n",
      "Weights Layer 2:\n",
      "[[0.12998379 0.07001783]\n",
      " [0.09089159 0.38749518]]\n",
      "\n",
      "Epoch 77\n",
      "Weights Layer 1:\n",
      "[[0.35771897 0.94937286]\n",
      " [0.7083526  0.3384016 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13248762 0.07156153]\n",
      " [0.08956891 0.38137281]]\n",
      "\n",
      "Epoch 78\n",
      "Weights Layer 1:\n",
      "[[0.3572585  0.94950739]\n",
      " [0.70845459 0.3368105 ]]\n",
      "Weights Layer 2:\n",
      "[[0.12804649 0.07723539]\n",
      " [0.08162521 0.38219366]]\n",
      "\n",
      "Epoch 79\n",
      "Weights Layer 1:\n",
      "[[0.35676845 0.94965986]\n",
      " [0.70846136 0.33567211]]\n",
      "Weights Layer 2:\n",
      "[[0.11680296 0.08582363]\n",
      " [0.0718594  0.38982715]]\n",
      "\n",
      "Epoch 80\n",
      "Weights Layer 1:\n",
      "[[0.35649314 0.94957766]\n",
      " [0.70846712 0.33370549]]\n",
      "Weights Layer 2:\n",
      "[[0.11619497 0.08174903]\n",
      " [0.07645621 0.38698359]]\n",
      "\n",
      "Epoch 81\n",
      "Weights Layer 1:\n",
      "[[0.35629876 0.94940455]\n",
      " [0.70854469 0.33102435]]\n",
      "Weights Layer 2:\n",
      "[[0.12595858 0.07082731]\n",
      " [0.08734123 0.37392971]]\n",
      "\n",
      "Epoch 82\n",
      "Weights Layer 1:\n",
      "[[0.35600503 0.94936785]\n",
      " [0.70872894 0.3287681 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13202427 0.06955282]\n",
      " [0.09084666 0.36456761]]\n",
      "\n",
      "Epoch 83\n",
      "Weights Layer 1:\n",
      "[[0.35568368 0.94935743]\n",
      " [0.70895222 0.32660612]]\n",
      "Weights Layer 2:\n",
      "[[0.13798617 0.06909741]\n",
      " [0.09300701 0.35533028]]\n",
      "\n",
      "Epoch 84\n",
      "Weights Layer 1:\n",
      "[[0.3552345  0.94946798]\n",
      " [0.70912557 0.32495795]]\n",
      "Weights Layer 2:\n",
      "[[0.13693026 0.07370629]\n",
      " [0.08796025 0.35309226]]\n",
      "\n",
      "Epoch 85\n",
      "Weights Layer 1:\n",
      "[[0.35490631 0.94942605]\n",
      " [0.70934668 0.32286739]]\n",
      "Weights Layer 2:\n",
      "[[0.14287429 0.07079793]\n",
      " [0.09110135 0.34395887]]\n",
      "\n",
      "Epoch 86\n",
      "Weights Layer 1:\n",
      "[[0.35433209 0.94960804]\n",
      " [0.70944344 0.3216948 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13486394 0.07954244]\n",
      " [0.07890638 0.34873698]]\n",
      "\n",
      "Epoch 87\n",
      "Weights Layer 1:\n",
      "[[0.35394323 0.94958933]\n",
      " [0.70955417 0.32002281]]\n",
      "Weights Layer 2:\n",
      "[[0.13400221 0.07890748]\n",
      " [0.0797679  0.34649103]]\n",
      "\n",
      "Epoch 88\n",
      "Weights Layer 1:\n",
      "[[0.35377378 0.94937351]\n",
      " [0.70982341 0.31728507]]\n",
      "Weights Layer 2:\n",
      "[[0.15047585 0.06430355]\n",
      " [0.09528621 0.32709409]]\n",
      "\n",
      "Epoch 89\n",
      "Weights Layer 1:\n",
      "[[0.35299477 0.9497    ]\n",
      " [0.70985605 0.31658569]]\n",
      "Weights Layer 2:\n",
      "[[0.13542863 0.08012088]\n",
      " [0.07585834 0.33906101]]\n",
      "\n",
      "Epoch 90\n",
      "Weights Layer 1:\n",
      "[[0.352738   0.94953344]\n",
      " [0.71006333 0.31432065]]\n",
      "Weights Layer 2:\n",
      "[[0.14500106 0.07047653]\n",
      " [0.08670056 0.32662978]]\n",
      "\n",
      "Epoch 91\n",
      "Weights Layer 1:\n",
      "[[0.35220715 0.94961735]\n",
      " [0.71021472 0.31302079]]\n",
      "Weights Layer 2:\n",
      "[[0.1404943  0.07702207]\n",
      " [0.08017549 0.32820892]]\n",
      "\n",
      "Epoch 92\n",
      "Weights Layer 1:\n",
      "[[0.35163672 0.94972785]\n",
      " [0.71024964 0.31208971]]\n",
      "Weights Layer 2:\n",
      "[[0.12917858 0.08612544]\n",
      " [0.07122394 0.33658774]]\n",
      "\n",
      "Epoch 93\n",
      "Weights Layer 1:\n",
      "[[0.35130694 0.94965382]\n",
      " [0.71030808 0.31045799]]\n",
      "Weights Layer 2:\n",
      "[[0.12847791 0.08223172]\n",
      " [0.07595184 0.33446224]]\n",
      "\n",
      "Epoch 94\n",
      "Weights Layer 1:\n",
      "[[0.35100352 0.94958383]\n",
      " [0.71040965 0.3086525 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13120301 0.07728797]\n",
      " [0.08111569 0.3289749 ]]\n",
      "\n",
      "Epoch 95\n",
      "Weights Layer 1:\n",
      "[[0.35054452 0.94967559]\n",
      " [0.71045941 0.30751957]]\n",
      "Weights Layer 2:\n",
      "[[0.12344642 0.0834934 ]\n",
      " [0.07465376 0.33391971]]\n",
      "\n",
      "Epoch 96\n",
      "Weights Layer 1:\n",
      "[[0.35030305 0.94955857]\n",
      " [0.71055322 0.30550075]]\n",
      "Weights Layer 2:\n",
      "[[0.12971294 0.07475567]\n",
      " [0.08319076 0.32501132]]\n",
      "\n",
      "Epoch 97\n",
      "Weights Layer 1:\n",
      "[[0.34992701 0.94958644]\n",
      " [0.7106734  0.30399973]]\n",
      "Weights Layer 2:\n",
      "[[0.12888647 0.0766829 ]\n",
      " [0.08183681 0.32316998]]\n",
      "\n",
      "Epoch 98\n",
      "Weights Layer 1:\n",
      "[[0.34968605 0.94948118]\n",
      " [0.71087106 0.3019138 ]]\n",
      "Weights Layer 2:\n",
      "[[0.13848381 0.06935236]\n",
      " [0.09051343 0.31102877]]\n",
      "\n",
      "Epoch 99\n",
      "Weights Layer 1:\n",
      "[[0.34934452 0.94947625]\n",
      " [0.7111352  0.30017166]]\n",
      "Weights Layer 2:\n",
      "[[0.14441578 0.06928058]\n",
      " [0.09247339 0.30253707]]\n",
      "\n",
      "Epoch 100\n",
      "Weights Layer 1:\n",
      "[[0.34886928 0.94956891]\n",
      " [0.71133753 0.29885066]]\n",
      "Weights Layer 2:\n",
      "[[0.14333954 0.07411909]\n",
      " [0.08726511 0.30103143]]\n",
      "\n",
      "=== Final Test ===\n",
      "Input: [[0.1 0.9]]\n",
      "Z1 (pre-activation hidden): [[0.6750907  0.36392248]]\n",
      "A1 (post-activation hidden): [[0.6463779  0.58099066]]\n",
      "Z2 (pre-activation output): [[0.14335173 0.22280539]]\n",
      "A2 (final output): [[0.53219902 0.54992485]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (sigmoid scaled to [0.05, 0.95])\n",
    "def scaled_sigmoid(x):\n",
    "    return 0.9 / (1 + np.exp(-x)) + 0.05\n",
    "\n",
    "# Derivative of the scaled sigmoid\n",
    "def d_scaled_sigmoid(x):\n",
    "    sx = scaled_sigmoid(x)\n",
    "    return 0.9 * sx * (1 - sx) / 0.9\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(inputs, weights1, weights2):\n",
    "    z1 = np.dot(inputs, weights1)\n",
    "    a1 = scaled_sigmoid(z1)\n",
    "    z2 = np.dot(a1, weights2)\n",
    "    a2 = scaled_sigmoid(z2)\n",
    "    return {\n",
    "        'input': inputs,\n",
    "        'z1': z1,\n",
    "        'a1': a1,\n",
    "        'z2': z2,\n",
    "        'a2': a2\n",
    "    }\n",
    "\n",
    "# Backpropagation and weight update\n",
    "def backward_pass(cache, desired_output, weights1, weights2, learning_rate=0.5):\n",
    "    z1, a1, z2, a2, inputs = cache['z1'], cache['a1'], cache['z2'], cache['a2'], cache['input']\n",
    "    \n",
    "    # Output layer error\n",
    "    error = a2 - desired_output\n",
    "    delta2 = error * d_scaled_sigmoid(z2)\n",
    "    \n",
    "    # Hidden layer error\n",
    "    delta1 = np.dot(delta2, weights2.T) * d_scaled_sigmoid(z1)\n",
    "    \n",
    "    # Weight updates\n",
    "    grad_w2 = np.dot(a1.T, delta2)\n",
    "    grad_w1 = np.dot(inputs.T, delta1)\n",
    "    \n",
    "    # Update weights\n",
    "    weights2_new = weights2 - learning_rate * grad_w2\n",
    "    weights1_new = weights1 - learning_rate * grad_w1\n",
    "    \n",
    "    # Clip weights to keep values in range\n",
    "    weights1_new = np.clip(weights1_new, 0.05, 0.95)\n",
    "    weights2_new = np.clip(weights2_new, 0.05, 0.95)\n",
    "    \n",
    "    return weights1_new, weights2_new\n",
    "\n",
    "# Training loop with batching\n",
    "def train_network(X, Y, epochs=100, batch_size=10):\n",
    "    # Initialize weights randomly within [0.05, 0.95]\n",
    "    weights1 = np.random.uniform(0.05, 0.95, (2, 2))\n",
    "    weights2 = np.random.uniform(0.05, 0.95, (2, 2))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_w1 = []\n",
    "        batch_w2 = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            idx = np.random.randint(0, X.shape[0])\n",
    "            x_sample = X[idx].reshape(1, -1)\n",
    "            y_sample = Y[idx].reshape(1, -1)\n",
    "            \n",
    "            # Forward\n",
    "            cache = forward_pass(x_sample, weights1, weights2)\n",
    "            \n",
    "            # Backward\n",
    "            w1_new, w2_new = backward_pass(cache, y_sample, weights1, weights2)\n",
    "            batch_w1.append(w1_new)\n",
    "            batch_w2.append(w2_new)\n",
    "        \n",
    "        # Average the weights across batch\n",
    "        weights1 = np.mean(batch_w1, axis=0)\n",
    "        weights2 = np.mean(batch_w2, axis=0)\n",
    "        \n",
    "        # Output training progress\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"Weights Layer 1:\\n{weights1}\")\n",
    "        print(f\"Weights Layer 2:\\n{weights2}\\n\")\n",
    "        \n",
    "    return weights1, weights2\n",
    "\n",
    "# Example use\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Example dataset (X: inputs, Y: target outputs), scaled between 0.05 and 0.95\n",
    "    X = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
    "    Y = np.array([[0.95, 0.05], [0.05, 0.95]])\n",
    "\n",
    "    final_w1, final_w2 = train_network(X, Y, epochs=100, batch_size=20)\n",
    "\n",
    "    # Test on one sample\n",
    "    sample_input = np.array([[0.1, 0.9]])\n",
    "    activations = forward_pass(sample_input, final_w1, final_w2)\n",
    "\n",
    "    print(\"=== Final Test ===\")\n",
    "    print(f\"Input: {sample_input}\")\n",
    "    print(f\"Z1 (pre-activation hidden): {activations['z1']}\")\n",
    "    print(f\"A1 (post-activation hidden): {activations['a1']}\")\n",
    "    print(f\"Z2 (pre-activation output): {activations['z2']}\")\n",
    "    print(f\"A2 (final output): {activations['a2']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a059c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000 - Loss: 0.0788\n",
      "Epoch 200/1000 - Loss: 0.0293\n",
      "Epoch 300/1000 - Loss: 0.0145\n",
      "Epoch 400/1000 - Loss: 0.0124\n",
      "Epoch 500/1000 - Loss: 0.0115\n",
      "Epoch 600/1000 - Loss: 0.0108\n",
      "Epoch 700/1000 - Loss: 0.0104\n",
      "Epoch 800/1000 - Loss: 0.0101\n",
      "Epoch 900/1000 - Loss: 0.0099\n",
      "Epoch 1000/1000 - Loss: 0.0097\n",
      "Test Accuracy: 100.00%\n",
      "Saved SNN-formatted weights: NN_W_1.npy, NN_W_2.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========================\n",
    "# Activation functions\n",
    "# ========================\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: maps any real value into (0,1)\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid given output of sigmoid\"\"\"\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# ========================\n",
    "# Neural Network class\n",
    "# ========================\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1,\n",
    "                 W1_init=None, W2_init=None):\n",
    "        \"\"\"\n",
    "        If W1_init and W2_init are provided, they should be numpy arrays of shape:\n",
    "            W1_init: (input_size+1, hidden_size)  # includes bias row\n",
    "            W2_init: (hidden_size+1, output_size) # includes bias row\n",
    "        Otherwise randomly initialize weights in [0,1).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr  # learning rate\n",
    "\n",
    "        # Initialize or load weights\n",
    "        if W1_init is not None and W2_init is not None:\n",
    "            # Split combined weights into w and b\n",
    "            self.w1 = W1_init[:-1, :]  # shape (input_size, hidden_size)\n",
    "            self.b1 = W1_init[-1:, :]  # shape (1, hidden_size)\n",
    "            self.w2 = W2_init[:-1, :]  # shape (hidden_size, output_size)\n",
    "            self.b2 = W2_init[-1:, :]  # shape (1, output_size)\n",
    "        else:\n",
    "            # Random init in [0,1)\n",
    "            self.w1 = np.random.uniform(0.0, 1.0, (input_size, hidden_size))\n",
    "            self.b1 = np.random.uniform(0.0, 1.0, (1, hidden_size))\n",
    "            self.w2 = np.random.uniform(0.0, 1.0, (hidden_size, output_size))\n",
    "            self.b2 = np.random.uniform(0.0, 1.0, (1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backpropagation and weight updates\"\"\"\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "        hidden_error = output_delta.dot(self.w2.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.a1)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.w2 += self.a1.T.dot(output_delta) * self.lr\n",
    "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * self.lr\n",
    "        self.w1 += X.T.dot(hidden_delta) * self.lr\n",
    "        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * self.lr\n",
    "\n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"Train for a fixed number of epochs\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                loss = np.mean((y - output) ** 2)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class indices and probabilities\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1), probs\n",
    "\n",
    "    def export_snn_weights(self, prefix='NN_W_'):\n",
    "        \"\"\"\n",
    "        Combine weights and biases into SNN format and save to .npy files:\n",
    "          W1: shape (input_size+1, hidden_size)\n",
    "          W2: shape (hidden_size+1, output_size)\n",
    "        Saves as '{prefix}1.npy' and '{prefix}2.npy'.\n",
    "        Returns the combined arrays.\n",
    "        \"\"\"\n",
    "        W1 = np.vstack([self.w1, self.b1])\n",
    "        W2 = np.vstack([self.w2, self.b2])\n",
    "        np.save(f\"{prefix}1.npy\", W1)\n",
    "        np.save(f\"{prefix}2.npy\", W2)\n",
    "        print(f\"Saved SNN-formatted weights: {prefix}1.npy, {prefix}2.npy\")\n",
    "        return W1, W2\n",
    "\n",
    "    def print_weights(self):\n",
    "        \"\"\"Utility: print raw w and b arrays\"\"\"\n",
    "        print(\"=== w1 (input->hidden) ===\")\n",
    "        print(self.w1)\n",
    "        print(\"=== b1 ===\")\n",
    "        print(self.b1)\n",
    "        print(\"=== w2 (hidden->output) ===\")\n",
    "        print(self.w2)\n",
    "        print(\"=== b2 ===\")\n",
    "        print(self.b2)\n",
    "\n",
    "# ========================\n",
    "# Data preparation & example\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Iris dataset and scale features to [0.05,0.95]\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    y_encoded = np.zeros((y.size, y.max()+1))\n",
    "    y_encoded[np.arange(y.size), y] = 1\n",
    "    \n",
    "    def scale_features(x):\n",
    "        mn, mx = x.min(axis=0), x.max(axis=0)\n",
    "        x_norm = (x - mn) / (mx - mn)\n",
    "        return x_norm * 0.9 + 0.05\n",
    "\n",
    "    X_scaled = scale_features(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2,\n",
    "        random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Create, train, export\n",
    "    nn = NeuralNetwork(input_size=4, hidden_size=10, output_size=3, lr=0.1)\n",
    "    nn.train(X_train, y_train, epochs=1000)\n",
    "    acc = np.mean(nn.predict(X_test)[0] == np.argmax(y_test, axis=1))\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    # Export in SNN-compatible format\n",
    "    W1_snn, W2_snn = nn.export_snn_weights(prefix='NN_W_')\n",
    "\n",
    "# example of loading them here\n",
    "#     W1_0 = np.load(\"W1.npy\")\n",
    "#       W2_0 = np.load(\"W2.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb73ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n",
      "Saved SNN-formatted weights: NN_W_1.npy, NN_W_2.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- your synthetic inputs and targets ---\n",
    "x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "X = np.vstack([x0 if i % 2 == 0 else x1 for i in range(150)])  # shape (150,4)\n",
    "\n",
    "y0 = np.array([0.95, 0.0, 0.0])   # Class 0 target\n",
    "y1 = np.array([0.0, 0.0, 0.95])   # Class 2 target\n",
    "Y = np.vstack([y0 if i % 2 == 0 else y1 for i in range(150)])  # shape (150,3)\n",
    "\n",
    "# --- split into 80/20 train/test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=[0 if i%2==0 else 1 for i in range(150)]\n",
    ")\n",
    "\n",
    "# --- build & train your network ---\n",
    "nn = NeuralNetwork(input_size=4, hidden_size=10, output_size=3, lr=0.1)\n",
    "nn.train(X_train, y_train, epochs=30)\n",
    "\n",
    "# --- evaluate on the held‐out 20% ---\n",
    "y_pred, y_probs = nn.predict(X_test)\n",
    "\n",
    "# if you want an “accuracy”‐style measure, you need class indices:\n",
    "y_true_idx = np.argmax(y_test, axis=1)\n",
    "y_pred_idx = y_pred\n",
    "accuracy = np.mean(y_pred_idx == y_true_idx)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "W1_snn, W2_snn = nn.export_snn_weights(prefix='NN_W_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5ce33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.40257277 -0.25394539 -0.96149219 -0.89614586  2.58302045  0.13876311\n",
      "  -2.31420108  1.92268281  0.97686347 -0.27843057]\n",
      " [ 0.0325858  -0.19441178 -0.19954195 -0.23771653  0.34532378 -0.23813447\n",
      "  -0.78544654  0.31412567  0.4422104   0.23649642]\n",
      " [ 2.77117847  0.15841422  0.96708964  1.3150534  -2.69370418 -0.20829802\n",
      "   2.90278259 -1.37555797 -0.34827805  0.47227648]\n",
      " [ 1.9274688   0.11515945  1.01965938  0.97235911 -2.9576282  -0.31292552\n",
      "   2.47642143 -1.63834017 -0.82248908 -0.49885525]] [[-0.88895809 -0.82863464 -0.396815   -0.4715086   1.21101224 -0.17061962\n",
      "  -0.8219439   0.07770242  0.34755369 -0.44260157]] [[-2.80590062 -0.76734158  2.40506059]\n",
      " [-0.26756683 -0.36640872  0.07490121]\n",
      " [-1.05946893 -0.89097957  0.94773775]\n",
      " [-1.2111273  -0.73491921  1.02770323]\n",
      " [ 3.29446964 -0.57184223 -3.45912812]\n",
      " [-0.01217772 -0.71448122 -0.6308465 ]\n",
      " [-2.98062316 -0.92565131  2.88581609]\n",
      " [ 1.84750426 -0.55110052 -2.04794773]\n",
      " [ 0.68793264 -1.57224144 -1.17886332]\n",
      " [-0.09207141 -0.8643283  -0.04376193]] [[ 0.17223993 -1.85447367 -1.0169616 ]]\n"
     ]
    }
   ],
   "source": [
    "np.save(\"w1.npy\", nn.w1)\n",
    "np.save(\"b1.npy\", nn.b1)\n",
    "np.save(\"w2.npy\", nn.w2)\n",
    "np.save(\"b2.npy\", nn.b2)\n",
    "\n",
    "print(nn.w1, nn.b1, nn.w2, nn.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd505fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000 - Loss: 0.0876\n",
      "Epoch 200/1000 - Loss: 0.0411\n",
      "Epoch 300/1000 - Loss: 0.0147\n",
      "Epoch 400/1000 - Loss: 0.0124\n",
      "Epoch 500/1000 - Loss: 0.0113\n",
      "Epoch 600/1000 - Loss: 0.0107\n",
      "Epoch 700/1000 - Loss: 0.0103\n",
      "Epoch 800/1000 - Loss: 0.0100\n",
      "Epoch 900/1000 - Loss: 0.0098\n",
      "Epoch 1000/1000 - Loss: 0.0096\n",
      "Test Accuracy: 100.00%\n",
      "Saved SNN-formatted weights: NN_W_1.npy, NN_W_2.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========================\n",
    "# Activation functions\n",
    "# ========================\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: maps any real value into (0,1)\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid given output of sigmoid\"\"\"\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# ========================\n",
    "# Neural Network class\n",
    "# ========================\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.1,\n",
    "                 W1_init=None, W2_init=None):\n",
    "        \"\"\"\n",
    "        If W1_init and W2_init are provided, they should be numpy arrays of shape:\n",
    "            W1_init: (input_size+1, hidden_size)  # includes bias row\n",
    "            W2_init: (hidden_size+1, output_size) # includes bias row\n",
    "        Otherwise randomly initialize weights in [0,1).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr  # learning rate\n",
    "\n",
    "        # Initialize or load weights\n",
    "        if W1_init is not None and W2_init is not None:\n",
    "            # Split combined weights into w and b\n",
    "            self.w1 = W1_init[:-1, :]  # shape (input_size, hidden_size)\n",
    "            self.b1 = W1_init[-1:, :]  # shape (1, hidden_size)\n",
    "            self.w2 = W2_init[:-1, :]  # shape (hidden_size, output_size)\n",
    "            self.b2 = W2_init[-1:, :]  # shape (1, output_size)\n",
    "        else:\n",
    "            # Random init in [0,1)\n",
    "            self.w1 = np.random.uniform(0.0, 1.0, (input_size, hidden_size))\n",
    "            self.b1 = np.random.uniform(0.0, 1.0, (1, hidden_size))\n",
    "            self.w2 = np.random.uniform(0.0, 1.0, (hidden_size, output_size))\n",
    "            self.b2 = np.random.uniform(0.0, 1.0, (1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backpropagation and weight updates\"\"\"\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "        hidden_error = output_delta.dot(self.w2.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.a1)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.w2 += self.a1.T.dot(output_delta) * self.lr\n",
    "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * self.lr\n",
    "        self.w1 += X.T.dot(hidden_delta) * self.lr\n",
    "        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * self.lr\n",
    "\n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"Train for a fixed number of epochs\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                loss = np.mean((y - output) ** 2)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class indices and probabilities\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1), probs\n",
    "\n",
    "    def export_snn_weights(self, prefix='NN_W_'):\n",
    "        \"\"\"\n",
    "        Combine weights and biases into SNN format and save to .npy files:\n",
    "          W1: shape (input_size+1, hidden_size)\n",
    "          W2: shape (hidden_size+1, output_size)\n",
    "        Saves as '{prefix}1.npy' and '{prefix}2.npy'.\n",
    "        Returns the combined arrays.\n",
    "        \"\"\"\n",
    "        W1 = np.vstack([self.w1, self.b1])\n",
    "        W2 = np.vstack([self.w2, self.b2])\n",
    "        np.save(f\"{prefix}1.npy\", W1)\n",
    "        np.save(f\"{prefix}2.npy\", W2)\n",
    "        print(f\"Saved SNN-formatted weights: {prefix}1.npy, {prefix}2.npy\")\n",
    "        return W1, W2\n",
    "\n",
    "    def print_weights(self):\n",
    "        \"\"\"Utility: print raw w and b arrays\"\"\"\n",
    "        print(\"=== w1 (input->hidden) ===\")\n",
    "        print(self.w1)\n",
    "        print(\"=== b1 ===\")\n",
    "        print(self.b1)\n",
    "        print(\"=== w2 (hidden->output) ===\")\n",
    "        print(self.w2)\n",
    "        print(\"=== b2 ===\")\n",
    "        print(self.b2)\n",
    "\n",
    "# ========================\n",
    "# Data preparation & example\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Iris dataset and scale features to [0.05,0.95]\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    y_encoded = np.zeros((y.size, y.max()+1))\n",
    "    y_encoded[np.arange(y.size), y] = 1\n",
    "    \n",
    "    def scale_features(x):\n",
    "        mn, mx = x.min(axis=0), x.max(axis=0)\n",
    "        x_norm = (x - mn) / (mx - mn)\n",
    "        return x_norm * 0.9 + 0.05\n",
    "\n",
    "    X_scaled = scale_features(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2,\n",
    "        random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Create, train, export\n",
    "    nn = NeuralNetwork(input_size=4, hidden_size=10, output_size=3, lr=0.1)\n",
    "    nn.train(X_train, y_train, epochs=1000)\n",
    "    acc = np.mean(nn.predict(X_test)[0] == np.argmax(y_test, axis=1))\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    # Export in SNN-compatible format\n",
    "    W1_snn, W2_snn = nn.export_snn_weights(prefix='NN_W_')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
