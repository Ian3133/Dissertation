{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fee676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(5, 10)\n",
      "(3, 2)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 270\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(W2_0\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "Cell \u001b[1;32mIn[10], line 149\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n\u001b[0;32m    145\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y): \u001b[38;5;66;03m# iterates in pairs at same time x1 and y1 x2 & y2 ect.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# — Forward pass —\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m layer_forward(h_times, W2, layer2_idx)  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m aug_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((inputs, [bias_time]))  \u001b[38;5;66;03m# shape (n_in+1,)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m n_in_plus_bias, n_out \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m aug_inputs\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m n_in_plus_bias\n\u001b[0;32m     62\u001b[0m out_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_out):\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    return np.tanh(w * x)\n",
    "\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(G.scheduled_time[0] / ms)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # can come back to above to check ^^^^\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            # print(\"otimes \", o_times)\n",
    "            # print(\"delta_o \",delta_o) \n",
    "        \n",
    "            # otimes  [2.666 2.003 2.524]\n",
    "            # delta_o  [-0.284  -0.0235  0.237 ]\n",
    "\n",
    "            # otimes  [2.726 2.003 2.619]\n",
    "            # delta_o  [ 0.338  -0.0235 -0.331 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0])) # just adds a 0 to end of list\n",
    "            # print(\"h_times \", h_times)\n",
    "            # print(\"aug_h \", aug_h)\n",
    "\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):        # will need to check out spinking time_dw\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "            # print(\"w2 \", W2)\n",
    "            # print(\"dW2 \", dW2)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  \n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        #lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = W1 - lr * vW1  \n",
    "        W2 = W2 - lr * vW2\n",
    "\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"NN_W_1.npy\")\n",
    "    print(W1_0.shape)\n",
    "\n",
    "    test  = np.load(\"W1.npy\")\n",
    "    print(test.shape)\n",
    "\n",
    "    W2_0 = np.load(\"NN_W_2.npy\")\n",
    "    print(W2_0.shape)\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=10, lr=0.1)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for xi, yi in zip(X, Y):\n",
    "        # call layer_forward(positionally) rather than with layer1_idx=\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  \n",
    "        true_class = np.argmax(yi)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db93b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gradient Debug ===\n",
      "Hidden times: [1.45820835 1.58188232 1.53247909 1.42944979 1.4640665  1.49132949\n",
      " 1.42066056 1.4655997  1.35691895 1.37716041]\n",
      "Output times: [2.51084009 2.39558591 2.40642763]\n",
      "Output variance: 0.002700\n",
      "WARNING: Outputs are too similar - poor class separation\n",
      "W1 range: [-0.935, 0.742]\n",
      "W2 range: [-0.810, 0.702]\n",
      "Epoch 1/2 — avg loss=0.0207\n",
      "  LR=0.1900, ‖∇W1‖=1.0908, ‖∇W2‖=1.7358\n",
      "  ‖W1‖=2.543, ‖W2‖=2.117\n",
      "\n",
      "Epoch 2/2 — avg loss=0.0213\n",
      "  LR=0.1900, ‖∇W1‖=1.0809, ‖∇W2‖=1.7324\n",
      "  ‖W1‖=2.532, ‖W2‖=2.096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Key fixes to your SNN code:\n",
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "# 1. Fix the derivative function to handle edge cases better\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-6  # Increased epsilon\n",
    "    \n",
    "    # Add bounds checking to prevent extreme values\n",
    "    x = np.clip(x, eps, 1-eps)\n",
    "    \n",
    "    if w >= 0:\n",
    "        # For positive weights, add safeguards against log(0)\n",
    "        log_term = np.log(np.maximum(x, eps))\n",
    "        result = - np.power(x, (1 - w)) * log_term\n",
    "    else:\n",
    "        # For negative weights\n",
    "        log_term = np.log(np.maximum(1 - x, eps))\n",
    "        result = - np.power((1 - x), (1 + w)) * log_term\n",
    "    \n",
    "    # Clip gradients to prevent explosion\n",
    "    return np.clip(result, -10.0, 10.0)\n",
    "\n",
    "# 2. Improve the spike timing function for better gradients\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-6\n",
    "    x = np.clip(x, eps, 1-eps)  # Prevent saturation\n",
    "    \n",
    "    if w >= 0:\n",
    "        result = np.power(x, (1 - w))\n",
    "    else:\n",
    "        result = 1 - np.power((1 - x), (1 + w))\n",
    "    \n",
    "    return np.clip(result, eps, 1-eps)  # Prevent complete saturation\n",
    "\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(G.scheduled_time[0] / ms)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# 3. Modified training function with better hyperparameters\n",
    "def train_snn_backprop_fixed(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=50, lr=0.05,  # Reduced learning rate\n",
    "    max_grad=5.0,        # Reduced gradient clipping\n",
    "    w_min=-10.0, w_max=10.0,  # Reduced weight bounds\n",
    "    non_target_time=2.5,  # Increased separation\n",
    "    λ=1.0                # Increased penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Fixed version with better gradient handling and hyperparameters\n",
    "    \"\"\"\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "\n",
    "    # Adaptive learning rate\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "    \n",
    "    # Add learning rate decay\n",
    "    lr_decay = 0.95\n",
    "    current_lr = lr\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            # Forward pass\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            # Improved loss function with better class separation\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            \n",
    "            # Encourage non-target outputs to be far from target\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([\n",
    "                max(0, 0.2 - abs(o_times[j] - o_times[target_idx]))**2 \n",
    "                for j in non_ids\n",
    "            ])  # Margin loss for better separation\n",
    "            \n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # Gradients for W2\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            \n",
    "            for j in non_ids:\n",
    "                if abs(o_times[j] - o_times[target_idx]) < 0.2:\n",
    "                    delta_o[j] = λ * np.sign(o_times[j] - o_times[target_idx])\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            \n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    grad = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    dW2[k, j] = delta_o[j] * grad\n",
    "\n",
    "            # Backprop into hidden layer\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    grad = d_spike_timing_dw(W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "                    dW1[i, k] = delta_h[k] * grad\n",
    "\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # Average gradients\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Check for vanishing gradients\n",
    "        if np.mean(np.abs(acc_dW1)) < 1e-8 or np.mean(np.abs(acc_dW2)) < 1e-8:\n",
    "            print(f\"Warning: Vanishing gradients detected at epoch {ep+1}\")\n",
    "            current_lr *= 2  # Increase learning rate temporarily\n",
    "\n",
    "        # Gradient clipping\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "\n",
    "        # Momentum updates\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # Weight updates with bounds\n",
    "        W1 = np.clip(W1 - current_lr * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - current_lr * vW2, w_min, w_max)\n",
    "\n",
    "        # Learning rate decay\n",
    "        if ep % 10 == 0:\n",
    "            current_lr *= lr_decay\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"  LR={current_lr:.4f}, ‖∇W1‖={np.linalg.norm(acc_dW1):.4f}, ‖∇W2‖={np.linalg.norm(acc_dW2):.4f}\")\n",
    "        print(f\"  ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# 4. Better weight initialization\n",
    "def initialize_weights_better():\n",
    "    \"\"\"Initialize weights with better variance scaling\"\"\"\n",
    "    # Xavier/Glorot initialization scaled for this problem\n",
    "    W1_0 = np.random.randn(4+1, 10) * np.sqrt(2.0 / (4+10))\n",
    "    W2_0 = np.random.randn(10+1, 3) * np.sqrt(2.0 / (10+3))\n",
    "    return W1_0, W2_0\n",
    "\n",
    "# 5. Add debugging function\n",
    "def debug_gradients(W1, W2, X, Y):\n",
    "    \"\"\"Debug gradient flow\"\"\"\n",
    "    print(\"=== Gradient Debug ===\")\n",
    "    xi, yi = X[0], Y[0]\n",
    "    \n",
    "    h_times = layer_forward(xi, W1, 1)\n",
    "    o_times = layer_forward(h_times, W2, 2)\n",
    "    \n",
    "    print(f\"Hidden times: {h_times}\")\n",
    "    print(f\"Output times: {o_times}\")\n",
    "    \n",
    "    # Check if outputs are too similar\n",
    "    output_variance = np.var(o_times)\n",
    "    print(f\"Output variance: {output_variance:.6f}\")\n",
    "    \n",
    "    if output_variance < 0.01:\n",
    "        print(\"WARNING: Outputs are too similar - poor class separation\")\n",
    "    \n",
    "    # Check weight ranges\n",
    "    print(f\"W1 range: [{W1.min():.3f}, {W1.max():.3f}]\")\n",
    "    print(f\"W2 range: [{W2.min():.3f}, {W2.max():.3f}]\")\n",
    "\n",
    "# Usage with better initialization:\n",
    "if __name__ == \"__main__\":\n",
    "    # Your existing data\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    y0 = np.array([1.5, 2.0, 2.5])  # Changed targets for better separation\n",
    "    y1 = np.array([2.5, 2.0, 1.5])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Use better initialization instead of loaded weights\n",
    "    W1_0, W2_0 = initialize_weights_better()\n",
    "    \n",
    "    # Debug before training\n",
    "    debug_gradients(W1_0, W2_0, X, Y)\n",
    "    \n",
    "    # Train with fixed function\n",
    "    W1_tr, W2_tr = train_snn_backprop_fixed(X, Y, W1_0, W2_0, epochs=2, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cfb09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spike Timing Function Analysis ===\n",
      "Weight\tTime\tSpike_timing\tDerivative\n",
      "-2.0\t0.1\t0.0000\t\t0.1171\n",
      "-2.0\t0.3\t0.0000\t\t0.5095\n",
      "-2.0\t0.5\t0.0000\t\t1.3863\n",
      "-2.0\t0.7\t0.0000\t\t4.0132\n",
      "-2.0\t0.9\t0.0000\t\t10.0000\n",
      "-1.0\t0.1\t0.0000\t\t0.1054\n",
      "-1.0\t0.3\t0.0000\t\t0.3567\n",
      "-1.0\t0.5\t0.0000\t\t0.6931\n",
      "-1.0\t0.7\t0.0000\t\t1.2040\n",
      "-1.0\t0.9\t0.0000\t\t2.3026\n",
      "-0.5\t0.1\t0.0513\t\t0.1000\n",
      "-0.5\t0.3\t0.1633\t\t0.2984\n",
      "-0.5\t0.5\t0.2929\t\t0.4901\n",
      "-0.5\t0.7\t0.4523\t\t0.6594\n",
      "-0.5\t0.9\t0.6838\t\t0.7281\n",
      "0.0\t0.1\t0.1000\t\t0.2303\n",
      "0.0\t0.3\t0.3000\t\t0.3612\n",
      "0.0\t0.5\t0.5000\t\t0.3466\n",
      "0.0\t0.7\t0.7000\t\t0.2497\n",
      "0.0\t0.9\t0.9000\t\t0.0948\n",
      "0.5\t0.1\t0.3162\t\t0.7281\n",
      "0.5\t0.3\t0.5477\t\t0.6594\n",
      "0.5\t0.5\t0.7071\t\t0.4901\n",
      "0.5\t0.7\t0.8367\t\t0.2984\n",
      "0.5\t0.9\t0.9487\t\t0.1000\n",
      "1.0\t0.1\t1.0000\t\t2.3026\n",
      "1.0\t0.3\t1.0000\t\t1.2040\n",
      "1.0\t0.5\t1.0000\t\t0.6931\n",
      "1.0\t0.7\t1.0000\t\t0.3567\n",
      "1.0\t0.9\t1.0000\t\t0.1054\n",
      "2.0\t0.1\t1.0000\t\t10.0000\n",
      "2.0\t0.3\t1.0000\t\t4.0132\n",
      "2.0\t0.5\t1.0000\t\t1.3863\n",
      "2.0\t0.7\t1.0000\t\t0.5095\n",
      "2.0\t0.9\t1.0000\t\t0.1171\n",
      "=== Debug Timing Function Inputs ===\n",
      "Hidden layer spike times: [1.43008574 1.43948677 1.51182876 1.46719479 1.43742272 1.47630132\n",
      " 1.42447831 1.42702498 1.43493524 1.51686609]\n",
      "Hidden time range: [1.424, 1.517]\n",
      "Augmented hidden times: [1.43008574 1.43948677 1.51182876 1.46719479 1.43742272 1.47630132\n",
      " 1.42447831 1.42702498 1.43493524 1.51686609 0.        ]\n",
      "For layer 2, global_clock % 1 values will be: [0.43008574 0.43948677 0.51182876 0.46719479 0.43742272 0.47630132\n",
      " 0.42447831 0.42702498 0.43493524 0.51686609 0.        ]\n",
      "=== Weight Update Analysis ===\n",
      "\n",
      "Sample 0: input=[0.9 0.7 0.3 0.4], target=[2.95 2.   2.  ]\n",
      "  Hidden times: [1.43008574 1.43948677 1.51182876 1.46719479 1.43742272 1.47630132\n",
      " 1.42447831 1.42702498 1.43493524 1.51686609]\n",
      "  Output times: [2.68163337 2.0310563  2.54159021]\n",
      "  Hidden variance: 0.001092\n",
      "  Output variance: 0.078167\n",
      "\n",
      "Sample 1: input=[0.6 0.7 0.8 0.9], target=[2.   2.   2.95]\n",
      "  Hidden times: [1.55603581 1.57524016 1.60839374 1.57913611 1.56056221 1.60274556\n",
      " 1.54266361 1.56410982 1.56720231 1.61015288]\n",
      "  Output times: [2.73485722 2.04321863 2.63214298]\n",
      "  Hidden variance: 0.000490\n",
      "  Output variance: 0.092861\n",
      "  WARNING: Hidden layer outputs are too similar!\n"
     ]
    }
   ],
   "source": [
    "# Analysis of the real issues in your SNN code while preserving layer timing\n",
    "\n",
    "# 1. The derivative function has numerical issues\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw_fixed(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-7  # Small epsilon to prevent log(0)\n",
    "    \n",
    "    # The issue: x can be very close to 0 or 1, making log(x) explode\n",
    "    # Your logic is correct, but needs numerical stability\n",
    "    \n",
    "    if w >= 0:\n",
    "        # Prevent log(0) by ensuring x > eps\n",
    "        x_safe = np.maximum(x, eps)\n",
    "        result = - np.power(x_safe, (1 - w)) * np.log(x_safe)\n",
    "    else:\n",
    "        # Prevent log(0) by ensuring (1-x) > eps  \n",
    "        x_safe = np.minimum(x, 1 - eps)\n",
    "        result = - np.power((1 - x_safe), (1 + w)) * np.log(1 - x_safe)\n",
    "    \n",
    "    # Clip extreme gradients but preserve sign and magnitude relationships\n",
    "    return np.clip(result, -50.0, 50.0)\n",
    "\n",
    "# 2. The timing function also needs numerical stability\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing_fixed(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-7\n",
    "    \n",
    "    if w >= 0:\n",
    "        # Prevent x=0 case\n",
    "        x_safe = np.maximum(x, eps)\n",
    "        result = np.power(x_safe, (1 - w))\n",
    "    else:\n",
    "        # Prevent x=1 case\n",
    "        x_safe = np.minimum(x, 1 - eps)\n",
    "        result = 1 - np.power((1 - x_safe), (1 + w))\n",
    "    \n",
    "    return np.clip(result, 0.0, 1.0)\n",
    "\n",
    "# 3. The main issue: Check what values are being passed to these functions\n",
    "def debug_timing_inputs(W1, W2, X):\n",
    "    \"\"\"Debug what actual values are being passed to timing functions\"\"\"\n",
    "    print(\"=== Debug Timing Function Inputs ===\")\n",
    "    \n",
    "    xi = X[0]  # First input\n",
    "    \n",
    "    # Forward pass through layer 1\n",
    "    h_times = layer_forward(xi, W1, 1)\n",
    "    print(f\"Hidden layer spike times: {h_times}\")\n",
    "    print(f\"Hidden time range: [{h_times.min():.3f}, {h_times.max():.3f}]\")\n",
    "    \n",
    "    # What gets passed to layer 2?\n",
    "    aug_h = np.concatenate((h_times, [0.0]))  # bias at t=0\n",
    "    print(f\"Augmented hidden times: {aug_h}\")\n",
    "    \n",
    "    # Check what global_clock values are used\n",
    "    print(f\"For layer 2, global_clock % 1 values will be: {aug_h % 1}\")\n",
    "    \n",
    "    # These are the values that go into your timing functions!\n",
    "    # If they're all very close to 0 or 1, gradients will vanish\n",
    "    \n",
    "    return h_times, aug_h\n",
    "\n",
    "# 4. The real issue might be in your layer_forward function\n",
    "def layer_forward_with_debug(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    Debug version of layer_forward to see what's happening\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Layer {layer_idx} Forward Pass ---\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    \n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))\n",
    "    print(f\"Augmented inputs: {aug_inputs}\")\n",
    "    \n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    out_times = []\n",
    "    \n",
    "    for j in range(n_out):\n",
    "        # Your simulation setup\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "        \n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "        \n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "        \n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "        \n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "        \n",
    "        # The issue might be here - what values is global_clock taking?\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "        \n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "        \n",
    "        # Debug the actual computation\n",
    "        final_sum = G.sum[0]\n",
    "        final_sr = G.sr[0]\n",
    "        scheduled_t = float(G.scheduled_time[0] / ms)\n",
    "        \n",
    "        print(f\"  Neuron {j}: sum={final_sum:.4f}, sr={final_sr}, scheduled_time={scheduled_t:.4f}\")\n",
    "        \n",
    "        # The real issue might be that sum/sr is always similar values\n",
    "        # This would make all outputs similar regardless of weights\n",
    "        \n",
    "        out_times.append(scheduled_t)\n",
    "    \n",
    "    print(f\"Layer {layer_idx} outputs: {out_times}\")\n",
    "    return np.array(out_times)\n",
    "\n",
    "# 5. Check if the issue is in the weight update order\n",
    "def analyze_weight_updates(X, Y, W1, W2):\n",
    "    \"\"\"\n",
    "    Check if weight updates are consistent across samples\n",
    "    \"\"\"\n",
    "    print(\"=== Weight Update Analysis ===\")\n",
    "    \n",
    "    # Check gradients for first two samples\n",
    "    for i, (xi, yi) in enumerate(zip(X[:2], Y[:2])):\n",
    "        print(f\"\\nSample {i}: input={xi}, target={yi}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        h_times = layer_forward(xi, W1, 1)\n",
    "        o_times = layer_forward(h_times, W2, 2)\n",
    "        \n",
    "        print(f\"  Hidden times: {h_times}\")\n",
    "        print(f\"  Output times: {o_times}\")\n",
    "        \n",
    "        # Check if hidden times are too similar\n",
    "        h_variance = np.var(h_times)\n",
    "        o_variance = np.var(o_times)\n",
    "        \n",
    "        print(f\"  Hidden variance: {h_variance:.6f}\")\n",
    "        print(f\"  Output variance: {o_variance:.6f}\")\n",
    "        \n",
    "        if h_variance < 0.001:\n",
    "            print(\"  WARNING: Hidden layer outputs are too similar!\")\n",
    "        if o_variance < 0.001:\n",
    "            print(\"  WARNING: Output layer outputs are too similar!\")\n",
    "\n",
    "# 6. The corrected training function (keeping your timing structure)\n",
    "def train_snn_backprop_corrected(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=50, lr=0.05,  # Reduced learning rate\n",
    "    max_grad=10.0,\n",
    "    w_min=-10.0, w_max=10.0,\n",
    "    non_target_time=2.05,  # Keep your original range\n",
    "    λ=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Corrected version that preserves your layer timing structure\n",
    "    \"\"\"\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    \n",
    "    # Add momentum\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "    \n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for xi, yi in zip(X, Y):\n",
    "            # Forward pass (unchanged)\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "            \n",
    "            # Loss calculation (unchanged - your structure is correct)\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "            \n",
    "            # Gradient computation (your logic is correct, just need numerical stability)\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "            \n",
    "            # Rest of gradient computation is correct...\n",
    "            # The issue is in the timing functions, not the structure\n",
    "            \n",
    "            # [Include your original gradient computation here with fixed timing functions]\n",
    "            \n",
    "        # Check for vanishing gradients\n",
    "        grad_norm_1 = np.linalg.norm(acc_dW1)\n",
    "        grad_norm_2 = np.linalg.norm(acc_dW2)\n",
    "        \n",
    "        if grad_norm_1 < 1e-10 or grad_norm_2 < 1e-10:\n",
    "            print(f\"  WARNING: Very small gradients - norm1={grad_norm_1:.2e}, norm2={grad_norm_2:.2e}\")\n",
    "        \n",
    "        # [Rest of your training loop...]\n",
    "        \n",
    "        print(f\"Epoch {ep+1}: loss={epoch_loss/N:.4f}, grad_norms=({grad_norm_1:.4f}, {grad_norm_2:.4f})\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# 7. Key insight: Your problem might be here\n",
    "def check_spike_timing_behavior():\n",
    "    \"\"\"\n",
    "    Check if your spike_timing function is producing reasonable gradients\n",
    "    \"\"\"\n",
    "    print(\"=== Spike Timing Function Analysis ===\")\n",
    "    \n",
    "    # Test with typical values that would occur in your network\n",
    "    test_weights = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0])\n",
    "    test_times = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # global_clock % 1 values\n",
    "    \n",
    "    print(\"Weight\\tTime\\tSpike_timing\\tDerivative\")\n",
    "    for w in test_weights:\n",
    "        for t in test_times:\n",
    "            st = spike_timing(w, t, 1, 0, 1)\n",
    "            dt = d_spike_timing_dw(w, t, 1, 0, 1)\n",
    "            print(f\"{w:.1f}\\t{t:.1f}\\t{st:.4f}\\t\\t{dt:.4f}\")\n",
    "    \n",
    "    # Look for patterns where derivatives are always very small\n",
    "\n",
    "# Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Your original data (this is correct)\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    # Your original targets (this is correct for your architecture)\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Load your existing weights\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "    \n",
    "    # Run diagnostics\n",
    "    check_spike_timing_behavior()\n",
    "    debug_timing_inputs(W1_0, W2_0, X)\n",
    "    analyze_weight_updates(X, Y, W1_0, W2_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hidden Layer Collapse Analysis ===\n",
      "Input times: [0.9 0.7 0.3 0.4 0. ]\n",
      "W1 first few columns:\n",
      "[[-2.24802890e-01 -5.02760943e-05 -3.02786849e-02]\n",
      " [-1.11390344e-01 -1.80939746e-01 -1.34243925e-01]\n",
      " [-2.41631112e-01 -1.56714608e-01  3.69612932e-01]\n",
      " [ 5.13877625e-02  1.55790838e-02  3.39575082e-01]\n",
      " [-1.31178265e-01  6.46637541e-03 -9.75420633e-02]]\n",
      "\n",
      "Hidden neuron analysis:\n",
      "\n",
      "Hidden neuron 0:\n",
      "  Weights: [-0.22480289 -0.11139034 -0.24163111  0.05138776 -0.13117826]\n",
      "    Input 0: time=0.900, w=-0.225, spike_timing=0.8322\n",
      "    Input 1: time=0.700, w=-0.111, spike_timing=0.6569\n",
      "    Input 2: time=0.300, w=-0.242, spike_timing=0.2370\n",
      "    Input 3: time=0.400, w=0.051, spike_timing=0.4193\n",
      "    Input 4: time=0.000, w=-0.131, spike_timing=0.0000\n",
      "  Total sum: 2.1454, sum/sr: 0.4291\n",
      "  Scheduled time: 1.4291\n",
      "\n",
      "Hidden neuron 1:\n",
      "  Weights: [-5.02760943e-05 -1.80939746e-01 -1.56714608e-01  1.55790838e-02\n",
      "  6.46637541e-03]\n",
      "    Input 0: time=0.900, w=-0.000, spike_timing=0.9000\n",
      "    Input 1: time=0.700, w=-0.181, spike_timing=0.6270\n",
      "    Input 2: time=0.300, w=-0.157, spike_timing=0.2598\n",
      "    Input 3: time=0.400, w=0.016, spike_timing=0.4058\n",
      "    Input 4: time=0.000, w=0.006, spike_timing=0.0000\n",
      "  Total sum: 2.1925, sum/sr: 0.4385\n",
      "  Scheduled time: 1.4385\n",
      "\n",
      "Hidden neuron 2:\n",
      "  Weights: [-0.03027868 -0.13424393  0.36961293  0.33957508 -0.09754206]\n",
      "    Input 0: time=0.900, w=-0.030, spike_timing=0.8928\n",
      "    Input 1: time=0.700, w=-0.134, spike_timing=0.6474\n",
      "    Input 2: time=0.300, w=0.370, spike_timing=0.4681\n",
      "    Input 3: time=0.400, w=0.340, spike_timing=0.5460\n",
      "    Input 4: time=0.000, w=-0.098, spike_timing=0.0000\n",
      "  Total sum: 2.5543, sum/sr: 0.5109\n",
      "  Scheduled time: 1.5109\n",
      "=== Solution: Diverse Initialization + Diversity Loss ===\n",
      "Initial hidden diversity:\n",
      "  Sample 0 variance: 0.034311\n",
      "  Sample 1 variance: 0.039116\n",
      "  Sample 0 range: [1.116, 1.726]\n",
      "  Sample 1 range: [1.109, 1.795]\n",
      "✓ Good initial diversity!\n",
      "Epoch 1: loss=0.1997\n",
      "  Hidden variance: 0.034343 (sample 0), 0.039143 (sample 1)\n",
      "  Hidden range: [1.116, 1.727]\n",
      "Epoch 6: loss=0.1981\n",
      "  Hidden variance: 0.034917 (sample 0), 0.039640 (sample 1)\n",
      "  Hidden range: [1.115, 1.736]\n",
      "Epoch 11: loss=0.1947\n",
      "  Hidden variance: 0.036218 (sample 0), 0.040795 (sample 1)\n",
      "  Hidden range: [1.112, 1.756]\n",
      "Epoch 16: loss=0.1905\n",
      "  Hidden variance: 0.038345 (sample 0), 0.042712 (sample 1)\n",
      "  Hidden range: [1.108, 1.787]\n",
      "Epoch 21: loss=0.1861\n",
      "  Hidden variance: 0.040601 (sample 0), 0.044736 (sample 1)\n",
      "  Hidden range: [1.104, 1.816]\n",
      "Epoch 26: loss=0.1820\n",
      "  Hidden variance: 0.040554 (sample 0), 0.044381 (sample 1)\n",
      "  Hidden range: [1.099, 1.815]\n",
      "\n",
      "=== Final Test ===\n",
      "Sample 0: pred=0, true=0\n",
      "  Hidden variance: 0.038600\n",
      "  Output times: [2.47545645 2.33920008 2.46592541]\n",
      "Sample 1: pred=0, true=2\n",
      "  Hidden variance: 0.041874\n",
      "  Output times: [2.5425644  2.41112667 2.53502445]\n"
     ]
    }
   ],
   "source": [
    "# Analysis of Hidden Layer Collapse in Your SNN\n",
    "\n",
    "# The Problem: Why all hidden neurons spike at similar times\n",
    "\"\"\"\n",
    "In your layer_forward function:\n",
    "\n",
    "scheduled_time = (sum/sr + layer)*ms\n",
    "\n",
    "For layer 1:\n",
    "- sum/sr is computed from: sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "- If all weights are small and similar, sum/sr will be similar for all neurons\n",
    "- Result: scheduled_time ≈ (similar_value + 1)*ms for all hidden neurons\n",
    "\n",
    "This creates a \"hidden layer collapse\" where all neurons do the same thing.\n",
    "\"\"\"\n",
    "\n",
    "# Let's trace through what happens:\n",
    "def analyze_hidden_collapse():\n",
    "    \"\"\"\n",
    "    Analyze why all hidden neurons produce similar spike times\n",
    "    \"\"\"\n",
    "    print(\"=== Hidden Layer Collapse Analysis ===\")\n",
    "    \n",
    "    # Your input and typical weights\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    bias = 0.0\n",
    "    aug_x0 = np.concatenate([x0, [bias]])\n",
    "    \n",
    "    print(f\"Input times: {aug_x0}\")\n",
    "    \n",
    "    # Let's see what happens with your actual weights\n",
    "    W1 = np.load(\"W1.npy\")\n",
    "    print(f\"W1 first few columns:\\n{W1[:, :3]}\")\n",
    "    \n",
    "    # For each hidden neuron, calculate what sum/sr would be\n",
    "    print(\"\\nHidden neuron analysis:\")\n",
    "    for j in range(3):  # First 3 hidden neurons\n",
    "        weights = W1[:, j]\n",
    "        print(f\"\\nHidden neuron {j}:\")\n",
    "        print(f\"  Weights: {weights}\")\n",
    "        \n",
    "        # Simulate the sum computation\n",
    "        total_sum = 0\n",
    "        for i, (input_time, weight) in enumerate(zip(aug_x0, weights)):\n",
    "            # What spike_timing returns for this input\n",
    "            st_value = spike_timing(weight, input_time, 1, 0, 1)\n",
    "            print(f\"    Input {i}: time={input_time:.3f}, w={weight:.3f}, spike_timing={st_value:.4f}\")\n",
    "            total_sum += st_value\n",
    "        \n",
    "        # Final scheduled time\n",
    "        sr = len(weights)  # Number of synapses\n",
    "        scheduled_time = (total_sum/sr + 1)  # +1 for layer\n",
    "        print(f\"  Total sum: {total_sum:.4f}, sum/sr: {total_sum/sr:.4f}\")\n",
    "        print(f\"  Scheduled time: {scheduled_time:.4f}\")\n",
    "\n",
    "# Solutions to fix hidden layer collapse:\n",
    "\n",
    "# Solution 1: Better weight initialization with more diversity\n",
    "def initialize_diverse_weights():\n",
    "    \"\"\"\n",
    "    Initialize weights to encourage diversity in hidden layer\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Create weights with intentional diversity\n",
    "    W1 = np.random.randn(5, 10) * 0.5  # Larger initial variance\n",
    "    \n",
    "    # Add some structure to encourage different behaviors\n",
    "    # Some neurons prefer early spikes, others late spikes\n",
    "    for j in range(10):\n",
    "        if j < 3:  # Early spike neurons\n",
    "            W1[:, j] += 0.5  # Positive bias\n",
    "        elif j >= 7:  # Late spike neurons  \n",
    "            W1[:, j] -= 0.5  # Negative bias\n",
    "        # Middle neurons stay random\n",
    "    \n",
    "    W2 = np.random.randn(11, 3) * 0.3\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Solution 2: Add explicit diversity regularization\n",
    "def train_with_diversity_regularization(X, Y, W1_init, W2_init, epochs=50):\n",
    "    \"\"\"\n",
    "    Training with explicit diversity loss to prevent collapse\n",
    "    \"\"\"\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    \n",
    "    # Momentum\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "    \n",
    "    lr = 0.1\n",
    "    diversity_weight = 0.1  # Weight for diversity loss\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        \n",
    "        for xi, yi in zip(X, Y):\n",
    "            # Forward pass\n",
    "            h_times = layer_forward(xi, W1, 1)\n",
    "            o_times = layer_forward(h_times, W2, 2)\n",
    "            \n",
    "            # Regular classification loss\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * 0.5 * sum([(o_times[j] - 2.05)**2 for j in non_ids])\n",
    "            \n",
    "            # DIVERSITY LOSS: Encourage hidden neurons to have different spike times\n",
    "            h_variance = np.var(h_times)\n",
    "            target_variance = 0.01  # We want at least this much variance\n",
    "            L_diversity = diversity_weight * max(0, target_variance - h_variance)**2\n",
    "            \n",
    "            L = L_target + L_non + L_diversity\n",
    "            epoch_loss += L\n",
    "            \n",
    "            # Regular gradients for classification\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = 0.5 * (o_times[j] - 2.05)\n",
    "            \n",
    "            # Diversity gradients for hidden layer\n",
    "            if h_variance < target_variance:\n",
    "                # Encourage diversity by pushing hidden times away from mean\n",
    "                h_mean = np.mean(h_times)\n",
    "                delta_h_diversity = 2 * diversity_weight * (target_variance - h_variance) * (h_times - h_mean) / len(h_times)\n",
    "            else:\n",
    "                delta_h_diversity = np.zeros_like(h_times)\n",
    "            \n",
    "            # Backprop through W2 (your original code)\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(W2[k, j], aug_h[k], 2, 0, 1)\n",
    "            \n",
    "            # Backprop into hidden layer (classification + diversity)\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                # Classification gradient\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], 2, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output\n",
    "                \n",
    "                # Add diversity gradient\n",
    "                delta_h[k] += delta_h_diversity[k]\n",
    "            \n",
    "            # Gradients for W1\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(W1[i, k], aug_xi[i], 1, 0, 1)\n",
    "            \n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "        \n",
    "        # Update weights\n",
    "        acc_dW1 /= len(X)\n",
    "        acc_dW2 /= len(X)\n",
    "        \n",
    "        # Clip gradients\n",
    "        acc_dW1 = np.clip(acc_dW1, -10, 10)\n",
    "        acc_dW2 = np.clip(acc_dW2, -10, 10)\n",
    "        \n",
    "        # Momentum update\n",
    "        vW1 = beta * vW1 + (1 - beta) * acc_dW1\n",
    "        vW2 = beta * vW2 + (1 - beta) * acc_dW2\n",
    "        \n",
    "        # Apply updates\n",
    "        W1 = np.clip(W1 - lr * vW1, -10, 10)\n",
    "        W2 = np.clip(W2 - lr * vW2, -10, 10)\n",
    "        \n",
    "        # Monitor progress\n",
    "        if ep % 5 == 0:\n",
    "            # Check current diversity\n",
    "            h_times_0 = layer_forward(X[0], W1, 1)\n",
    "            h_times_1 = layer_forward(X[1], W1, 1)\n",
    "            \n",
    "            print(f\"Epoch {ep+1}: loss={epoch_loss/len(X):.4f}\")\n",
    "            print(f\"  Hidden variance: {np.var(h_times_0):.6f} (sample 0), {np.var(h_times_1):.6f} (sample 1)\")\n",
    "            print(f\"  Hidden range: [{np.min(h_times_0):.3f}, {np.max(h_times_0):.3f}]\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Solution 3: Architectural fix - Add noise or different activation patterns\n",
    "def layer_forward_with_noise(inputs, W, layer_idx, noise_std=0.01):\n",
    "    \"\"\"\n",
    "    Add small amount of noise to break symmetry\n",
    "    \"\"\"\n",
    "    # Your original layer_forward code here, but add:\n",
    "    # scheduled_time = (sum/sr + layer)*ms + noise\n",
    "    \n",
    "    # This is a band-aid solution - better to fix the root cause\n",
    "    pass\n",
    "\n",
    "# Solution 4: Different initialization strategy\n",
    "def initialize_with_opposing_weights():\n",
    "    \"\"\"\n",
    "    Initialize some weights to be strongly positive, others strongly negative\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(5, 10) * 0.3\n",
    "    \n",
    "    # Make some neurons strongly favor early inputs\n",
    "    W1[:2, :5] += 1.0  # First 2 inputs get positive boost for first 5 neurons\n",
    "    \n",
    "    # Make some neurons strongly favor late inputs  \n",
    "    W1[2:4, 5:] += 1.0  # Last 2 inputs get positive boost for last 5 neurons\n",
    "    \n",
    "    W2 = np.random.randn(11, 3) * 0.3\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# The key insight: Your architecture is sound, but you need initial weight diversity\n",
    "def main_solution():\n",
    "    \"\"\"\n",
    "    The main solution: Start with diverse weights and add diversity regularization\n",
    "    \"\"\"\n",
    "    print(\"=== Solution: Diverse Initialization + Diversity Loss ===\")\n",
    "    \n",
    "    # Your data\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Use diverse initialization instead of collapsed weights\n",
    "    W1_init, W2_init = initialize_diverse_weights()\n",
    "    \n",
    "    # Check initial diversity\n",
    "    h_times_0 = layer_forward(X[0], W1_init, 1)\n",
    "    h_times_1 = layer_forward(X[1], W1_init, 1)\n",
    "    \n",
    "    print(f\"Initial hidden diversity:\")\n",
    "    print(f\"  Sample 0 variance: {np.var(h_times_0):.6f}\")\n",
    "    print(f\"  Sample 1 variance: {np.var(h_times_1):.6f}\")\n",
    "    print(f\"  Sample 0 range: [{np.min(h_times_0):.3f}, {np.max(h_times_0):.3f}]\")\n",
    "    print(f\"  Sample 1 range: [{np.min(h_times_1):.3f}, {np.max(h_times_1):.3f}]\")\n",
    "    \n",
    "    if np.var(h_times_0) > 0.005 and np.var(h_times_1) > 0.005:\n",
    "        print(\"✓ Good initial diversity!\")\n",
    "        \n",
    "        # Train with diversity regularization\n",
    "        W1_final, W2_final = train_with_diversity_regularization(X, Y, W1_init, W2_init, epochs=30)\n",
    "        \n",
    "        # Test final performance\n",
    "        print(\"\\n=== Final Test ===\")\n",
    "        for i, (xi, yi) in enumerate(zip(X[:2], Y[:2])):\n",
    "            h_times = layer_forward(xi, W1_final, 1)\n",
    "            o_times = layer_forward(h_times, W2_final, 2)\n",
    "            \n",
    "            pred_class = np.argmax(o_times)\n",
    "            true_class = np.argmax(yi)\n",
    "            \n",
    "            print(f\"Sample {i}: pred={pred_class}, true={true_class}\")\n",
    "            print(f\"  Hidden variance: {np.var(h_times):.6f}\")\n",
    "            print(f\"  Output times: {o_times}\")\n",
    "    else:\n",
    "        print(\"✗ Still collapsed - need different initialization\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    analyze_hidden_collapse()\n",
    "    \n",
    "    # Try the solution\n",
    "    main_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9897b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa435ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method InstanceTrackerSet.remove of InstanceTrackerSet({<weakref at 0x0000029EE47F2480; to 'CythonCodeObject' at 0x0000029EE46FB050>, <weakref at 0x0000029EE47F3D30; to 'CythonCodeObject' at 0x0000029EE47F6050>, <weakref at 0x0000029EE437CDB0; dead>, <weakref at 0x0000029EE47E8680; to 'Clock' at 0x0000029EE4532090>, <weakref at 0x0000029EE437C860; dead>, <weakref at 0x0000029EE47E8AE0; to 'CodeRunner' at 0x0000029EE47F6110>, <weakref at 0x0000029EE4618590; to 'Synapses' at 0x0000029EE4614110>, <weakref at 0x0000029EE4558310; to 'CythonCodeObject' at 0x0000029EE48D7150>, <weakref at 0x0000029EE47EBC90; to 'CythonCodeObject' at 0x0000029EE4670190>, <weakref at 0x0000029EE47F2570; to 'Resetter' at 0x0000029EE3CF9250>, <weakref at 0x0000029EE437D800; dead>, <weakref at 0x0000029EE47F25C0; to 'StateUpdater' at 0x0000029EE46F72D0>, <weakref at 0x0000029EE48054E0; to 'SynapticPathway' at 0x0000029EE47E2310>, <weakref at 0x0000029EE437C720; to 'Clock' at 0x0000029EE45423D0>, <weakref at 0x0000029EE3C93B50; to 'CodeRunner' at 0x0000029EE470E3D0>, <weakref at 0x0000029EE47EB740; dead>, <weakref at 0x0000029EE1769B70; dead>, <weakref at 0x0000029EE47F0A90; to 'SpikeGeneratorGroup' at 0x0000029EE46F4610>, <weakref at 0x0000029EE4807100; to 'Clock' at 0x0000029EE46F0690>, <weakref at 0x0000029EE3D90E00; dead>, <weakref at 0x0000029EE4295F30; to 'SynapticPathway' at 0x0000029EE3CB0750>, <weakref at 0x0000029EE461A890; to 'SpikeGeneratorGroup' at 0x0000029EE45E3790>, <weakref at 0x0000029EE3D93EC0; to 'Thresholder' at 0x0000029EE427D790>, <weakref at 0x0000029EE437C130; dead>, <weakref at 0x0000029EE4806840; to 'Synapses' at 0x0000029EE46F2810>, <weakref at 0x0000029EE42B3F60; to 'CythonCodeObject' at 0x0000029EE46FA950>, <weakref at 0x0000029EE47F0C70; to 'NeuronGroup' at 0x0000029EE3660990>, <weakref at 0x0000029EE4806FC0; to 'NeuronGroup' at 0x0000029EE1779990>, <weakref at 0x0000029EE4807650; dead>, <weakref at 0x0000029EE437EE30; dead>, <weakref at 0x0000029EE437C360; dead>, <weakref at 0x0000029EE47E86D0; to 'CythonCodeObject' at 0x0000029EE46F6AD0>, <weakref at 0x0000029EE47F36A0; to 'Thresholder' at 0x0000029EE46F5BD0>, <weakref at 0x0000029EE3D853F0; to 'Resetter' at 0x0000029EE427CBD0>, <weakref at 0x0000029EE3D93970; dead>, <weakref at 0x0000029EE4297DD0; to 'SpikeMonitor' at 0x0000029EE45E6C90>, <weakref at 0x0000029EE17D91C0; to 'MagicNetwork' at 0x0000029EE17CFCD0>, <weakref at 0x0000029EE48047C0; to 'SpikeMonitor' at 0x0000029EE3C7BD10>, <weakref at 0x0000029EE184CEA0; to 'Clock' at 0x0000029EE177BE10>, <weakref at 0x0000029EE4806CA0; dead>, <weakref at 0x0000029EE466B830; to 'CythonCodeObject' at 0x0000029EE47F4F10>, <weakref at 0x0000029EE3D873D0; to 'StateUpdater' at 0x0000029EE4242F50>})>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\irtho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\tracking.py\", line 32, in remove\n",
      "    set.remove(self, value)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n",
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n",
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n",
      "Epoch 1/1 — avg loss=0.3348\n",
      "             ‖W1‖=1.306, ‖W2‖=3.945\n",
      "\n",
      "Trained W1: [[-2.24802890e-01 -5.02760943e-05 -3.02786849e-02 -7.89465047e-02\n",
      "  -8.32850990e-02 -4.77545338e-02 -8.20297270e-02 -8.11190851e-02\n",
      "  -1.19828297e-01 -1.44893684e-01]\n",
      " [-1.11390344e-01 -1.80939746e-01 -1.34243925e-01 -9.72715411e-02\n",
      "  -1.90581208e-01 -2.27950846e-02 -1.44948526e-01 -1.37085700e-01\n",
      "   1.53694720e-01  4.46186585e-02]\n",
      " [-2.41631112e-01 -1.56714608e-01  3.69612932e-01 -1.93654213e-01\n",
      "  -2.64931721e-01  2.68245103e-01 -4.42425639e-01  1.83656054e-02\n",
      "  -2.24950602e-01  2.78443194e-01]\n",
      " [ 5.13877625e-02  1.55790838e-02  3.39575082e-01  3.24111147e-01\n",
      "   1.25768049e-01 -5.69899260e-02  3.95072697e-02 -3.10625651e-01\n",
      "  -2.45227505e-01  4.21001016e-01]\n",
      " [-1.31178265e-01  6.46637541e-03 -9.75420633e-02  4.69224677e-03\n",
      "   1.15690476e-01 -2.10678045e-01  6.00154219e-02  1.02195157e-01\n",
      "   8.55197047e-02 -9.51946507e-02]]\n",
      "Trained W2: [[-0.31485765 -1.08552814  0.37171857]\n",
      " [ 0.78860029 -0.93310515  0.24188146]\n",
      " [ 0.66625006 -0.90895183  0.29448823]\n",
      " [ 0.88711645 -0.93572897  0.34326311]\n",
      " [ 0.08522722 -0.82672152  0.30371417]\n",
      " [ 0.90997097 -1.05585606  0.36229413]\n",
      " [ 0.77692459 -1.06224411  0.45785575]\n",
      " [ 0.72549459 -1.05064915  0.407461  ]\n",
      " [ 0.53035912 -0.81390781  0.28468818]\n",
      " [ 0.8701721  -0.99474145  0.29435131]\n",
      " [-0.01780186 -0.05807777  0.03830319]]\n",
      "Hidden times for x0: [0.934 0.966 1.024 0.986 0.952 0.999 0.941 0.942 0.966 1.03 ]\n",
      "Hidden times for x1: [0.899 0.952 1.094 1.002 0.944 1.023 0.877 0.919 0.927 1.105]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    return(np.tanh(x*w))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = W1 - lr * vW1\n",
    "        W2 = W2 - lr * vW2\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=10, lr=0.2)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e7d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n",
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 255\u001b[0m\n",
      "\u001b[0;32m    251\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "\u001b[1;32m--> 255\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m h_times \u001b[38;5;241m=\u001b[39m layer_forward(xi, W1, layer1_idx)\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n",
      "\u001b[1;32m--> 156\u001b[0m o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# — Separation loss — # calcs to loss for a single sample will not touch for now \u001b[39;00m\n",
      "\u001b[0;32m    160\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yi)\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n",
      "\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n",
      "\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n",
      "\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    336\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    342\u001b[0m ):\n",
      "\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n",
      "\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    240\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n",
      "\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n",
      "\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1276\u001b[0m )\n",
      "\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n",
      "\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=1, lr=0.0)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58402421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n",
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 255\u001b[0m\n",
      "\u001b[0;32m    251\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "\u001b[1;32m--> 255\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m h_times \u001b[38;5;241m=\u001b[39m layer_forward(xi, W1, layer1_idx)\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n",
      "\u001b[1;32m--> 156\u001b[0m o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# — Separation loss — # calcs to loss for a single sample will not touch for now \u001b[39;00m\n",
      "\u001b[0;32m    160\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yi)\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n",
      "\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n",
      "\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n",
      "\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    336\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    342\u001b[0m ):\n",
      "\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n",
      "\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    240\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n",
      "\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n",
      "\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1276\u001b[0m )\n",
      "\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n",
      "\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=1, lr=0.0)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd4c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n",
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 255\u001b[0m\n",
      "\u001b[0;32m    251\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "\u001b[1;32m--> 255\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m h_times \u001b[38;5;241m=\u001b[39m layer_forward(xi, W1, layer1_idx)\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n",
      "\u001b[1;32m--> 156\u001b[0m o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# — Separation loss — # calcs to loss for a single sample will not touch for now \u001b[39;00m\n",
      "\u001b[0;32m    160\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yi)\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n",
      "\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n",
      "\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n",
      "\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    336\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    342\u001b[0m ):\n",
      "\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n",
      "\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    240\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n",
      "\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n",
      "\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1276\u001b[0m )\n",
      "\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n",
      "\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = W1 - lr * vW1\n",
    "        W2 = W2 - lr * vW2\n",
    "\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=1, lr=0.0)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.228 5.    5.   ]\n",
      " Predicted class: 1, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.228 5.    5.   ]\n",
      " Predicted class: 1, True class: 2\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.359 5.    5.   ]\n",
      " Predicted class: 1, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.267 5.    5.   ]\n",
      " Predicted class: 1, True class: 2\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.228 5.    5.   ]\n",
      " Predicted class: 1, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.305 1.152 1.397]\n",
      " Predicted class: 0, True class: 2\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_test, y_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# call layer_forward(positionally) rather than with layer1_idx=\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m layer_forward(x_test, W1_tr, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     pred_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(o_times)  \n\u001b[0;32m     33\u001b[0m     true_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test)\n",
      "Cell \u001b[1;32mIn[14], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1276\u001b[0m )\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#testing on Iris\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Iris dataset and scale features to [0.05,0.95]\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    y_encoded = np.zeros((y.size, y.max()+1))\n",
    "    y_encoded[np.arange(y.size), y] = 1\n",
    "    \n",
    "    def scale_features(x):\n",
    "        mn, mx = x.min(axis=0), x.max(axis=0)\n",
    "        x_norm = (x - mn) / (mx - mn)\n",
    "        return x_norm * 0.9 + 0.05\n",
    "\n",
    "    X_scaled = scale_features(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2,\n",
    "        random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for x_test, y_test in zip(X, Y):\n",
    "        # call layer_forward(positionally) rather than with layer1_idx=\n",
    "        h_times = layer_forward(x_test, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  \n",
    "        true_class = np.argmax(y_test)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
