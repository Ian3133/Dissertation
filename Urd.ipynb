{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fee676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — avg loss=2.2882\n",
      "             ‖W1‖=9.166, ‖W2‖=9.157\n",
      "\n",
      "Epoch 2/10 — avg loss=2.2877\n",
      "             ‖W1‖=9.342, ‖W2‖=9.557\n",
      "\n",
      "Epoch 3/10 — avg loss=1.2062\n",
      "             ‖W1‖=9.647, ‖W2‖=9.949\n",
      "\n",
      "Epoch 4/10 — avg loss=1.1838\n",
      "             ‖W1‖=10.164, ‖W2‖=10.250\n",
      "\n",
      "Epoch 5/10 — avg loss=0.1218\n",
      "             ‖W1‖=10.706, ‖W2‖=10.539\n",
      "\n",
      "Epoch 6/10 — avg loss=1.1477\n",
      "             ‖W1‖=11.334, ‖W2‖=10.897\n",
      "\n",
      "Epoch 7/10 — avg loss=0.1275\n",
      "             ‖W1‖=11.967, ‖W2‖=11.237\n",
      "\n",
      "Epoch 8/10 — avg loss=1.1472\n",
      "             ‖W1‖=12.690, ‖W2‖=11.640\n",
      "\n",
      "Epoch 9/10 — avg loss=0.1362\n",
      "             ‖W1‖=13.393, ‖W2‖=12.016\n",
      "\n",
      "Epoch 10/10 — avg loss=0.1400\n",
      "             ‖W1‖=14.060, ‖W2‖=12.366\n",
      "\n",
      "Trained W1: [[-5.47237496 -0.30621458 -1.39157777 -1.3210122   2.02990935  0.13222637\n",
      "  -5.66102315  1.87108098  0.97082094 -0.32550164]\n",
      " [-3.03721634 -0.25017633 -0.288491   -0.34870021 -0.86124509 -0.28085861\n",
      "  -4.13226868  0.19064628  0.40280951  0.20816894]\n",
      " [-0.29862368  0.11241383  0.78464379  1.01685423 -5.64681193 -0.23029723\n",
      "  -0.44403956 -1.55846104 -0.37998453  0.41334346]\n",
      " [-0.51428181  0.07410395  0.87269359  0.81592763 -6.92248344 -0.3433382\n",
      "  -0.0732441  -1.95986422 -0.85840606 -0.54503142]\n",
      " [-0.88895798 -0.82863464 -0.396815   -0.4715086   1.21101224 -0.17061962\n",
      "  -0.82194381  0.07770242  0.34755369 -0.44260157]]\n",
      "Trained W2: [[-5.87570277 -0.77298144  2.40613507]\n",
      " [-0.35713855 -0.37364176  0.05067742]\n",
      " [-1.20775789 -0.90124756  0.91527691]\n",
      " [-1.41811004 -0.74584293  1.01938602]\n",
      " [ 2.23950076 -0.5753259  -5.7475052 ]\n",
      " [-0.0841799  -0.72316865 -0.63856046]\n",
      " [-6.3274453  -0.9335057   2.88800156]\n",
      " [ 1.46798696 -0.55346132 -2.11077972]\n",
      " [ 0.57088565 -1.58826654 -1.18078915]\n",
      " [-0.17113957 -0.87478809 -0.05865646]\n",
      " [ 0.17223993 -1.85447367 -1.0169616 ]]\n",
      "Hidden times for x0: [5.    1.433 1.153 1.261 5.    1.404 5.    1.198 1.417 1.449]\n",
      "Hidden times for x1: [5.    1.559 1.416 1.437 5.    1.543 5.    5.    1.543 1.549]\n",
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.465 2.039 2.318]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.255 2.041 2.449]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.465 2.039 2.318]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.255 2.041 2.449]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.465 2.039 2.318]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.255 2.041 2.449]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.465 2.039 2.318]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.255 2.041 2.449]\n",
      " Predicted class: 2, True class: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.0,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators for this epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            # — Separation loss —\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # # print changes before applying them\n",
    "            # print(\"\\nWeight changes for this sample:\")\n",
    "            # for i in range(W1.shape[0]):\n",
    "            #     for k in range(W1.shape[1]):\n",
    "            #         change = -lr * dW1[i,k]\n",
    "            #         print(f\"  W1[{i},{k}] change: {change:+.5f}\")\n",
    "            # for k in range(W2.shape[0]):\n",
    "            #     for j in range(W2.shape[1]):\n",
    "            #         change = -lr * dW2[k,j]\n",
    "            #         print(f\"  W2[{k},{j}] change: {change:+.5f}\")\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.05, 2.05])\n",
    "    y1 = np.array([2.05, 2.05, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    # X= []\n",
    "    # Y = []\n",
    "    # for _ in range(10):\n",
    "    #     X.append(x0 + np.random.randn(4)*0.02);  Y.append(y0)\n",
    "    #     X.append(x1 + np.random.randn(4)*0.02);  Y.append(y1)\n",
    "    \n",
    "\n",
    "    \n",
    "     #W1_0 = np.array([[0.21958991, 0.16223261, 0.02545166, 0.18849804, 0.09521701, 0.22744421, 0.05556097, 0.33130229, 0.03974721, 0.1968464],\n",
    "    #             [0.41958955, 0.47541312, 0.22287581, 0.69627866, 0.83639384, 0.79597959, 0.15029805, 0.126486, 0.18285382, 0.07470098],\n",
    "    #             [0.69559509, 0.41228614, 0.06028855, 0.51098037, 0.33730611, 1.17605488, 0.15405119, 0.28079173, 0.17365651, 0.23041775],\n",
    "    #             [0.79721356, 0.82210554, 0.15028745, 1.09421856, 0.68280376, 1.07577422, 0.16962136, 0.23838796, 0.0735181, 0.1719861],\n",
    "    #             [0.0631449, 0.10618091, 0.05791614, 0.0260418, -0.01797577, -0.1209534, 0.18702474, -0.01662061, -0.0683026, 0.05468931]])\n",
    "        #W1_0 = [[ 0.21958991,  0.16223261,  0.02545166  0.18849804  0.09521701  0.22744421, 0.05556097  0.33130229  0.03974721  0.1968464 ], [ 0.41958955  0.47541312  0.22287581  0.69627866  0.83639384  0.79597959, 0.15029805  0.126486    0.18285382  0.07470098], [ 0.69559509  0.41228614  0.06028855  0.51098037  0.33730611  1.17605488, 0.15405119  0.28079173  0.17365651  0.23041775], [ 0.79721356  0.82210554  0.15028745  1.09421856  0.68280376  1.07577422, 0.16962136  0.23838796  0.0735181   0.1719861 ], [ 0.0631449   0.10618091  0.05791614  0.0260418  -0.01797577 -0.1209534, 0.18702474 -0.01662061 -0.0683026   0.05468931]]\n",
    "        \n",
    "    \n",
    "        \n",
    "        #np.random.randn(4+1, 10)*0.1   # +1 for bias\n",
    "    w1 = np.load(\"w1.npy\")\n",
    "    b1 = np.load(\"b1.npy\")\n",
    "    w2 = np.load(\"w2.npy\")\n",
    "    b2 = np.load(\"b2.npy\")\n",
    "\n",
    "    b1 = b1.reshape(1, -1)\n",
    "    b2 = b2.reshape(1, -1)\n",
    "\n",
    "    W1_0 = np.vstack([w1, b1])  # Combine weight and bias\n",
    "    W2_0 = np.vstack([w2, b2])\n",
    "    \n",
    "    # W2_0 = np.array([[ 0.38843549, -1.10085101,  0.38776897],\n",
    "    #             [ 0.35841177, -1.06985881,  0.40542767],\n",
    "    #             [ 0.40732835, -0.63317637,  0.59202003],\n",
    "    #             [ 0.32589618, -0.9691458,   0.48481597],\n",
    "    #             [ 0.26060079, -0.85267046,  0.75057083],\n",
    "    #             [ 0.31287437, -1.34806606,  0.44407244],\n",
    "    #             [ 0.1996638,  -0.65507816,  0.27473825],\n",
    "    #             [ 0.40169137, -0.9979045,   0.09884702],\n",
    "    #             [ 0.37579471, -0.62826323,  0.58035222],\n",
    "    #             [ 0.2191151,  -0.84980247,  0.14767976],\n",
    "    #             [ 0.11199583, -0.16498428,  0.00871235]])\n",
    "    \n",
    "    # will print out last times so DO NOT run the same the same expermeent to have a differnt outcoem\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=10, lr=0.1)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # ── Now test on the same two patterns ──\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for xi, yi in zip(X, Y):\n",
    "        # call layer_forward(positionally) rather than with layer1_idx=\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  # changed to argmin WHY???\n",
    "        true_class = np.argmax(yi)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
