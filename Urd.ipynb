{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — avg loss=0.1989\n",
      "             ‖W1‖=1.010, ‖W2‖=6.809\n",
      "\n",
      "Epoch 2/30 — avg loss=0.1989\n",
      "             ‖W1‖=1.009, ‖W2‖=6.812\n",
      "\n",
      "Epoch 3/30 — avg loss=0.1989\n",
      "             ‖W1‖=1.008, ‖W2‖=6.815\n",
      "\n",
      "Epoch 4/30 — avg loss=0.1989\n",
      "             ‖W1‖=1.007, ‖W2‖=6.819\n",
      "\n",
      "Epoch 5/30 — avg loss=0.1991\n",
      "             ‖W1‖=1.006, ‖W2‖=6.824\n",
      "\n",
      "Epoch 6/30 — avg loss=0.1991\n",
      "             ‖W1‖=1.004, ‖W2‖=6.829\n",
      "\n",
      "Epoch 7/30 — avg loss=0.1991\n",
      "             ‖W1‖=1.003, ‖W2‖=6.835\n",
      "\n",
      "Epoch 8/30 — avg loss=0.1991\n",
      "             ‖W1‖=1.001, ‖W2‖=6.842\n",
      "\n",
      "Epoch 9/30 — avg loss=0.1991\n",
      "             ‖W1‖=0.999, ‖W2‖=6.849\n",
      "\n",
      "Epoch 10/30 — avg loss=0.1994\n",
      "             ‖W1‖=0.997, ‖W2‖=6.856\n",
      "\n",
      "Epoch 11/30 — avg loss=0.1994\n",
      "             ‖W1‖=0.995, ‖W2‖=6.864\n",
      "\n",
      "Epoch 12/30 — avg loss=0.1997\n",
      "             ‖W1‖=0.993, ‖W2‖=6.872\n",
      "\n",
      "Epoch 13/30 — avg loss=0.1998\n",
      "             ‖W1‖=0.991, ‖W2‖=6.881\n",
      "\n",
      "Epoch 14/30 — avg loss=0.1998\n",
      "             ‖W1‖=0.989, ‖W2‖=6.889\n",
      "\n",
      "Epoch 15/30 — avg loss=0.1998\n",
      "             ‖W1‖=0.987, ‖W2‖=6.898\n",
      "\n",
      "Epoch 16/30 — avg loss=0.1999\n",
      "             ‖W1‖=0.985, ‖W2‖=6.907\n",
      "\n",
      "Epoch 17/30 — avg loss=0.2002\n",
      "             ‖W1‖=0.983, ‖W2‖=6.916\n",
      "\n",
      "Epoch 18/30 — avg loss=0.2002\n",
      "             ‖W1‖=0.981, ‖W2‖=6.925\n",
      "\n",
      "Epoch 19/30 — avg loss=0.2004\n",
      "             ‖W1‖=0.979, ‖W2‖=6.934\n",
      "\n",
      "Epoch 20/30 — avg loss=0.2005\n",
      "             ‖W1‖=0.977, ‖W2‖=6.944\n",
      "\n",
      "Epoch 21/30 — avg loss=0.2007\n",
      "             ‖W1‖=0.975, ‖W2‖=6.953\n",
      "\n",
      "Epoch 22/30 — avg loss=0.2008\n",
      "             ‖W1‖=0.973, ‖W2‖=6.962\n",
      "\n",
      "Epoch 23/30 — avg loss=0.2008\n",
      "             ‖W1‖=0.971, ‖W2‖=6.972\n",
      "\n",
      "Epoch 24/30 — avg loss=0.2010\n",
      "             ‖W1‖=0.970, ‖W2‖=6.981\n",
      "\n",
      "Epoch 25/30 — avg loss=0.2010\n",
      "             ‖W1‖=0.968, ‖W2‖=6.990\n",
      "\n",
      "Epoch 26/30 — avg loss=0.2010\n",
      "             ‖W1‖=0.966, ‖W2‖=7.000\n",
      "\n",
      "Epoch 27/30 — avg loss=0.2014\n",
      "             ‖W1‖=0.965, ‖W2‖=7.009\n",
      "\n",
      "Epoch 28/30 — avg loss=0.2014\n",
      "             ‖W1‖=0.963, ‖W2‖=7.018\n",
      "\n",
      "Epoch 29/30 — avg loss=0.2016\n",
      "             ‖W1‖=0.961, ‖W2‖=7.027\n",
      "\n",
      "Epoch 30/30 — avg loss=0.2019\n",
      "             ‖W1‖=0.960, ‖W2‖=7.036\n",
      "\n",
      "Trained W1: [[-0.22115258  0.00480701  0.0504227  -0.04234957 -0.02433013 -0.06530512\n",
      "  -0.08251779 -0.02829691 -0.13250274 -0.06513951]\n",
      " [-0.1007527  -0.15704425 -0.17607711 -0.09344395 -0.13127104 -0.07096185\n",
      "  -0.09552267 -0.06075495  0.20018693 -0.00531869]\n",
      " [-0.22322761 -0.10913267  0.14601375 -0.24501361 -0.2258843   0.18754175\n",
      "  -0.33488155  0.10860824 -0.09568412  0.03032264]\n",
      " [ 0.07559678  0.07560278  0.10016442  0.27714082  0.19470578 -0.16332578\n",
      "   0.17990326 -0.19124761 -0.09573063  0.16887984]\n",
      " [-0.15704821 -0.05368842  0.00682978 -0.00489857 -0.03238849 -0.09575397\n",
      "  -0.06384195 -0.08819658 -0.03101678  0.02892516]]\n",
      "Trained W2: [[-0.18926713 -0.97746804  0.17947572]\n",
      " [ 1.11016034 -0.67953316 -0.23710475]\n",
      " [-1.35142694 -1.28430503  2.51003419]\n",
      " [ 0.35481847 -0.88335741  0.83581751]\n",
      " [ 0.11047056 -0.18745652 -0.12699667]\n",
      " [ 0.38453657 -1.55923921  1.20202237]\n",
      " [ 1.61483604 -0.53665939 -0.70913311]\n",
      " [ 1.14120798 -0.23282919 -0.5271077 ]\n",
      " [ 1.53035287 -0.3232158  -1.01835754]\n",
      " [-1.30245483 -1.43756164  2.70133655]\n",
      " [-1.79493475  2.32391731 -1.74951349]]\n",
      "Hidden times for x0: [1.501 1.502 1.485 1.496 1.501 1.496 1.506 1.504 1.507 1.483]\n",
      "Hidden times for x1: [1.499 1.498 1.512 1.503 1.499 1.504 1.494 1.496 1.493 1.513]\n",
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.552 2.458 2.532]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.538 2.45  2.554]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.552 2.458 2.532]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.538 2.45  2.554]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.552 2.458 2.532]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.538 2.45  2.554]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.552 2.458 2.532]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.538 2.45  2.554]\n",
      " Predicted class: 2, True class: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from brian2 import *\n",
    "# import numpy as np\n",
    "# import logging\n",
    "# import warnings\n",
    "\n",
    "\n",
    "\n",
    "# from brian2 import prefs, set_device\n",
    "\n",
    "# # Tell Brian2 to use the Cython code generator:\n",
    "# prefs.codegen.target = 'cython'\n",
    "\n",
    "# # Optionally compile but keep Python interface:\n",
    "# set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # suppress overflow warnings\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "# numpy.seterr(over='ignore', under='ignore')\n",
    "# logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Spike timing and derivative\n",
    "\n",
    "# start_scope()\n",
    "# defaultclock.dt = 0.001*ms\n",
    "\n",
    "# @implementation('numpy', discard_units=True)\n",
    "# @check_units(w=1, time=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "# def spike_timing(w, time):\n",
    "#     #print(\"spikes recived\" , spikes_received)\n",
    "#     #print(\"weight= \", w)\n",
    "#     x = (time % 1)\n",
    "#     #print(\"x= \", x)\n",
    "#     z = 5.0 * (x - 0.5)  # Smoother sigmoid\n",
    "#     #print(\"z= \", z)\n",
    "#     sigmoid_val = 1.0 / (1.0 + np.exp(-w * z))\n",
    "#     # if sigmoid_val < layer:\n",
    "#     #     return layer\n",
    "#     return sigmoid_val\n",
    "\n",
    "# @implementation('numpy', discard_units=True)\n",
    "# @check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "# def d_spike_timing_dw(w, time):\n",
    "#     x = (time % 1)\n",
    "#     z = 5.0 * (x - 0.5)\n",
    "#     sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "#     return sig * (1.0 - sig) * z\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "# def layer_forward(inputs, W, layer_idx):\n",
    "#     \"\"\"\n",
    "#     inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "#     W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "#     layer_idx: integer layer number\n",
    "#     returns: array of output spike times (ms)\n",
    "#     \"\"\"\n",
    "#     # 1) augment inputs with bias spike @ t=0\n",
    "#     bias_time = 0.0\n",
    "#     aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "#     n_in_plus_bias, n_out = W.shape\n",
    "#     assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "#     out_times = []\n",
    "#     for j in range(n_out):\n",
    "#         start_scope()\n",
    "#         defaultclock.dt = 0.001*ms\n",
    "\n",
    "#         # single post‐synaptic neuron\n",
    "#         G = NeuronGroup(1, '''\n",
    "#             v : 1\n",
    "#             sum : 1\n",
    "#             sr : 1\n",
    "#             scheduled_time : second\n",
    "#             global_clock : 1\n",
    "#             spiked : boolean\n",
    "#         ''', threshold='v>1', reset='''\n",
    "#         v = 0\n",
    "#         spiked = True\n",
    "#         ''', method='exact')\n",
    "\n",
    "#         # init\n",
    "#         G.v = G.sum = G.sr = 0\n",
    "#         G.global_clock = 0\n",
    "#         G.scheduled_time = 1e9*second\n",
    "\n",
    "#         # stim: now includes bias spike at t=0\n",
    "#         stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "#                                    indices=list(range(n_in_plus_bias)),\n",
    "#                                    times=aug_inputs*ms)\n",
    "\n",
    "#         S = Synapses(stim, G, '''w:1\n",
    "#             layer:1''',\n",
    "#             on_pre='''\n",
    "#             sr += 1\n",
    "#             sum += spike_timing(w, global_clock)\n",
    "#             scheduled_time = (sum/sr + layer + 0.004)*ms\n",
    "#         ''')\n",
    "#         S.connect(True)\n",
    "#         S.w = W[:, j]\n",
    "#         S.layer = layer_idx\n",
    "\n",
    "#         G.run_regularly('''\n",
    "#             v = (1.0 - spiked) * int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "#             global_clock += 0.001\n",
    "#         ''', dt=0.001*ms)\n",
    "\n",
    "#         mon = SpikeMonitor(G)\n",
    "#         run(5*ms)\n",
    "\n",
    "#         ts = mon.spike_trains()[0]\n",
    "#         t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "#         out_times.append(t0)\n",
    "\n",
    "#     return np.array(out_times)\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Training loop with backprop for 4-10-3\n",
    "# def train_snn_backprop(\n",
    "#     X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "#     W1_init, W2_init,\n",
    "#     epochs=10, lr=0.1,\n",
    "#     max_grad=50.0, w_min=-50.0, w_max=50.0,\n",
    "#     non_target_time=2.0,\n",
    "#     λ=0.5 # changed from 0.5               # non-target penalty weight\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Trains a 4→10→3 spiking network with:\n",
    "#       • batched gradient updates\n",
    "#       • boosted hidden-layer learning rate\n",
    "#       • separate gradient clipping per layer\n",
    "#       • classical momentum smoothing\n",
    "#     \"\"\"\n",
    "#     # Initialize weights\n",
    "#     W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "#     W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "#     # Momentum buffers\n",
    "#     beta = 0.9\n",
    "#     vW1 = np.zeros_like(W1)\n",
    "#     vW2 = np.zeros_like(W2)\n",
    "\n",
    "#     layer1_idx, layer2_idx = 1, 2\n",
    "#     N = len(X)\n",
    "\n",
    "#     for ep in range(epochs):\n",
    "#         # Accumulators for this epoch\n",
    "#         acc_dW1 = np.zeros_like(W1)\n",
    "#         acc_dW2 = np.zeros_like(W2)\n",
    "#         epoch_loss = 0.0\n",
    "\n",
    "#         for xi, yi in zip(X, Y):\n",
    "#             # — Forward pass —\n",
    "#             h_times = layer_forward(xi, W1, layer1_idx)\n",
    "#             o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "#             # — Separation loss —\n",
    "#             target_idx = np.argmax(yi)\n",
    "#             L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "#             non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "#             L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "#             L = L_target + L_non\n",
    "#             epoch_loss += L\n",
    "\n",
    "#             # — Gradients for W2 —\n",
    "#             delta_o = np.zeros_like(o_times)\n",
    "#             delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "#             for j in non_ids:\n",
    "#                 delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "#             aug_h = np.concatenate((h_times, [0.0]))\n",
    "#             dW2 = np.zeros_like(W2)\n",
    "#             for k in range(W2.shape[0]):\n",
    "#                 for j in range(W2.shape[1]):\n",
    "#                     dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "#                         W2[k, j], aug_h[k])\n",
    "\n",
    "#             # — Backprop into hidden & gradients for W1 —\n",
    "#             delta_h = np.zeros_like(h_times)\n",
    "#             for k in range(len(h_times)):\n",
    "#                 for j in range(W2.shape[1]):\n",
    "#                     dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "#                     delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "#             aug_xi = np.concatenate((xi, [0.0]))\n",
    "#             dW1 = np.zeros_like(W1)\n",
    "#             for i in range(W1.shape[0]):\n",
    "#                 for k in range(W1.shape[1]):\n",
    "#                     dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "#                         W1[i, k], aug_xi[i])\n",
    "\n",
    "#             # — Accumulate —\n",
    "#             acc_dW1 += dW1\n",
    "#             acc_dW2 += dW2\n",
    "\n",
    "#         # — Average & clip gradients —\n",
    "#         acc_dW1 /= N\n",
    "#         acc_dW2 /= N\n",
    "\n",
    "#         # Boost hidden-layer rate\n",
    "#         lr1 = 5 * lr\n",
    "\n",
    "#         # Separate clipping thresholds\n",
    "#         g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "#         g2 = np.clip(acc_dW2, -max_grad, 5 * max_grad)\n",
    "\n",
    "#         # — Momentum updates —\n",
    "#         vW1 = beta * vW1 + (1 - beta) * g1\n",
    "#         vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "#         # — Apply weight updates & clamp —\n",
    "#         W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "#         W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "#         print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "#         print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "#     return W1, W2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # example usage with fixed input/target pairs\n",
    "#     # 4 inputs per sample, constant across 8 samples\n",
    "#     x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "#     x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "#     X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "#     # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "#     y0 = np.array([2.95, 2.05, 2.05])\n",
    "#     y1 = np.array([2.05, 2.05, 2.95])\n",
    "#     Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "#     # X= []\n",
    "#     # Y = []\n",
    "#     # for _ in range(10):\n",
    "#     #     X.append(x0 + np.random.randn(4)*0.02);  Y.append(y0)\n",
    "#     #     X.append(x1 + np.random.randn(4)*0.02);  Y.append(y1)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#     #np.random.randn(4+1, 10)*0.1   # +1 for bias\n",
    "#     # W1_0 = np.random.randn(5, 10) * 0.1  # 4 inputs + bias, 10 hidden\n",
    "#     # W2_0 = np.random.randn(11, 3) * 0.1\n",
    "#     #W1_0 = \n",
    "    \n",
    "#     # print(np.load(\"w1.npy\").shape())\n",
    "#     # #W2_0 = np.load(\"w2.npy\").shape() \n",
    "#     # print(np.load(\"w1.npy\").shape())  \n",
    "\n",
    "#     W1_0 = np.load(\"w1.npy\")\n",
    "#     #print(\"W1_0 shape:\", W1_0.shape)\n",
    "#     W2_0 = np.load(\"w2.npy\")\n",
    "#     #print(\"W2_0 shape:\", W2_0.shape)\n",
    "  \n",
    "#     # W1_0 = np.load(\"NN_W_1.npy\")\n",
    "#     # W2_0 = np.load(\"NN_W_2.npy\")\n",
    "\n",
    "#     # train\n",
    "#     W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "#                                       epochs=30, lr=0.75)\n",
    "#     print(\"Trained W1:\", W1_tr)\n",
    "#     print(\"Trained W2:\", W2_tr) \n",
    "#     print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "#     print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "#     # ── Now test on the same two patterns ──\n",
    "#     print(\"\\n=== Test predictions ===\")\n",
    "#     for xi, yi in zip(X, Y):\n",
    "#         # call layer_forward(positionally) rather than with layer1_idx=\n",
    "#         h_times = layer_forward(xi, W1_tr, 1)\n",
    "#         o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "#         pred_class = np.argmax(o_times)  \n",
    "#         true_class = np.argmax(yi)\n",
    "\n",
    "#         print(f\"Input: {xi}\")\n",
    "#         print(f\" Spike times: {o_times}\")\n",
    "#         print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "\n",
    "#     np.save(\"w1.npy\", W1_tr)\n",
    "#     np.save(\"w2.npy\", W2_tr)\n",
    "#     print(\"weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gradient norms: ‖∇W1‖=0.027865, ‖∇W2‖=0.164626\n",
      "Epoch 1/30 — avg loss=1.1456\n",
      "             ‖W1‖=28.824, ‖W2‖=36.501\n",
      "Epoch 2/30 — avg loss=1.1445\n",
      "             ‖W1‖=28.826, ‖W2‖=36.509\n",
      "Epoch 3/30 — avg loss=1.1445\n",
      "             ‖W1‖=28.830, ‖W2‖=36.520\n",
      "Epoch 4/30 — avg loss=1.1417\n",
      "             ‖W1‖=28.834, ‖W2‖=36.535\n",
      "Epoch 5/30 — avg loss=1.1400\n",
      "             ‖W1‖=28.838, ‖W2‖=36.553\n",
      "  Gradient norms: ‖∇W1‖=0.029020, ‖∇W2‖=0.171121\n",
      "Epoch 6/30 — avg loss=1.1371\n",
      "             ‖W1‖=28.844, ‖W2‖=36.575\n",
      "Epoch 7/30 — avg loss=1.1335\n",
      "             ‖W1‖=28.851, ‖W2‖=36.599\n",
      "Epoch 8/30 — avg loss=1.1304\n",
      "             ‖W1‖=28.858, ‖W2‖=36.627\n",
      "Epoch 9/30 — avg loss=1.1287\n",
      "             ‖W1‖=28.867, ‖W2‖=36.657\n",
      "Epoch 10/30 — avg loss=1.1254\n",
      "             ‖W1‖=28.876, ‖W2‖=36.690\n",
      "  Gradient norms: ‖∇W1‖=0.031737, ‖∇W2‖=0.168958\n",
      "Epoch 11/30 — avg loss=1.1187\n",
      "             ‖W1‖=28.887, ‖W2‖=36.726\n",
      "Epoch 12/30 — avg loss=1.1154\n",
      "             ‖W1‖=28.898, ‖W2‖=36.765\n",
      "Epoch 13/30 — avg loss=1.1121\n",
      "             ‖W1‖=28.910, ‖W2‖=36.806\n",
      "Epoch 14/30 — avg loss=1.1070\n",
      "             ‖W1‖=28.923, ‖W2‖=36.850\n",
      "Epoch 15/30 — avg loss=1.1024\n",
      "             ‖W1‖=28.937, ‖W2‖=36.896\n",
      "  Gradient norms: ‖∇W1‖=0.032745, ‖∇W2‖=0.151357\n",
      "Epoch 16/30 — avg loss=1.0971\n",
      "             ‖W1‖=28.953, ‖W2‖=36.945\n",
      "Epoch 17/30 — avg loss=1.0946\n",
      "             ‖W1‖=28.969, ‖W2‖=36.995\n",
      "Epoch 18/30 — avg loss=1.0898\n",
      "             ‖W1‖=28.985, ‖W2‖=37.048\n",
      "Epoch 19/30 — avg loss=1.0849\n",
      "             ‖W1‖=29.003, ‖W2‖=37.101\n",
      "Epoch 20/30 — avg loss=1.0817\n",
      "             ‖W1‖=29.021, ‖W2‖=37.157\n",
      "  Gradient norms: ‖∇W1‖=0.029779, ‖∇W2‖=0.125306\n",
      "Epoch 21/30 — avg loss=1.0767\n",
      "             ‖W1‖=29.040, ‖W2‖=37.213\n",
      "Epoch 22/30 — avg loss=1.0743\n",
      "             ‖W1‖=29.060, ‖W2‖=37.271\n",
      "Epoch 23/30 — avg loss=1.0696\n",
      "             ‖W1‖=29.080, ‖W2‖=37.329\n",
      "Epoch 24/30 — avg loss=1.0667\n",
      "             ‖W1‖=29.100, ‖W2‖=37.388\n",
      "Epoch 25/30 — avg loss=1.0637\n",
      "             ‖W1‖=29.121, ‖W2‖=37.447\n",
      "  Gradient norms: ‖∇W1‖=0.024016, ‖∇W2‖=0.097797\n",
      "Epoch 26/30 — avg loss=1.0589\n",
      "             ‖W1‖=29.142, ‖W2‖=37.506\n",
      "Epoch 27/30 — avg loss=1.0574\n",
      "             ‖W1‖=29.163, ‖W2‖=37.565\n",
      "Epoch 28/30 — avg loss=1.0545\n",
      "             ‖W1‖=29.184, ‖W2‖=37.624\n",
      "Epoch 29/30 — avg loss=1.0527\n",
      "             ‖W1‖=29.205, ‖W2‖=37.682\n",
      "Epoch 30/30 — avg loss=1.0500\n",
      "             ‖W1‖=29.226, ‖W2‖=37.740\n",
      "Trained W1: [[ 2.82407609e+00  1.78754769e-01  4.28510167e+00  3.43845145e-01\n",
      "   1.21955406e+00 -1.07609900e-02  4.75494530e+00  5.53929812e-02\n",
      "   8.96008075e-01  4.33953136e+00]\n",
      " [ 4.25075145e+00 -2.25572781e-01  5.72410170e+00  2.70550699e-03\n",
      "   5.88556084e-01  6.12214179e+00  7.90578982e+00  5.49248818e-01\n",
      "   2.78948477e-01  5.70645437e+00]\n",
      " [ 5.30279395e+00  6.45131646e-02 -6.17443000e+00  1.43087108e-01\n",
      "   5.61037564e-01  9.06232553e+00  2.23187035e-01  3.40438983e-01\n",
      "  -9.60888969e-02 -6.14108627e+00]\n",
      " [ 4.10796858e+00  4.59595569e-02 -9.35426055e+00  5.27593400e-02\n",
      "  -1.05381855e-01  8.99323806e+00 -4.46296947e+00 -3.08599885e-01\n",
      "   5.28710563e-01 -9.29690382e+00]\n",
      " [-2.53122589e+00  1.46794047e-02 -3.03427095e+00  1.98610927e-01\n",
      "   1.07569071e+00 -6.24820415e+00  7.45155066e+00  1.85178645e-01\n",
      "   8.09343750e-01 -3.02466221e+00]]\n",
      "Trained W2: [[ -2.258       -9.16954234  -6.62567737]\n",
      " [ -1.21756003   1.90895521  -0.56945369]\n",
      " [ -5.6508786  -16.49917155  -2.31084988]\n",
      " [  0.76358546   1.86036803  -0.07395268]\n",
      " [ -1.11183477   2.18367848  -0.57246955]\n",
      " [ -5.19504271  -8.3305199    8.14917874]\n",
      " [ 12.78229494 -10.58875541 -13.64129979]\n",
      " [  0.24031928   1.14850313  -0.25459988]\n",
      " [ -1.32281463   1.7332348   -0.48105188]\n",
      " [ -5.7361518  -16.30766885  -2.30744749]\n",
      " [  2.63375626   3.79316425  -3.09251059]]\n",
      "Hidden times for x0: [1.62  1.5   1.996 1.5   1.5   1.5   1.669 1.5   1.5   1.996]\n",
      "Hidden times for x1: [1.958 1.5   1.578 1.5   1.5   1.899 1.499 1.5   1.5   1.579]\n",
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.381 2.273 2.366]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.29  2.274 2.506]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.381 2.273 2.366]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.29  2.274 2.506]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.381 2.273 2.366]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.29  2.274 2.506]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.381 2.273 2.366]\n",
      " Predicted class: 0, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.29  2.274 2.506]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "weights saved\n"
     ]
    }
   ],
   "source": [
    "# #claude \"improvements\"\n",
    "\n",
    "# from brian2 import *\n",
    "# import numpy as np\n",
    "# import logging\n",
    "# import warnings\n",
    "\n",
    "# from brian2 import prefs, set_device\n",
    "\n",
    "# # Tell Brian2 to use the Cython code generator:\n",
    "# prefs.codegen.target = 'cython'\n",
    "\n",
    "# # Optionally compile but keep Python interface:\n",
    "# set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "# # suppress overflow warnings\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "# numpy.seterr(over='ignore', under='ignore')\n",
    "# logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Spike timing and derivative\n",
    "\n",
    "# start_scope()\n",
    "# defaultclock.dt = 0.001*ms\n",
    "\n",
    "# @implementation('numpy', discard_units=True)\n",
    "# @check_units(w=1, time=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "# def spike_timing(w, time):\n",
    "#     x = (time % 1)\n",
    "#     z = 5.0 * (x - 0.5)  # Smoother sigmoid\n",
    "#     sigmoid_val = 1.0 / (1.0 + np.exp(-w * z))\n",
    "#     return sigmoid_val\n",
    "\n",
    "# @implementation('numpy', discard_units=True)\n",
    "# @check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "# def d_spike_timing_dw(w, time):\n",
    "#     x = (time % 1)\n",
    "#     z = 5.0 * (x - 0.5)\n",
    "#     sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "#     return sig * (1.0 - sig) * z\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "# def layer_forward(inputs, W, layer_idx):\n",
    "#     \"\"\"\n",
    "#     inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "#     W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "#     layer_idx: integer layer number\n",
    "#     returns: array of output spike times (ms)\n",
    "#     \"\"\"\n",
    "#     # 1) augment inputs with bias spike @ t=0\n",
    "#     bias_time = 0.0\n",
    "#     aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "#     n_in_plus_bias, n_out = W.shape\n",
    "#     assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "#     out_times = []\n",
    "#     for j in range(n_out):\n",
    "#         start_scope()\n",
    "#         defaultclock.dt = 0.001*ms\n",
    "\n",
    "#         # single post‐synaptic neuron\n",
    "#         G = NeuronGroup(1, '''\n",
    "#             v : 1\n",
    "#             sum : 1\n",
    "#             sr : 1\n",
    "#             scheduled_time : second\n",
    "#             global_clock : 1\n",
    "#             spiked : boolean\n",
    "#         ''', threshold='v>1', reset='''\n",
    "#         v = 0\n",
    "#         spiked = True\n",
    "#         ''', method='exact')\n",
    "\n",
    "#         # init\n",
    "#         G.v = G.sum = G.sr = 0\n",
    "#         G.global_clock = 0\n",
    "#         G.scheduled_time = 1e9*second\n",
    "\n",
    "#         # stim: now includes bias spike at t=0\n",
    "#         stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "#                                    indices=list(range(n_in_plus_bias)),\n",
    "#                                    times=aug_inputs*ms)\n",
    "\n",
    "#         S = Synapses(stim, G, '''w:1\n",
    "#             layer:1''',\n",
    "#             on_pre='''\n",
    "#             sr += 1\n",
    "#             sum += spike_timing(w, global_clock)\n",
    "#             scheduled_time = (sum/sr + layer + 0.004)*ms\n",
    "#         ''')\n",
    "#         S.connect(True)\n",
    "#         S.w = W[:, j]\n",
    "#         S.layer = layer_idx\n",
    "\n",
    "#         G.run_regularly('''\n",
    "#             v = (1.0 - spiked) * int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "#             global_clock += 0.001\n",
    "#         ''', dt=0.001*ms)\n",
    "\n",
    "#         mon = SpikeMonitor(G)\n",
    "#         run(5*ms)\n",
    "\n",
    "#         ts = mon.spike_trains()[0]\n",
    "#         t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "#         out_times.append(t0)\n",
    "\n",
    "#     return np.array(out_times)\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "# # Training loop with backprop for 4-10-3\n",
    "# def train_snn_backprop(\n",
    "#     X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "#     W1_init, W2_init,\n",
    "#     epochs=10, lr=0.1,\n",
    "#     max_grad=50.0, w_min=-50.0, w_max=50.0,\n",
    "#     non_target_time=2.0,\n",
    "#     λ= 3               # non-target penalty weight\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Trains a 4→10→3 spiking network with FIXED backpropagation\n",
    "#     \"\"\"\n",
    "#     # Initialize weights\n",
    "#     W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "#     W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "#     # Momentum buffers\n",
    "#     beta = 0.9\n",
    "#     vW1 = np.zeros_like(W1)\n",
    "#     vW2 = np.zeros_like(W2)\n",
    "\n",
    "#     layer1_idx, layer2_idx = 1, 2\n",
    "#     N = len(X)\n",
    "\n",
    "#     for ep in range(epochs):\n",
    "#         # Accumulators for this epoch\n",
    "#         acc_dW1 = np.zeros_like(W1)\n",
    "#         acc_dW2 = np.zeros_like(W2)\n",
    "#         epoch_loss = 0.0\n",
    "\n",
    "#         for xi, yi in zip(X, Y):\n",
    "#             # — Forward pass —\n",
    "#             h_times = layer_forward(xi, W1, layer1_idx)\n",
    "#             o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "#             # — Separation loss —\n",
    "#             target_idx = np.argmax(yi)\n",
    "#             L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "#             non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "#             L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "#             L = L_target + L_non\n",
    "#             epoch_loss += L\n",
    "\n",
    "#             # — Gradients for W2 (output layer) —\n",
    "#             delta_o = np.zeros_like(o_times)\n",
    "#             delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "#             for j in non_ids:\n",
    "#                 delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "#             aug_h = np.concatenate((h_times, [0.0]))\n",
    "#             dW2 = np.zeros_like(W2)\n",
    "#             for k in range(W2.shape[0]):\n",
    "#                 for j in range(W2.shape[1]):\n",
    "#                     dW2[k, j] = delta_o[j] * d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "\n",
    "#             # — FIXED: Backprop into hidden layer —\n",
    "#             delta_h = np.zeros_like(h_times)\n",
    "#             for k in range(len(h_times)):\n",
    "#                 for j in range(W2.shape[1]):\n",
    "#                     # Chain rule: dL/dh_k = sum_j (dL/do_j * do_j/dh_k)\n",
    "#                     # where do_j/dh_k = W2[k,j] * d_spike_timing_dw(W2[k,j], aug_h[k])\n",
    "#                     dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "#                     delta_h[k] += delta_o[j] * W2[k, j] * dt_dw_output\n",
    "\n",
    "#             # — Gradients for W1 (hidden layer) —\n",
    "#             aug_xi = np.concatenate((xi, [0.0]))\n",
    "#             dW1 = np.zeros_like(W1)\n",
    "#             for i in range(W1.shape[0]):\n",
    "#                 for k in range(W1.shape[1]):\n",
    "#                     dW1[i, k] = delta_h[k] * d_spike_timing_dw(W1[i, k], aug_xi[i])\n",
    "\n",
    "#             # — Accumulate —\n",
    "#             acc_dW1 += dW1\n",
    "#             acc_dW2 += dW2\n",
    "\n",
    "#         # — Average gradients —\n",
    "#         acc_dW1 /= N\n",
    "#         acc_dW2 /= N\n",
    "\n",
    "#         # Print gradient norms for debugging\n",
    "#         if ep % 5 == 0:\n",
    "#             print(f\"  Gradient norms: ‖∇W1‖={np.linalg.norm(acc_dW1):.6f}, ‖∇W2‖={np.linalg.norm(acc_dW2):.6f}\")\n",
    "\n",
    "#         # Adaptive learning rates\n",
    "#         lr1 = 2.0 * lr  # Boost hidden layer learning\n",
    "#         lr2 = lr\n",
    "\n",
    "#         # Gradient clipping\n",
    "#         g1_norm = np.linalg.norm(acc_dW1)\n",
    "#         g2_norm = np.linalg.norm(acc_dW2)\n",
    "        \n",
    "#         if g1_norm > max_grad:\n",
    "#             acc_dW1 = acc_dW1 * (max_grad / g1_norm)\n",
    "#         if g2_norm > max_grad:\n",
    "#             acc_dW2 = acc_dW2 * (max_grad / g2_norm)\n",
    "\n",
    "#         # — Momentum updates —\n",
    "#         vW1 = beta * vW1 + (1 - beta) * acc_dW1\n",
    "#         vW2 = beta * vW2 + (1 - beta) * acc_dW2\n",
    "\n",
    "#         # — Apply weight updates & clamp —\n",
    "#         W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "#         W2 = np.clip(W2 - lr2 * vW2, w_min, w_max)\n",
    "\n",
    "#         print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "#         print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\")\n",
    "\n",
    "#     return W1, W2\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # example usage with fixed input/target pairs\n",
    "#     # 4 inputs per sample, constant across 8 samples\n",
    "#     x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "#     x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "#     X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "#     # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "#     y0 = np.array([2.95, 2.05, 2.05])\n",
    "#     y1 = np.array([2.05, 2.05, 2.95])\n",
    "#     Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "\n",
    "#     # Initialize weights with better scaling\n",
    "#     # W1_0 = np.random.randn(5, 10) * 0.5  # Larger initial weights\n",
    "#     # W2_0 = np.random.randn(11, 3) * 0.5\n",
    "\n",
    "#     # Or if you have saved weights:\n",
    "#     W1_0 = np.load(\"w1_2.npy\")\n",
    "#     W2_0 = np.load(\"w2_2.npy\")\n",
    "\n",
    "#     # train with higher learning rate\n",
    "#     W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "#                                       epochs=30, lr=2.0)  # Increased learning rate\n",
    "#     print(\"Trained W1:\", W1_tr)\n",
    "#     print(\"Trained W2:\", W2_tr) \n",
    "#     print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "#     print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "#     # ── Now test on the same two patterns ──\n",
    "#     print(\"\\n=== Test predictions ===\")\n",
    "#     for xi, yi in zip(X, Y):\n",
    "#         h_times = layer_forward(xi, W1_tr, 1)\n",
    "#         o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "#         pred_class = np.argmax(o_times)  \n",
    "#         true_class = np.argmax(yi)\n",
    "\n",
    "#         print(f\"Input: {xi}\")\n",
    "#         print(f\" Spike times: {o_times}\")\n",
    "#         print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "#     np.save(\"w1_2.npy\", W1_tr)\n",
    "#     np.save(\"w2_2.npy\", W2_tr)\n",
    "#     print(\"weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd4495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gradient norms: ‖∇W1‖=0.004634, ‖∇W2‖=0.019927\n",
      "Epoch 1/5 — avg loss=0.1629\n",
      "             ‖W1‖=35.164, ‖W2‖=45.110\n",
      "Epoch 2/5 — avg loss=0.1629\n",
      "             ‖W1‖=35.163, ‖W2‖=45.110\n",
      "Epoch 3/5 — avg loss=0.1629\n",
      "             ‖W1‖=35.163, ‖W2‖=45.110\n",
      "Epoch 4/5 — avg loss=0.1629\n",
      "             ‖W1‖=35.163, ‖W2‖=45.110\n",
      "Epoch 5/5 — avg loss=0.1630\n",
      "             ‖W1‖=35.163, ‖W2‖=45.110\n",
      "Trained W1: [[ 4.57249954e+00 -2.27474651e+00  4.15198485e+00 -4.87087578e+00\n",
      "  -3.64524969e+00  1.97076088e-03  4.67272605e+00  1.57225430e+00\n",
      "  -4.22569108e+00  4.18177578e+00]\n",
      " [ 4.86161887e+00 -2.52496754e+00  5.82313274e+00 -4.00703215e+00\n",
      "  -5.43309332e+00  6.14637823e+00  7.90542390e+00  1.50095109e+00\n",
      "  -5.34545971e+00  5.80566389e+00]\n",
      " [ 5.16844533e+00 -5.10321108e+00 -6.24602539e+00  2.13779753e+00\n",
      "   3.52692608e+00  9.06106855e+00 -1.06110591e+00 -3.38366579e+00\n",
      "   2.50201031e+00 -6.21518653e+00]\n",
      " [ 2.35068285e+00 -5.19106864e+00 -9.51061723e+00  1.86773635e+00\n",
      "  -1.26971504e+00  8.93491481e+00 -4.51528646e+00 -3.22086588e+00\n",
      "  -2.64782994e+00 -9.45775546e+00]\n",
      " [-2.73512870e+00  5.52773451e+00 -3.07298889e+00  7.86791544e+00\n",
      "   3.27411111e+00 -6.24820867e+00  7.45155068e+00 -1.37585544e+00\n",
      "   3.57309124e+00 -3.06369240e+00]]\n",
      "Trained W2: [[  6.73487919  -9.62633598  -8.71575322]\n",
      " [  8.19978936   4.73756771   9.85311162]\n",
      " [ -8.42690981 -16.5393955   -1.06650133]\n",
      " [  4.09677703   3.97473549   9.70737316]\n",
      " [  4.35751146   3.91502739  -3.113599  ]\n",
      " [ -6.31733004  -8.26251687   8.21400447]\n",
      " [ 12.61172941 -10.78965039 -13.60919272]\n",
      " [  2.6626167   -2.19078038  -1.44194424]\n",
      " [  8.0604935    4.47455059  -3.57956461]\n",
      " [ -8.44736748 -16.35024079  -1.06507278]\n",
      " [  2.40959898   3.83877131  -3.0239099 ]]\n",
      "Hidden times for x0: [1.617 1.4   1.999 1.03  1.155 1.5   1.776 1.981 1.187 1.999]\n",
      "Hidden times for x1: [1.998 1.019 1.597 1.401 1.206 1.9   1.406 1.556 1.202 1.597]\n",
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.316 2.044 2.321]\n",
      " Predicted class: 2, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.167 2.112 2.529]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.316 2.044 2.321]\n",
      " Predicted class: 2, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.167 2.112 2.529]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.316 2.044 2.321]\n",
      " Predicted class: 2, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.167 2.112 2.529]\n",
      " Predicted class: 2, True class: 2\n",
      "\n",
      "Input: [0.9 0.7 0.3 0.4]\n",
      " Spike times: [2.316 2.044 2.321]\n",
      " Predicted class: 2, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.167 2.112 2.529]\n",
      " Predicted class: 2, True class: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#claude \"improvements\"\n",
    "\n",
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, time=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, time):\n",
    "    x = (time % 1)\n",
    "    z = 10 * (x - 0.5)  # worked at 30 for some reason \n",
    "    sigmoid_val = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sigmoid_val\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, time):\n",
    "    x = (time % 1)\n",
    "    z = 5.0 * (x - 0.5)\n",
    "    sig = 1.0 / (1.0 + np.exp(-w * z))\n",
    "    return sig * (1.0 - sig) * z\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "            spiked : boolean\n",
    "        ''', threshold='v>1', reset='''\n",
    "        v = 0\n",
    "        spiked = True\n",
    "        ''', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock)\n",
    "            scheduled_time = (sum/sr + layer + 0.004)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = (1.0 - spiked) * int(abs(t - scheduled_time) < 0.005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=50.0, w_min=-50.0, w_max=50.0,\n",
    "    non_target_time=2.0,\n",
    "    λ= 0.5               # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with FIXED backpropagation\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators for this epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            # — Separation loss —\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # — Gradients for W2 (output layer) —\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "\n",
    "            # — FIXED: Backprop into hidden layer —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    # Chain rule: dL/dh_k = sum_j (dL/do_j * do_j/dh_k)\n",
    "                    # where do_j/dh_k = W2[k,j] * d_spike_timing_dw(W2[k,j], aug_h[k])\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k])\n",
    "                    delta_h[k] += delta_o[j] * W2[k, j] * dt_dw_output\n",
    "\n",
    "            # — Gradients for W1 (hidden layer) —\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(W1[i, k], aug_xi[i])\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Print gradient norms for debugging\n",
    "        if ep % 5 == 0:\n",
    "            print(f\"  Gradient norms: ‖∇W1‖={np.linalg.norm(acc_dW1):.6f}, ‖∇W2‖={np.linalg.norm(acc_dW2):.6f}\")\n",
    "\n",
    "        # Adaptive learning rates\n",
    "        lr1 = 2.0 * lr  # Boost hidden layer learning\n",
    "        lr2 = lr\n",
    "\n",
    "        # Gradient clipping\n",
    "        g1_norm = np.linalg.norm(acc_dW1)\n",
    "        g2_norm = np.linalg.norm(acc_dW2)\n",
    "        \n",
    "        if g1_norm > max_grad:\n",
    "            acc_dW1 = acc_dW1 * (max_grad / g1_norm)\n",
    "        if g2_norm > max_grad:\n",
    "            acc_dW2 = acc_dW2 * (max_grad / g2_norm)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * acc_dW1\n",
    "        vW2 = beta * vW2 + (1 - beta) * acc_dW2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr2 * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.05, 2.05])\n",
    "    y1 = np.array([2.05, 2.05, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "\n",
    "    # Initialize weights with better scaling\n",
    "    # W1_0 = np.random.randn(5, 10) * 0.5  # Larger initial weights\n",
    "    # W2_0 = np.random.randn(11, 3) * 0.5\n",
    "\n",
    "    # Or if you have saved weights:\n",
    "    W1_0 = np.load(\"w1_2.npy\")\n",
    "    W2_0 = np.load(\"w2_2.npy\")\n",
    "\n",
    "    # train with higher learning rate\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=5, lr=0.5)  # droping lr \n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # ── Now test on the same two patterns ──\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for xi, yi in zip(X, Y):\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  \n",
    "        true_class = np.argmax(yi)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "    # np.save(\"w1_2.npy\", W1_tr)\n",
    "    # np.save(\"w2_2.npy\", W2_tr)\n",
    "    # print(\"weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bcfa4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gradient norms: ‖∇W1‖=0.007660, ‖∇W2‖=0.019268\n",
      "Epoch 1/6 — avg loss=0.1249\n",
      "             ‖W1‖=11.459, ‖W2‖=14.367\n",
      "Epoch 2/6 — avg loss=0.1249\n",
      "             ‖W1‖=11.475, ‖W2‖=14.392\n",
      "Epoch 3/6 — avg loss=0.1249\n",
      "             ‖W1‖=11.497, ‖W2‖=14.429\n",
      "Epoch 4/6 — avg loss=0.1249\n",
      "             ‖W1‖=11.526, ‖W2‖=14.476\n",
      "Epoch 5/6 — avg loss=0.1251\n",
      "             ‖W1‖=11.561, ‖W2‖=14.532\n",
      "  Gradient norms: ‖∇W1‖=0.007735, ‖∇W2‖=0.015040\n",
      "Epoch 6/6 — avg loss=0.1254\n",
      "             ‖W1‖=11.602, ‖W2‖=14.597\n",
      "\n",
      "=== Iris SNN Test ===\n",
      "in=[0.03 0.4  0.05 0.04] → out=[2.61 2.49 2.16]  pred=0, true=0\n",
      "in=[0.47 0.4  0.63 0.67] → out=[2.13 2.55 2.62]  pred=2, true=2\n",
      "in=[0.16 0.16 0.37 0.36] → out=[2.5  2.5  2.21]  pred=0, true=1\n",
      "in=[0.18 0.12 0.37 0.36] → out=[2.5  2.5  2.21]  pred=0, true=1\n",
      "in=[0.03 0.48 0.05 0.04] → out=[2.65 2.47 2.14]  pred=0, true=0\n",
      "in=[0.53 0.51 0.6  0.59] → out=[2.2  2.48 2.61]  pred=2, true=1\n",
      "in=[0.08 0.63 0.   0.04] → out=[2.7  2.42 2.12]  pred=0, true=0\n",
      "in=[0.29 0.55 0.11 0.04] → out=[2.71 2.44 2.11]  pred=0, true=0\n",
      "in=[0.58 0.4  0.68 0.75] → out=[2.12 2.57 2.63]  pred=2, true=2\n",
      "in=[0.29 0.4  0.56 0.55] → out=[2.29 2.51 2.42]  pred=1, true=1\n",
      "in=[0.79 0.36 0.85 0.67] → out=[2.13 2.59 2.61]  pred=2, true=2\n",
      "in=[0.69 0.44 0.66 0.87] → out=[2.13 2.55 2.65]  pred=2, true=2\n",
      "in=[0.58 0.4  0.77 0.83] → out=[2.1  2.56 2.65]  pred=2, true=2\n",
      "in=[0.55 0.48 0.56 0.55] → out=[2.19 2.53 2.59]  pred=2, true=1\n",
      "in=[0.18 0.55 0.08 0.04] → out=[2.7  2.44 2.11]  pred=0, true=0\n",
      "in=[0.18 0.51 0.06 0.04] → out=[2.68 2.46 2.11]  pred=0, true=0\n",
      "in=[0.4  0.79 0.03 0.04] → out=[2.76 2.41 2.12]  pred=0, true=0\n",
      "in=[0.34 0.2  0.47 0.4 ] → out=[2.44 2.51 2.25]  pred=1, true=1\n",
      "in=[0.47 0.36 0.6  0.51] → out=[2.2  2.56 2.55]  pred=1, true=1\n",
      "in=[0.45 0.4  0.61 0.67] → out=[2.14 2.54 2.61]  pred=2, true=2\n",
      "in=[0.29 0.67 0.08 0.04] → out=[2.73 2.42 2.11]  pred=0, true=0\n",
      "in=[0.63 0.44 0.74 0.91] → out=[2.1  2.54 2.66]  pred=2, true=2\n",
      "in=[0.61 0.36 0.58 0.48] → out=[2.16 2.6  2.58]  pred=1, true=1\n",
      "in=[0.47 0.24 0.74 0.51] → out=[2.17 2.57 2.58]  pred=2, true=2\n",
      "in=[0.55 0.32 0.74 0.83] → out=[2.11 2.57 2.63]  pred=2, true=2\n",
      "in=[0.63 0.4  0.64 0.63] → out=[2.14 2.58 2.61]  pred=2, true=1\n",
      "in=[0.61 0.4  0.55 0.51] → out=[2.17 2.58 2.58]  pred=1, true=1\n",
      "in=[0.37 0.71 0.11 0.08] → out=[2.75 2.41 2.12]  pred=0, true=0\n",
      "in=[0.58 0.4  0.72 0.67] → out=[2.11 2.57 2.62]  pred=2, true=2\n",
      "in=[0.24 0.55 0.06 0.04] → out=[2.7  2.44 2.11]  pred=0, true=0\n",
      "\n",
      "Test Accuracy: 83.33%\n",
      "weights saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load Iris and normalize features to [0,1]\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "    # 2) Map each feature to a spike time in [1,5] ms\n",
    "    X_times = [0.95 * x for x in X]  # list of length 150, each a (4,) array\n",
    "    x_times = np.round(X_times, 3)  # round to 2 decimal places for clarity\n",
    "    \n",
    "\n",
    "    # 3) Build target spike-time lists: correct=2.05 ms, incorrect=2.95 ms\n",
    "    Y_times = []\n",
    "    for label in y:\n",
    "        t = np.full(3, 2.05)  # default target spike times\n",
    "        t[label] = 2.95\n",
    "        Y_times.append(t)\n",
    "    #print(Y_times)\n",
    "\n",
    "    # 4) 80/20 train/test split (keep stratification)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_times, Y_times,\n",
    "        test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    #print(X_train[:5], Y_train[:5])\n",
    "\n",
    "    # 5) Initialize weights (4 inputs + bias → 10 hidden; 10 hidden + bias → 3 output)\n",
    "    W1_0 = np.load(\"weights\\w1_iris.npy\")\n",
    "    W2_0 = np.load(\"weights\\w2_iris.npy\")\n",
    "\n",
    "    # # 6) Train SNN (use non_target_time=2.95 to match Y_times)\n",
    "    W1_tr, W2_tr = train_snn_backprop(\n",
    "        X_train, Y_train,\n",
    "        W1_0, W2_0,\n",
    "        epochs=6, lr=15, # 0.5 ideal\n",
    "    )\n",
    "\n",
    "    # # 7) Test on hold-out set\n",
    "    print(\"\\n=== Iris SNN Test ===\")\n",
    "    correct = 0\n",
    "    for xi, yi in zip(X_test, Y_test):\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "        \n",
    "        pred = np.argmax(o_times)  \n",
    "        true = np.argmax(yi)\n",
    "\n",
    "        correct += (pred == true)\n",
    "        print(f\"in={np.round(xi,2)} → out={np.round(o_times,2)}  pred={pred}, true={true}\")\n",
    "\n",
    "    acc = correct / len(X_test)\n",
    "    print(f\"\\nTest Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "np.save(\"weights\\w1_iris.npy\", W1_tr)\n",
    "np.save(\"weights\\w2_iris.npy\", W2_tr)\n",
    "print(\"weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce719551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights saved\n"
     ]
    }
   ],
   "source": [
    "np.save(\"weights\\w1_iris.npy\", W1_tr)\n",
    "np.save(\"weights\\w2_iris.npy\", W2_tr)\n",
    "print(\"weights saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919b9a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19a2de4a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf822681",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fee676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(5, 10)\n",
      "(3, 2)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 270\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(W2_0\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "Cell \u001b[1;32mIn[10], line 149\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n\u001b[0;32m    145\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y): \u001b[38;5;66;03m# iterates in pairs at same time x1 and y1 x2 & y2 ect.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# — Forward pass —\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m layer_forward(h_times, W2, layer2_idx)  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m aug_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((inputs, [bias_time]))  \u001b[38;5;66;03m# shape (n_in+1,)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m n_in_plus_bias, n_out \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m aug_inputs\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m n_in_plus_bias\n\u001b[0;32m     62\u001b[0m out_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_out):\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    return np.tanh(w * x)\n",
    "\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(G.scheduled_time[0] / ms)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # can come back to above to check ^^^^\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            # print(\"otimes \", o_times)\n",
    "            # print(\"delta_o \",delta_o) \n",
    "        \n",
    "            # otimes  [2.666 2.003 2.524]\n",
    "            # delta_o  [-0.284  -0.0235  0.237 ]\n",
    "\n",
    "            # otimes  [2.726 2.003 2.619]\n",
    "            # delta_o  [ 0.338  -0.0235 -0.331 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0])) # just adds a 0 to end of list\n",
    "            # print(\"h_times \", h_times)\n",
    "            # print(\"aug_h \", aug_h)\n",
    "\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):        # will need to check out spinking time_dw\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "            # print(\"w2 \", W2)\n",
    "            # print(\"dW2 \", dW2)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  \n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        #lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = W1 - lr * vW1  \n",
    "        W2 = W2 - lr * vW2\n",
    "\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"NN_W_1.npy\")\n",
    "    print(W1_0.shape)\n",
    "\n",
    "    test  = np.load(\"W1.npy\")\n",
    "    print(test.shape)\n",
    "\n",
    "    W2_0 = np.load(\"NN_W_2.npy\")\n",
    "    print(W2_0.shape)\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=10, lr=0.1)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for xi, yi in zip(X, Y):\n",
    "        # call layer_forward(positionally) rather than with layer1_idx=\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  \n",
    "        true_class = np.argmax(yi)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db93b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gradient Debug ===\n",
      "Hidden times: [1.45820835 1.58188232 1.53247909 1.42944979 1.4640665  1.49132949\n",
      " 1.42066056 1.4655997  1.35691895 1.37716041]\n",
      "Output times: [2.51084009 2.39558591 2.40642763]\n",
      "Output variance: 0.002700\n",
      "WARNING: Outputs are too similar - poor class separation\n",
      "W1 range: [-0.935, 0.742]\n",
      "W2 range: [-0.810, 0.702]\n",
      "Epoch 1/2 — avg loss=0.0207\n",
      "  LR=0.1900, ‖∇W1‖=1.0908, ‖∇W2‖=1.7358\n",
      "  ‖W1‖=2.543, ‖W2‖=2.117\n",
      "\n",
      "Epoch 2/2 — avg loss=0.0213\n",
      "  LR=0.1900, ‖∇W1‖=1.0809, ‖∇W2‖=1.7324\n",
      "  ‖W1‖=2.532, ‖W2‖=2.096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Key fixes to your SNN code:\n",
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "# 1. Fix the derivative function to handle edge cases better\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-6  # Increased epsilon\n",
    "    \n",
    "    # Add bounds checking to prevent extreme values\n",
    "    x = np.clip(x, eps, 1-eps)\n",
    "    \n",
    "    if w >= 0:\n",
    "        # For positive weights, add safeguards against log(0)\n",
    "        log_term = np.log(np.maximum(x, eps))\n",
    "        result = - np.power(x, (1 - w)) * log_term\n",
    "    else:\n",
    "        # For negative weights\n",
    "        log_term = np.log(np.maximum(1 - x, eps))\n",
    "        result = - np.power((1 - x), (1 + w)) * log_term\n",
    "    \n",
    "    # Clip gradients to prevent explosion\n",
    "    return np.clip(result, -10.0, 10.0)\n",
    "\n",
    "# 2. Improve the spike timing function for better gradients\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-6\n",
    "    x = np.clip(x, eps, 1-eps)  # Prevent saturation\n",
    "    \n",
    "    if w >= 0:\n",
    "        result = np.power(x, (1 - w))\n",
    "    else:\n",
    "        result = 1 - np.power((1 - x), (1 + w))\n",
    "    \n",
    "    return np.clip(result, eps, 1-eps)  # Prevent complete saturation\n",
    "\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(G.scheduled_time[0] / ms)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# 3. Modified training function with better hyperparameters\n",
    "def train_snn_backprop_fixed(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=50, lr=0.05,  # Reduced learning rate\n",
    "    max_grad=5.0,        # Reduced gradient clipping\n",
    "    w_min=-10.0, w_max=10.0,  # Reduced weight bounds\n",
    "    non_target_time=2.5,  # Increased separation\n",
    "    λ=1.0                # Increased penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Fixed version with better gradient handling and hyperparameters\n",
    "    \"\"\"\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "\n",
    "    # Adaptive learning rate\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "    \n",
    "    # Add learning rate decay\n",
    "    lr_decay = 0.95\n",
    "    current_lr = lr\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            # Forward pass\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            # Improved loss function with better class separation\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            \n",
    "            # Encourage non-target outputs to be far from target\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([\n",
    "                max(0, 0.2 - abs(o_times[j] - o_times[target_idx]))**2 \n",
    "                for j in non_ids\n",
    "            ])  # Margin loss for better separation\n",
    "            \n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # Gradients for W2\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            \n",
    "            for j in non_ids:\n",
    "                if abs(o_times[j] - o_times[target_idx]) < 0.2:\n",
    "                    delta_o[j] = λ * np.sign(o_times[j] - o_times[target_idx])\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            \n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    grad = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    dW2[k, j] = delta_o[j] * grad\n",
    "\n",
    "            # Backprop into hidden layer\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    grad = d_spike_timing_dw(W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "                    dW1[i, k] = delta_h[k] * grad\n",
    "\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # Average gradients\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Check for vanishing gradients\n",
    "        if np.mean(np.abs(acc_dW1)) < 1e-8 or np.mean(np.abs(acc_dW2)) < 1e-8:\n",
    "            print(f\"Warning: Vanishing gradients detected at epoch {ep+1}\")\n",
    "            current_lr *= 2  # Increase learning rate temporarily\n",
    "\n",
    "        # Gradient clipping\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "\n",
    "        # Momentum updates\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # Weight updates with bounds\n",
    "        W1 = np.clip(W1 - current_lr * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - current_lr * vW2, w_min, w_max)\n",
    "\n",
    "        # Learning rate decay\n",
    "        if ep % 10 == 0:\n",
    "            current_lr *= lr_decay\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"  LR={current_lr:.4f}, ‖∇W1‖={np.linalg.norm(acc_dW1):.4f}, ‖∇W2‖={np.linalg.norm(acc_dW2):.4f}\")\n",
    "        print(f\"  ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# 4. Better weight initialization\n",
    "def initialize_weights_better():\n",
    "    \"\"\"Initialize weights with better variance scaling\"\"\"\n",
    "    # Xavier/Glorot initialization scaled for this problem\n",
    "    W1_0 = np.random.randn(4+1, 10) * np.sqrt(2.0 / (4+10))\n",
    "    W2_0 = np.random.randn(10+1, 3) * np.sqrt(2.0 / (10+3))\n",
    "    return W1_0, W2_0\n",
    "\n",
    "# 5. Add debugging function\n",
    "def debug_gradients(W1, W2, X, Y):\n",
    "    \"\"\"Debug gradient flow\"\"\"\n",
    "    print(\"=== Gradient Debug ===\")\n",
    "    xi, yi = X[0], Y[0]\n",
    "    \n",
    "    h_times = layer_forward(xi, W1, 1)\n",
    "    o_times = layer_forward(h_times, W2, 2)\n",
    "    \n",
    "    print(f\"Hidden times: {h_times}\")\n",
    "    print(f\"Output times: {o_times}\")\n",
    "    \n",
    "    # Check if outputs are too similar\n",
    "    output_variance = np.var(o_times)\n",
    "    print(f\"Output variance: {output_variance:.6f}\")\n",
    "    \n",
    "    if output_variance < 0.01:\n",
    "        print(\"WARNING: Outputs are too similar - poor class separation\")\n",
    "    \n",
    "    # Check weight ranges\n",
    "    print(f\"W1 range: [{W1.min():.3f}, {W1.max():.3f}]\")\n",
    "    print(f\"W2 range: [{W2.min():.3f}, {W2.max():.3f}]\")\n",
    "\n",
    "# Usage with better initialization:\n",
    "if __name__ == \"__main__\":\n",
    "    # Your existing data\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    y0 = np.array([1.5, 2.0, 2.5])  # Changed targets for better separation\n",
    "    y1 = np.array([2.5, 2.0, 1.5])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Use better initialization instead of loaded weights\n",
    "    W1_0, W2_0 = initialize_weights_better()\n",
    "    \n",
    "    # Debug before training\n",
    "    debug_gradients(W1_0, W2_0, X, Y)\n",
    "    \n",
    "    # Train with fixed function\n",
    "    W1_tr, W2_tr = train_snn_backprop_fixed(X, Y, W1_0, W2_0, epochs=2, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cfb09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spike Timing Function Analysis ===\n",
      "Weight\tTime\tSpike_timing\tDerivative\n",
      "-2.0\t0.1\t0.0000\t\t0.1171\n",
      "-2.0\t0.3\t0.0000\t\t0.5095\n",
      "-2.0\t0.5\t0.0000\t\t1.3863\n",
      "-2.0\t0.7\t0.0000\t\t4.0132\n",
      "-2.0\t0.9\t0.0000\t\t10.0000\n",
      "-1.0\t0.1\t0.0000\t\t0.1054\n",
      "-1.0\t0.3\t0.0000\t\t0.3567\n",
      "-1.0\t0.5\t0.0000\t\t0.6931\n",
      "-1.0\t0.7\t0.0000\t\t1.2040\n",
      "-1.0\t0.9\t0.0000\t\t2.3026\n",
      "-0.5\t0.1\t0.0513\t\t0.1000\n",
      "-0.5\t0.3\t0.1633\t\t0.2984\n",
      "-0.5\t0.5\t0.2929\t\t0.4901\n",
      "-0.5\t0.7\t0.4523\t\t0.6594\n",
      "-0.5\t0.9\t0.6838\t\t0.7281\n",
      "0.0\t0.1\t0.1000\t\t0.2303\n",
      "0.0\t0.3\t0.3000\t\t0.3612\n",
      "0.0\t0.5\t0.5000\t\t0.3466\n",
      "0.0\t0.7\t0.7000\t\t0.2497\n",
      "0.0\t0.9\t0.9000\t\t0.0948\n",
      "0.5\t0.1\t0.3162\t\t0.7281\n",
      "0.5\t0.3\t0.5477\t\t0.6594\n",
      "0.5\t0.5\t0.7071\t\t0.4901\n",
      "0.5\t0.7\t0.8367\t\t0.2984\n",
      "0.5\t0.9\t0.9487\t\t0.1000\n",
      "1.0\t0.1\t1.0000\t\t2.3026\n",
      "1.0\t0.3\t1.0000\t\t1.2040\n",
      "1.0\t0.5\t1.0000\t\t0.6931\n",
      "1.0\t0.7\t1.0000\t\t0.3567\n",
      "1.0\t0.9\t1.0000\t\t0.1054\n",
      "2.0\t0.1\t1.0000\t\t10.0000\n",
      "2.0\t0.3\t1.0000\t\t4.0132\n",
      "2.0\t0.5\t1.0000\t\t1.3863\n",
      "2.0\t0.7\t1.0000\t\t0.5095\n",
      "2.0\t0.9\t1.0000\t\t0.1171\n",
      "=== Debug Timing Function Inputs ===\n",
      "Hidden layer spike times: [1.43008574 1.43948677 1.51182876 1.46719479 1.43742272 1.47630132\n",
      " 1.42447831 1.42702498 1.43493524 1.51686609]\n",
      "Hidden time range: [1.424, 1.517]\n",
      "Augmented hidden times: [1.43008574 1.43948677 1.51182876 1.46719479 1.43742272 1.47630132\n",
      " 1.42447831 1.42702498 1.43493524 1.51686609 0.        ]\n",
      "For layer 2, global_clock % 1 values will be: [0.43008574 0.43948677 0.51182876 0.46719479 0.43742272 0.47630132\n",
      " 0.42447831 0.42702498 0.43493524 0.51686609 0.        ]\n",
      "=== Weight Update Analysis ===\n",
      "\n",
      "Sample 0: input=[0.9 0.7 0.3 0.4], target=[2.95 2.   2.  ]\n",
      "  Hidden times: [1.43008574 1.43948677 1.51182876 1.46719479 1.43742272 1.47630132\n",
      " 1.42447831 1.42702498 1.43493524 1.51686609]\n",
      "  Output times: [2.68163337 2.0310563  2.54159021]\n",
      "  Hidden variance: 0.001092\n",
      "  Output variance: 0.078167\n",
      "\n",
      "Sample 1: input=[0.6 0.7 0.8 0.9], target=[2.   2.   2.95]\n",
      "  Hidden times: [1.55603581 1.57524016 1.60839374 1.57913611 1.56056221 1.60274556\n",
      " 1.54266361 1.56410982 1.56720231 1.61015288]\n",
      "  Output times: [2.73485722 2.04321863 2.63214298]\n",
      "  Hidden variance: 0.000490\n",
      "  Output variance: 0.092861\n",
      "  WARNING: Hidden layer outputs are too similar!\n"
     ]
    }
   ],
   "source": [
    "# Analysis of the real issues in your SNN code while preserving layer timing\n",
    "\n",
    "# 1. The derivative function has numerical issues\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw_fixed(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-7  # Small epsilon to prevent log(0)\n",
    "    \n",
    "    # The issue: x can be very close to 0 or 1, making log(x) explode\n",
    "    # Your logic is correct, but needs numerical stability\n",
    "    \n",
    "    if w >= 0:\n",
    "        # Prevent log(0) by ensuring x > eps\n",
    "        x_safe = np.maximum(x, eps)\n",
    "        result = - np.power(x_safe, (1 - w)) * np.log(x_safe)\n",
    "    else:\n",
    "        # Prevent log(0) by ensuring (1-x) > eps  \n",
    "        x_safe = np.minimum(x, 1 - eps)\n",
    "        result = - np.power((1 - x_safe), (1 + w)) * np.log(1 - x_safe)\n",
    "    \n",
    "    # Clip extreme gradients but preserve sign and magnitude relationships\n",
    "    return np.clip(result, -50.0, 50.0)\n",
    "\n",
    "# 2. The timing function also needs numerical stability\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing_fixed(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-7\n",
    "    \n",
    "    if w >= 0:\n",
    "        # Prevent x=0 case\n",
    "        x_safe = np.maximum(x, eps)\n",
    "        result = np.power(x_safe, (1 - w))\n",
    "    else:\n",
    "        # Prevent x=1 case\n",
    "        x_safe = np.minimum(x, 1 - eps)\n",
    "        result = 1 - np.power((1 - x_safe), (1 + w))\n",
    "    \n",
    "    return np.clip(result, 0.0, 1.0)\n",
    "\n",
    "# 3. The main issue: Check what values are being passed to these functions\n",
    "def debug_timing_inputs(W1, W2, X):\n",
    "    \"\"\"Debug what actual values are being passed to timing functions\"\"\"\n",
    "    print(\"=== Debug Timing Function Inputs ===\")\n",
    "    \n",
    "    xi = X[0]  # First input\n",
    "    \n",
    "    # Forward pass through layer 1\n",
    "    h_times = layer_forward(xi, W1, 1)\n",
    "    print(f\"Hidden layer spike times: {h_times}\")\n",
    "    print(f\"Hidden time range: [{h_times.min():.3f}, {h_times.max():.3f}]\")\n",
    "    \n",
    "    # What gets passed to layer 2?\n",
    "    aug_h = np.concatenate((h_times, [0.0]))  # bias at t=0\n",
    "    print(f\"Augmented hidden times: {aug_h}\")\n",
    "    \n",
    "    # Check what global_clock values are used\n",
    "    print(f\"For layer 2, global_clock % 1 values will be: {aug_h % 1}\")\n",
    "    \n",
    "    # These are the values that go into your timing functions!\n",
    "    # If they're all very close to 0 or 1, gradients will vanish\n",
    "    \n",
    "    return h_times, aug_h\n",
    "\n",
    "# 4. The real issue might be in your layer_forward function\n",
    "def layer_forward_with_debug(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    Debug version of layer_forward to see what's happening\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Layer {layer_idx} Forward Pass ---\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    \n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))\n",
    "    print(f\"Augmented inputs: {aug_inputs}\")\n",
    "    \n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    out_times = []\n",
    "    \n",
    "    for j in range(n_out):\n",
    "        # Your simulation setup\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "        \n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "        \n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "        \n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "        \n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "        \n",
    "        # The issue might be here - what values is global_clock taking?\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "        \n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "        \n",
    "        # Debug the actual computation\n",
    "        final_sum = G.sum[0]\n",
    "        final_sr = G.sr[0]\n",
    "        scheduled_t = float(G.scheduled_time[0] / ms)\n",
    "        \n",
    "        print(f\"  Neuron {j}: sum={final_sum:.4f}, sr={final_sr}, scheduled_time={scheduled_t:.4f}\")\n",
    "        \n",
    "        # The real issue might be that sum/sr is always similar values\n",
    "        # This would make all outputs similar regardless of weights\n",
    "        \n",
    "        out_times.append(scheduled_t)\n",
    "    \n",
    "    print(f\"Layer {layer_idx} outputs: {out_times}\")\n",
    "    return np.array(out_times)\n",
    "\n",
    "# 5. Check if the issue is in the weight update order\n",
    "def analyze_weight_updates(X, Y, W1, W2):\n",
    "    \"\"\"\n",
    "    Check if weight updates are consistent across samples\n",
    "    \"\"\"\n",
    "    print(\"=== Weight Update Analysis ===\")\n",
    "    \n",
    "    # Check gradients for first two samples\n",
    "    for i, (xi, yi) in enumerate(zip(X[:2], Y[:2])):\n",
    "        print(f\"\\nSample {i}: input={xi}, target={yi}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        h_times = layer_forward(xi, W1, 1)\n",
    "        o_times = layer_forward(h_times, W2, 2)\n",
    "        \n",
    "        print(f\"  Hidden times: {h_times}\")\n",
    "        print(f\"  Output times: {o_times}\")\n",
    "        \n",
    "        # Check if hidden times are too similar\n",
    "        h_variance = np.var(h_times)\n",
    "        o_variance = np.var(o_times)\n",
    "        \n",
    "        print(f\"  Hidden variance: {h_variance:.6f}\")\n",
    "        print(f\"  Output variance: {o_variance:.6f}\")\n",
    "        \n",
    "        if h_variance < 0.001:\n",
    "            print(\"  WARNING: Hidden layer outputs are too similar!\")\n",
    "        if o_variance < 0.001:\n",
    "            print(\"  WARNING: Output layer outputs are too similar!\")\n",
    "\n",
    "# 6. The corrected training function (keeping your timing structure)\n",
    "def train_snn_backprop_corrected(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=50, lr=0.05,  # Reduced learning rate\n",
    "    max_grad=10.0,\n",
    "    w_min=-10.0, w_max=10.0,\n",
    "    non_target_time=2.05,  # Keep your original range\n",
    "    λ=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Corrected version that preserves your layer timing structure\n",
    "    \"\"\"\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    \n",
    "    # Add momentum\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "    \n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for xi, yi in zip(X, Y):\n",
    "            # Forward pass (unchanged)\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "            \n",
    "            # Loss calculation (unchanged - your structure is correct)\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "            \n",
    "            # Gradient computation (your logic is correct, just need numerical stability)\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "            \n",
    "            # Rest of gradient computation is correct...\n",
    "            # The issue is in the timing functions, not the structure\n",
    "            \n",
    "            # [Include your original gradient computation here with fixed timing functions]\n",
    "            \n",
    "        # Check for vanishing gradients\n",
    "        grad_norm_1 = np.linalg.norm(acc_dW1)\n",
    "        grad_norm_2 = np.linalg.norm(acc_dW2)\n",
    "        \n",
    "        if grad_norm_1 < 1e-10 or grad_norm_2 < 1e-10:\n",
    "            print(f\"  WARNING: Very small gradients - norm1={grad_norm_1:.2e}, norm2={grad_norm_2:.2e}\")\n",
    "        \n",
    "        # [Rest of your training loop...]\n",
    "        \n",
    "        print(f\"Epoch {ep+1}: loss={epoch_loss/N:.4f}, grad_norms=({grad_norm_1:.4f}, {grad_norm_2:.4f})\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# 7. Key insight: Your problem might be here\n",
    "def check_spike_timing_behavior():\n",
    "    \"\"\"\n",
    "    Check if your spike_timing function is producing reasonable gradients\n",
    "    \"\"\"\n",
    "    print(\"=== Spike Timing Function Analysis ===\")\n",
    "    \n",
    "    # Test with typical values that would occur in your network\n",
    "    test_weights = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0])\n",
    "    test_times = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # global_clock % 1 values\n",
    "    \n",
    "    print(\"Weight\\tTime\\tSpike_timing\\tDerivative\")\n",
    "    for w in test_weights:\n",
    "        for t in test_times:\n",
    "            st = spike_timing(w, t, 1, 0, 1)\n",
    "            dt = d_spike_timing_dw(w, t, 1, 0, 1)\n",
    "            print(f\"{w:.1f}\\t{t:.1f}\\t{st:.4f}\\t\\t{dt:.4f}\")\n",
    "    \n",
    "    # Look for patterns where derivatives are always very small\n",
    "\n",
    "# Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Your original data (this is correct)\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    # Your original targets (this is correct for your architecture)\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Load your existing weights\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "    \n",
    "    # Run diagnostics\n",
    "    check_spike_timing_behavior()\n",
    "    debug_timing_inputs(W1_0, W2_0, X)\n",
    "    analyze_weight_updates(X, Y, W1_0, W2_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hidden Layer Collapse Analysis ===\n",
      "Input times: [0.9 0.7 0.3 0.4 0. ]\n",
      "W1 first few columns:\n",
      "[[-2.24802890e-01 -5.02760943e-05 -3.02786849e-02]\n",
      " [-1.11390344e-01 -1.80939746e-01 -1.34243925e-01]\n",
      " [-2.41631112e-01 -1.56714608e-01  3.69612932e-01]\n",
      " [ 5.13877625e-02  1.55790838e-02  3.39575082e-01]\n",
      " [-1.31178265e-01  6.46637541e-03 -9.75420633e-02]]\n",
      "\n",
      "Hidden neuron analysis:\n",
      "\n",
      "Hidden neuron 0:\n",
      "  Weights: [-0.22480289 -0.11139034 -0.24163111  0.05138776 -0.13117826]\n",
      "    Input 0: time=0.900, w=-0.225, spike_timing=0.8322\n",
      "    Input 1: time=0.700, w=-0.111, spike_timing=0.6569\n",
      "    Input 2: time=0.300, w=-0.242, spike_timing=0.2370\n",
      "    Input 3: time=0.400, w=0.051, spike_timing=0.4193\n",
      "    Input 4: time=0.000, w=-0.131, spike_timing=0.0000\n",
      "  Total sum: 2.1454, sum/sr: 0.4291\n",
      "  Scheduled time: 1.4291\n",
      "\n",
      "Hidden neuron 1:\n",
      "  Weights: [-5.02760943e-05 -1.80939746e-01 -1.56714608e-01  1.55790838e-02\n",
      "  6.46637541e-03]\n",
      "    Input 0: time=0.900, w=-0.000, spike_timing=0.9000\n",
      "    Input 1: time=0.700, w=-0.181, spike_timing=0.6270\n",
      "    Input 2: time=0.300, w=-0.157, spike_timing=0.2598\n",
      "    Input 3: time=0.400, w=0.016, spike_timing=0.4058\n",
      "    Input 4: time=0.000, w=0.006, spike_timing=0.0000\n",
      "  Total sum: 2.1925, sum/sr: 0.4385\n",
      "  Scheduled time: 1.4385\n",
      "\n",
      "Hidden neuron 2:\n",
      "  Weights: [-0.03027868 -0.13424393  0.36961293  0.33957508 -0.09754206]\n",
      "    Input 0: time=0.900, w=-0.030, spike_timing=0.8928\n",
      "    Input 1: time=0.700, w=-0.134, spike_timing=0.6474\n",
      "    Input 2: time=0.300, w=0.370, spike_timing=0.4681\n",
      "    Input 3: time=0.400, w=0.340, spike_timing=0.5460\n",
      "    Input 4: time=0.000, w=-0.098, spike_timing=0.0000\n",
      "  Total sum: 2.5543, sum/sr: 0.5109\n",
      "  Scheduled time: 1.5109\n",
      "=== Solution: Diverse Initialization + Diversity Loss ===\n",
      "Initial hidden diversity:\n",
      "  Sample 0 variance: 0.034311\n",
      "  Sample 1 variance: 0.039116\n",
      "  Sample 0 range: [1.116, 1.726]\n",
      "  Sample 1 range: [1.109, 1.795]\n",
      "✓ Good initial diversity!\n",
      "Epoch 1: loss=0.1997\n",
      "  Hidden variance: 0.034343 (sample 0), 0.039143 (sample 1)\n",
      "  Hidden range: [1.116, 1.727]\n",
      "Epoch 6: loss=0.1981\n",
      "  Hidden variance: 0.034917 (sample 0), 0.039640 (sample 1)\n",
      "  Hidden range: [1.115, 1.736]\n",
      "Epoch 11: loss=0.1947\n",
      "  Hidden variance: 0.036218 (sample 0), 0.040795 (sample 1)\n",
      "  Hidden range: [1.112, 1.756]\n",
      "Epoch 16: loss=0.1905\n",
      "  Hidden variance: 0.038345 (sample 0), 0.042712 (sample 1)\n",
      "  Hidden range: [1.108, 1.787]\n",
      "Epoch 21: loss=0.1861\n",
      "  Hidden variance: 0.040601 (sample 0), 0.044736 (sample 1)\n",
      "  Hidden range: [1.104, 1.816]\n",
      "Epoch 26: loss=0.1820\n",
      "  Hidden variance: 0.040554 (sample 0), 0.044381 (sample 1)\n",
      "  Hidden range: [1.099, 1.815]\n",
      "\n",
      "=== Final Test ===\n",
      "Sample 0: pred=0, true=0\n",
      "  Hidden variance: 0.038600\n",
      "  Output times: [2.47545645 2.33920008 2.46592541]\n",
      "Sample 1: pred=0, true=2\n",
      "  Hidden variance: 0.041874\n",
      "  Output times: [2.5425644  2.41112667 2.53502445]\n"
     ]
    }
   ],
   "source": [
    "# Analysis of Hidden Layer Collapse in Your SNN\n",
    "\n",
    "# The Problem: Why all hidden neurons spike at similar times\n",
    "\"\"\"\n",
    "In your layer_forward function:\n",
    "\n",
    "scheduled_time = (sum/sr + layer)*ms\n",
    "\n",
    "For layer 1:\n",
    "- sum/sr is computed from: sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "- If all weights are small and similar, sum/sr will be similar for all neurons\n",
    "- Result: scheduled_time ≈ (similar_value + 1)*ms for all hidden neurons\n",
    "\n",
    "This creates a \"hidden layer collapse\" where all neurons do the same thing.\n",
    "\"\"\"\n",
    "\n",
    "# Let's trace through what happens:\n",
    "def analyze_hidden_collapse():\n",
    "    \"\"\"\n",
    "    Analyze why all hidden neurons produce similar spike times\n",
    "    \"\"\"\n",
    "    print(\"=== Hidden Layer Collapse Analysis ===\")\n",
    "    \n",
    "    # Your input and typical weights\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    bias = 0.0\n",
    "    aug_x0 = np.concatenate([x0, [bias]])\n",
    "    \n",
    "    print(f\"Input times: {aug_x0}\")\n",
    "    \n",
    "    # Let's see what happens with your actual weights\n",
    "    W1 = np.load(\"W1.npy\")\n",
    "    print(f\"W1 first few columns:\\n{W1[:, :3]}\")\n",
    "    \n",
    "    # For each hidden neuron, calculate what sum/sr would be\n",
    "    print(\"\\nHidden neuron analysis:\")\n",
    "    for j in range(3):  # First 3 hidden neurons\n",
    "        weights = W1[:, j]\n",
    "        print(f\"\\nHidden neuron {j}:\")\n",
    "        print(f\"  Weights: {weights}\")\n",
    "        \n",
    "        # Simulate the sum computation\n",
    "        total_sum = 0\n",
    "        for i, (input_time, weight) in enumerate(zip(aug_x0, weights)):\n",
    "            # What spike_timing returns for this input\n",
    "            st_value = spike_timing(weight, input_time, 1, 0, 1)\n",
    "            print(f\"    Input {i}: time={input_time:.3f}, w={weight:.3f}, spike_timing={st_value:.4f}\")\n",
    "            total_sum += st_value\n",
    "        \n",
    "        # Final scheduled time\n",
    "        sr = len(weights)  # Number of synapses\n",
    "        scheduled_time = (total_sum/sr + 1)  # +1 for layer\n",
    "        print(f\"  Total sum: {total_sum:.4f}, sum/sr: {total_sum/sr:.4f}\")\n",
    "        print(f\"  Scheduled time: {scheduled_time:.4f}\")\n",
    "\n",
    "# Solutions to fix hidden layer collapse:\n",
    "\n",
    "# Solution 1: Better weight initialization with more diversity\n",
    "def initialize_diverse_weights():\n",
    "    \"\"\"\n",
    "    Initialize weights to encourage diversity in hidden layer\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Create weights with intentional diversity\n",
    "    W1 = np.random.randn(5, 10) * 0.5  # Larger initial variance\n",
    "    \n",
    "    # Add some structure to encourage different behaviors\n",
    "    # Some neurons prefer early spikes, others late spikes\n",
    "    for j in range(10):\n",
    "        if j < 3:  # Early spike neurons\n",
    "            W1[:, j] += 0.5  # Positive bias\n",
    "        elif j >= 7:  # Late spike neurons  \n",
    "            W1[:, j] -= 0.5  # Negative bias\n",
    "        # Middle neurons stay random\n",
    "    \n",
    "    W2 = np.random.randn(11, 3) * 0.3\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Solution 2: Add explicit diversity regularization\n",
    "def train_with_diversity_regularization(X, Y, W1_init, W2_init, epochs=50):\n",
    "    \"\"\"\n",
    "    Training with explicit diversity loss to prevent collapse\n",
    "    \"\"\"\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    \n",
    "    # Momentum\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "    \n",
    "    lr = 0.1\n",
    "    diversity_weight = 0.1  # Weight for diversity loss\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        \n",
    "        for xi, yi in zip(X, Y):\n",
    "            # Forward pass\n",
    "            h_times = layer_forward(xi, W1, 1)\n",
    "            o_times = layer_forward(h_times, W2, 2)\n",
    "            \n",
    "            # Regular classification loss\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * 0.5 * sum([(o_times[j] - 2.05)**2 for j in non_ids])\n",
    "            \n",
    "            # DIVERSITY LOSS: Encourage hidden neurons to have different spike times\n",
    "            h_variance = np.var(h_times)\n",
    "            target_variance = 0.01  # We want at least this much variance\n",
    "            L_diversity = diversity_weight * max(0, target_variance - h_variance)**2\n",
    "            \n",
    "            L = L_target + L_non + L_diversity\n",
    "            epoch_loss += L\n",
    "            \n",
    "            # Regular gradients for classification\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = 0.5 * (o_times[j] - 2.05)\n",
    "            \n",
    "            # Diversity gradients for hidden layer\n",
    "            if h_variance < target_variance:\n",
    "                # Encourage diversity by pushing hidden times away from mean\n",
    "                h_mean = np.mean(h_times)\n",
    "                delta_h_diversity = 2 * diversity_weight * (target_variance - h_variance) * (h_times - h_mean) / len(h_times)\n",
    "            else:\n",
    "                delta_h_diversity = np.zeros_like(h_times)\n",
    "            \n",
    "            # Backprop through W2 (your original code)\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(W2[k, j], aug_h[k], 2, 0, 1)\n",
    "            \n",
    "            # Backprop into hidden layer (classification + diversity)\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                # Classification gradient\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], 2, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output\n",
    "                \n",
    "                # Add diversity gradient\n",
    "                delta_h[k] += delta_h_diversity[k]\n",
    "            \n",
    "            # Gradients for W1\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(W1[i, k], aug_xi[i], 1, 0, 1)\n",
    "            \n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "        \n",
    "        # Update weights\n",
    "        acc_dW1 /= len(X)\n",
    "        acc_dW2 /= len(X)\n",
    "        \n",
    "        # Clip gradients\n",
    "        acc_dW1 = np.clip(acc_dW1, -10, 10)\n",
    "        acc_dW2 = np.clip(acc_dW2, -10, 10)\n",
    "        \n",
    "        # Momentum update\n",
    "        vW1 = beta * vW1 + (1 - beta) * acc_dW1\n",
    "        vW2 = beta * vW2 + (1 - beta) * acc_dW2\n",
    "        \n",
    "        # Apply updates\n",
    "        W1 = np.clip(W1 - lr * vW1, -10, 10)\n",
    "        W2 = np.clip(W2 - lr * vW2, -10, 10)\n",
    "        \n",
    "        # Monitor progress\n",
    "        if ep % 5 == 0:\n",
    "            # Check current diversity\n",
    "            h_times_0 = layer_forward(X[0], W1, 1)\n",
    "            h_times_1 = layer_forward(X[1], W1, 1)\n",
    "            \n",
    "            print(f\"Epoch {ep+1}: loss={epoch_loss/len(X):.4f}\")\n",
    "            print(f\"  Hidden variance: {np.var(h_times_0):.6f} (sample 0), {np.var(h_times_1):.6f} (sample 1)\")\n",
    "            print(f\"  Hidden range: [{np.min(h_times_0):.3f}, {np.max(h_times_0):.3f}]\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# Solution 3: Architectural fix - Add noise or different activation patterns\n",
    "def layer_forward_with_noise(inputs, W, layer_idx, noise_std=0.01):\n",
    "    \"\"\"\n",
    "    Add small amount of noise to break symmetry\n",
    "    \"\"\"\n",
    "    # Your original layer_forward code here, but add:\n",
    "    # scheduled_time = (sum/sr + layer)*ms + noise\n",
    "    \n",
    "    # This is a band-aid solution - better to fix the root cause\n",
    "    pass\n",
    "\n",
    "# Solution 4: Different initialization strategy\n",
    "def initialize_with_opposing_weights():\n",
    "    \"\"\"\n",
    "    Initialize some weights to be strongly positive, others strongly negative\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(5, 10) * 0.3\n",
    "    \n",
    "    # Make some neurons strongly favor early inputs\n",
    "    W1[:2, :5] += 1.0  # First 2 inputs get positive boost for first 5 neurons\n",
    "    \n",
    "    # Make some neurons strongly favor late inputs  \n",
    "    W1[2:4, 5:] += 1.0  # Last 2 inputs get positive boost for last 5 neurons\n",
    "    \n",
    "    W2 = np.random.randn(11, 3) * 0.3\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# The key insight: Your architecture is sound, but you need initial weight diversity\n",
    "def main_solution():\n",
    "    \"\"\"\n",
    "    The main solution: Start with diverse weights and add diversity regularization\n",
    "    \"\"\"\n",
    "    print(\"=== Solution: Diverse Initialization + Diversity Loss ===\")\n",
    "    \n",
    "    # Your data\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Use diverse initialization instead of collapsed weights\n",
    "    W1_init, W2_init = initialize_diverse_weights()\n",
    "    \n",
    "    # Check initial diversity\n",
    "    h_times_0 = layer_forward(X[0], W1_init, 1)\n",
    "    h_times_1 = layer_forward(X[1], W1_init, 1)\n",
    "    \n",
    "    print(f\"Initial hidden diversity:\")\n",
    "    print(f\"  Sample 0 variance: {np.var(h_times_0):.6f}\")\n",
    "    print(f\"  Sample 1 variance: {np.var(h_times_1):.6f}\")\n",
    "    print(f\"  Sample 0 range: [{np.min(h_times_0):.3f}, {np.max(h_times_0):.3f}]\")\n",
    "    print(f\"  Sample 1 range: [{np.min(h_times_1):.3f}, {np.max(h_times_1):.3f}]\")\n",
    "    \n",
    "    if np.var(h_times_0) > 0.005 and np.var(h_times_1) > 0.005:\n",
    "        print(\"✓ Good initial diversity!\")\n",
    "        \n",
    "        # Train with diversity regularization\n",
    "        W1_final, W2_final = train_with_diversity_regularization(X, Y, W1_init, W2_init, epochs=30)\n",
    "        \n",
    "        # Test final performance\n",
    "        print(\"\\n=== Final Test ===\")\n",
    "        for i, (xi, yi) in enumerate(zip(X[:2], Y[:2])):\n",
    "            h_times = layer_forward(xi, W1_final, 1)\n",
    "            o_times = layer_forward(h_times, W2_final, 2)\n",
    "            \n",
    "            pred_class = np.argmax(o_times)\n",
    "            true_class = np.argmax(yi)\n",
    "            \n",
    "            print(f\"Sample {i}: pred={pred_class}, true={true_class}\")\n",
    "            print(f\"  Hidden variance: {np.var(h_times):.6f}\")\n",
    "            print(f\"  Output times: {o_times}\")\n",
    "    else:\n",
    "        print(\"✗ Still collapsed - need different initialization\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    analyze_hidden_collapse()\n",
    "    \n",
    "    # Try the solution\n",
    "    main_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9897b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa435ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method InstanceTrackerSet.remove of InstanceTrackerSet({<weakref at 0x0000029EE47F2480; to 'CythonCodeObject' at 0x0000029EE46FB050>, <weakref at 0x0000029EE47F3D30; to 'CythonCodeObject' at 0x0000029EE47F6050>, <weakref at 0x0000029EE437CDB0; dead>, <weakref at 0x0000029EE47E8680; to 'Clock' at 0x0000029EE4532090>, <weakref at 0x0000029EE437C860; dead>, <weakref at 0x0000029EE47E8AE0; to 'CodeRunner' at 0x0000029EE47F6110>, <weakref at 0x0000029EE4618590; to 'Synapses' at 0x0000029EE4614110>, <weakref at 0x0000029EE4558310; to 'CythonCodeObject' at 0x0000029EE48D7150>, <weakref at 0x0000029EE47EBC90; to 'CythonCodeObject' at 0x0000029EE4670190>, <weakref at 0x0000029EE47F2570; to 'Resetter' at 0x0000029EE3CF9250>, <weakref at 0x0000029EE437D800; dead>, <weakref at 0x0000029EE47F25C0; to 'StateUpdater' at 0x0000029EE46F72D0>, <weakref at 0x0000029EE48054E0; to 'SynapticPathway' at 0x0000029EE47E2310>, <weakref at 0x0000029EE437C720; to 'Clock' at 0x0000029EE45423D0>, <weakref at 0x0000029EE3C93B50; to 'CodeRunner' at 0x0000029EE470E3D0>, <weakref at 0x0000029EE47EB740; dead>, <weakref at 0x0000029EE1769B70; dead>, <weakref at 0x0000029EE47F0A90; to 'SpikeGeneratorGroup' at 0x0000029EE46F4610>, <weakref at 0x0000029EE4807100; to 'Clock' at 0x0000029EE46F0690>, <weakref at 0x0000029EE3D90E00; dead>, <weakref at 0x0000029EE4295F30; to 'SynapticPathway' at 0x0000029EE3CB0750>, <weakref at 0x0000029EE461A890; to 'SpikeGeneratorGroup' at 0x0000029EE45E3790>, <weakref at 0x0000029EE3D93EC0; to 'Thresholder' at 0x0000029EE427D790>, <weakref at 0x0000029EE437C130; dead>, <weakref at 0x0000029EE4806840; to 'Synapses' at 0x0000029EE46F2810>, <weakref at 0x0000029EE42B3F60; to 'CythonCodeObject' at 0x0000029EE46FA950>, <weakref at 0x0000029EE47F0C70; to 'NeuronGroup' at 0x0000029EE3660990>, <weakref at 0x0000029EE4806FC0; to 'NeuronGroup' at 0x0000029EE1779990>, <weakref at 0x0000029EE4807650; dead>, <weakref at 0x0000029EE437EE30; dead>, <weakref at 0x0000029EE437C360; dead>, <weakref at 0x0000029EE47E86D0; to 'CythonCodeObject' at 0x0000029EE46F6AD0>, <weakref at 0x0000029EE47F36A0; to 'Thresholder' at 0x0000029EE46F5BD0>, <weakref at 0x0000029EE3D853F0; to 'Resetter' at 0x0000029EE427CBD0>, <weakref at 0x0000029EE3D93970; dead>, <weakref at 0x0000029EE4297DD0; to 'SpikeMonitor' at 0x0000029EE45E6C90>, <weakref at 0x0000029EE17D91C0; to 'MagicNetwork' at 0x0000029EE17CFCD0>, <weakref at 0x0000029EE48047C0; to 'SpikeMonitor' at 0x0000029EE3C7BD10>, <weakref at 0x0000029EE184CEA0; to 'Clock' at 0x0000029EE177BE10>, <weakref at 0x0000029EE4806CA0; dead>, <weakref at 0x0000029EE466B830; to 'CythonCodeObject' at 0x0000029EE47F4F10>, <weakref at 0x0000029EE3D873D0; to 'StateUpdater' at 0x0000029EE4242F50>})>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\irtho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\tracking.py\", line 32, in remove\n",
      "    set.remove(self, value)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n",
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n",
      "otimes  [2.261 1.536 2.202]\n",
      "delta_o  [-0.689 -0.257  0.076]\n",
      "otimes  [2.204 1.595 2.172]\n",
      "delta_o  [ 0.077  -0.2275 -0.778 ]\n",
      "Epoch 1/1 — avg loss=0.3348\n",
      "             ‖W1‖=1.306, ‖W2‖=3.945\n",
      "\n",
      "Trained W1: [[-2.24802890e-01 -5.02760943e-05 -3.02786849e-02 -7.89465047e-02\n",
      "  -8.32850990e-02 -4.77545338e-02 -8.20297270e-02 -8.11190851e-02\n",
      "  -1.19828297e-01 -1.44893684e-01]\n",
      " [-1.11390344e-01 -1.80939746e-01 -1.34243925e-01 -9.72715411e-02\n",
      "  -1.90581208e-01 -2.27950846e-02 -1.44948526e-01 -1.37085700e-01\n",
      "   1.53694720e-01  4.46186585e-02]\n",
      " [-2.41631112e-01 -1.56714608e-01  3.69612932e-01 -1.93654213e-01\n",
      "  -2.64931721e-01  2.68245103e-01 -4.42425639e-01  1.83656054e-02\n",
      "  -2.24950602e-01  2.78443194e-01]\n",
      " [ 5.13877625e-02  1.55790838e-02  3.39575082e-01  3.24111147e-01\n",
      "   1.25768049e-01 -5.69899260e-02  3.95072697e-02 -3.10625651e-01\n",
      "  -2.45227505e-01  4.21001016e-01]\n",
      " [-1.31178265e-01  6.46637541e-03 -9.75420633e-02  4.69224677e-03\n",
      "   1.15690476e-01 -2.10678045e-01  6.00154219e-02  1.02195157e-01\n",
      "   8.55197047e-02 -9.51946507e-02]]\n",
      "Trained W2: [[-0.31485765 -1.08552814  0.37171857]\n",
      " [ 0.78860029 -0.93310515  0.24188146]\n",
      " [ 0.66625006 -0.90895183  0.29448823]\n",
      " [ 0.88711645 -0.93572897  0.34326311]\n",
      " [ 0.08522722 -0.82672152  0.30371417]\n",
      " [ 0.90997097 -1.05585606  0.36229413]\n",
      " [ 0.77692459 -1.06224411  0.45785575]\n",
      " [ 0.72549459 -1.05064915  0.407461  ]\n",
      " [ 0.53035912 -0.81390781  0.28468818]\n",
      " [ 0.8701721  -0.99474145  0.29435131]\n",
      " [-0.01780186 -0.05807777  0.03830319]]\n",
      "Hidden times for x0: [0.934 0.966 1.024 0.986 0.952 0.999 0.941 0.942 0.966 1.03 ]\n",
      "Hidden times for x1: [0.899 0.952 1.094 1.002 0.944 1.023 0.877 0.919 0.927 1.105]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    return(np.tanh(x*w))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = W1 - lr * vW1\n",
    "        W2 = W2 - lr * vW2\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=10, lr=0.2)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e7d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n",
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 255\u001b[0m\n",
      "\u001b[0;32m    251\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "\u001b[1;32m--> 255\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m h_times \u001b[38;5;241m=\u001b[39m layer_forward(xi, W1, layer1_idx)\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n",
      "\u001b[1;32m--> 156\u001b[0m o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# — Separation loss — # calcs to loss for a single sample will not touch for now \u001b[39;00m\n",
      "\u001b[0;32m    160\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yi)\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n",
      "\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n",
      "\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n",
      "\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    336\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    342\u001b[0m ):\n",
      "\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n",
      "\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    240\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n",
      "\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n",
      "\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1276\u001b[0m )\n",
      "\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n",
      "\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=1, lr=0.0)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58402421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n",
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 255\u001b[0m\n",
      "\u001b[0;32m    251\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "\u001b[1;32m--> 255\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m h_times \u001b[38;5;241m=\u001b[39m layer_forward(xi, W1, layer1_idx)\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n",
      "\u001b[1;32m--> 156\u001b[0m o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# — Separation loss — # calcs to loss for a single sample will not touch for now \u001b[39;00m\n",
      "\u001b[0;32m    160\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yi)\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n",
      "\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n",
      "\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n",
      "\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    336\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    342\u001b[0m ):\n",
      "\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n",
      "\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    240\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n",
      "\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n",
      "\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1276\u001b[0m )\n",
      "\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n",
      "\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=1, lr=0.0)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd4c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n",
      "otimes  [2.666 2.003 2.524]\n",
      "delta_o  [-0.284  -0.0235  0.237 ]\n",
      "otimes  [2.726 2.003 2.619]\n",
      "delta_o  [ 0.338  -0.0235 -0.331 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 255\u001b[0m\n",
      "\u001b[0;32m    251\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n",
      "\u001b[1;32m--> 255\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m h_times \u001b[38;5;241m=\u001b[39m layer_forward(xi, W1, layer1_idx)\n",
      "\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m#print(h_times)\u001b[39;00m\n",
      "\u001b[1;32m--> 156\u001b[0m o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2_idx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this is sending in input array and outputs array for each layer\u001b[39;00m\n",
      "\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# — Separation loss — # calcs to loss for a single sample will not touch for now \u001b[39;00m\n",
      "\u001b[0;32m    160\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yi)\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n",
      "\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n",
      "\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n",
      "\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n",
      "\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    336\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    342\u001b[0m ):\n",
      "\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n",
      "\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    240\u001b[0m     duration,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n",
      "\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n",
      "\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   2647\u001b[0m             )\n",
      "\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n",
      "\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n",
      "\u001b[0;32m   2650\u001b[0m             )\n",
      "\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n",
      "\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n",
      "\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n",
      "\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n",
      "\u001b[0;32m   2657\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n",
      "\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n",
      "\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1276\u001b[0m )\n",
      "\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n",
      "\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.05,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # copy over the weights given \n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)  # changes how quckly it updates as we slow down or speed up   \n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2 \n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators - stores the collecting gradients for each epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2) \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y): # iterates in pairs at same time x1 and y1 x2 & y2 ect.\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            #print(h_times)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)  # this is sending in input array and outputs array for each layer\n",
    "\n",
    "\n",
    "            # — Separation loss — # calcs to loss for a single sample will not touch for now \n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum([(o_times[j] - non_target_time)**2 for j in non_ids])\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)  # a 3 element array for the 3 outputs\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            print(\"otimes \", o_times)\n",
    "            print(\"delta_o \",delta_o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = W1 - lr * vW1\n",
    "        W2 = W2 - lr * vW2\n",
    "\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.9, 0.7, 0.3, 0.4])\n",
    "    x1 = np.array([0.6, 0.7, 0.8, 0.9])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.0, 2.0])\n",
    "    y1 = np.array([2.0, 2.0, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "  \n",
    "\n",
    "    \n",
    "    # W1_0 = np.random.randn(4+1, 10) * 0.1  # +1 for bias\n",
    "    # W2_0 = np.random.randn(10+1, 3) * 0.1  # +1 for bias \n",
    "\n",
    "\n",
    "    W1_0 = np.load(\"W1.npy\")\n",
    "    W2_0 = np.load(\"W2.npy\")\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=1, lr=0.0)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # # ── Now test on the same two patterns ──\n",
    "    # print(\"\\n=== Test predictions ===\")\n",
    "    # for xi, yi in zip(X, Y):\n",
    "    #     # call layer_forward(positionally) rather than with layer1_idx=\n",
    "    #     h_times = layer_forward(xi, W1_tr, 1)\n",
    "    #     o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "    #     pred_class = np.argmax(o_times)  \n",
    "    #     true_class = np.argmax(yi)\n",
    "\n",
    "    #     print(f\"Input: {xi}\")\n",
    "    #     print(f\" Spike times: {o_times}\")\n",
    "    #     print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "# np.save('W1.npy', W1_tr)\n",
    "# np.save('W2.npy', W2_tr)\n",
    "# print(\"weights saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test predictions ===\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.228 5.    5.   ]\n",
      " Predicted class: 1, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.228 5.    5.   ]\n",
      " Predicted class: 1, True class: 2\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.359 5.    5.   ]\n",
      " Predicted class: 1, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.267 5.    5.   ]\n",
      " Predicted class: 1, True class: 2\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.228 5.    5.   ]\n",
      " Predicted class: 1, True class: 0\n",
      "\n",
      "Input: [0.6 0.7 0.8 0.9]\n",
      " Spike times: [2.305 1.152 1.397]\n",
      " Predicted class: 0, True class: 2\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_test, y_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# call layer_forward(positionally) rather than with layer1_idx=\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m layer_forward(x_test, W1_tr, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     pred_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(o_times)  \n\u001b[0;32m     33\u001b[0m     true_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test)\n",
      "Cell \u001b[1;32mIn[14], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.001*ms) * 1.2\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1270\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1269\u001b[0m     report_callback((end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m second, \u001b[38;5;241m1.0\u001b[39m, t_start, duration)\n\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1272\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished simulating network \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1276\u001b[0m )\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# Store profiling info (or erase old info to avoid confusion)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:236\u001b[0m, in \u001b[0;36mMagicNetwork.after_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mafter_run()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m--> 236\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#testing on Iris\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Iris dataset and scale features to [0.05,0.95]\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    y_encoded = np.zeros((y.size, y.max()+1))\n",
    "    y_encoded[np.arange(y.size), y] = 1\n",
    "    \n",
    "    def scale_features(x):\n",
    "        mn, mx = x.min(axis=0), x.max(axis=0)\n",
    "        x_norm = (x - mn) / (mx - mn)\n",
    "        return x_norm * 0.9 + 0.05\n",
    "\n",
    "    X_scaled = scale_features(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2,\n",
    "        random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for x_test, y_test in zip(X, Y):\n",
    "        # call layer_forward(positionally) rather than with layer1_idx=\n",
    "        h_times = layer_forward(x_test, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  \n",
    "        true_class = np.argmax(y_test)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
