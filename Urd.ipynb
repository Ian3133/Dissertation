{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fee676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_21960\\3780920302.py:161: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  L_non = 0.5 * λ * sum((o_times[j] - non_target_time)**2 for j in non_ids)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — avg loss=0.1507\n",
      "             ‖W1‖=3.104, ‖W2‖=3.468\n",
      "\n",
      "Epoch 2/30 — avg loss=0.1507\n",
      "             ‖W1‖=3.106, ‖W2‖=3.467\n",
      "\n",
      "Epoch 3/30 — avg loss=0.1506\n",
      "             ‖W1‖=3.110, ‖W2‖=3.465\n",
      "\n",
      "Epoch 4/30 — avg loss=0.1506\n",
      "             ‖W1‖=3.115, ‖W2‖=3.462\n",
      "\n",
      "Epoch 5/30 — avg loss=0.1509\n",
      "             ‖W1‖=3.122, ‖W2‖=3.459\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 285\u001b[0m\n\u001b[0;32m    269\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[ \u001b[38;5;241m0.38843549\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.10085101\u001b[39m,  \u001b[38;5;241m0.38776897\u001b[39m],\n\u001b[0;32m    270\u001b[0m             [ \u001b[38;5;241m0.35841177\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.06985881\u001b[39m,  \u001b[38;5;241m0.40542767\u001b[39m],\n\u001b[0;32m    271\u001b[0m             [ \u001b[38;5;241m0.40732835\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.63317637\u001b[39m,  \u001b[38;5;241m0.59202003\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m             [ \u001b[38;5;241m0.2191151\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.84980247\u001b[39m,  \u001b[38;5;241m0.14767976\u001b[39m],\n\u001b[0;32m    279\u001b[0m             [ \u001b[38;5;241m0.11199583\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.16498428\u001b[39m,  \u001b[38;5;241m0.00871235\u001b[39m]])\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# will print out last times so DO NOT run the same the same expermeent to have a differnt outcoem\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \n\u001b[0;32m    283\u001b[0m \n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W1_tr)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained W2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, W2_tr) \n",
      "Cell \u001b[1;32mIn[6], line 154\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n\u001b[0;32m    150\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# — Forward pass —\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m layer_forward(h_times, W2, layer2_idx)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# — Separation loss —\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 108\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m    102\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n\u001b[0;32m    107\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1230\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m active_objects:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_clock \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[1;32m-> 1230\u001b[0m             \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[0;32m   1233\u001b[0m     timestep, t, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_variables[c]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:236\u001b[0m, in \u001b[0;36mBrianObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m codeobj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_objects:\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mcodeobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "\n",
    "# Optionally compile but keep Python interface:\n",
    "set_device('runtime')  # default; compiles operations to .so but stays in Python process\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with backprop for 4-10-3\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1,\n",
    "    max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    non_target_time=2.0,\n",
    "    λ=0.5                # non-target penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network with:\n",
    "      • batched gradient updates\n",
    "      • boosted hidden-layer learning rate\n",
    "      • separate gradient clipping per layer\n",
    "      • classical momentum smoothing\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators for this epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            # — Separation loss —\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum((o_times[j] - non_target_time)**2 for j in non_ids)\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # — Gradients for W2 —\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.0]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k, j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "\n",
    "            # — Backprop into hidden & gradients for W1 —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw_output = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw_output  # Remove the W2[k,j] multiplication\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.0]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i, k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "\n",
    "            # — Accumulate —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average & clip gradients —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # Boost hidden-layer rate\n",
    "        lr1 = 5 * lr\n",
    "\n",
    "        # Separate clipping thresholds\n",
    "        g1 = np.clip(acc_dW1, -max_grad*5, max_grad*5)\n",
    "        g2 = np.clip(acc_dW2, -max_grad,   max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * g1\n",
    "        vW2 = beta * vW2 + (1 - beta) * g2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr1 * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr  * vW2, w_min, w_max)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "        print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # # print changes before applying them\n",
    "            # print(\"\\nWeight changes for this sample:\")\n",
    "            # for i in range(W1.shape[0]):\n",
    "            #     for k in range(W1.shape[1]):\n",
    "            #         change = -lr * dW1[i,k]\n",
    "            #         print(f\"  W1[{i},{k}] change: {change:+.5f}\")\n",
    "            # for k in range(W2.shape[0]):\n",
    "            #     for j in range(W2.shape[1]):\n",
    "            #         change = -lr * dW2[k,j]\n",
    "            #         print(f\"  W2[{k},{j}] change: {change:+.5f}\")\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    x0 = np.array([0.8, 0.7, 0.3, 0.6])\n",
    "    x1 = np.array([0.9, 0.5, 0.5, 0.2])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    y0 = np.array([2.95, 2.05, 2.05])\n",
    "    y1 = np.array([2.05, 2.05, 2.95])\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    # X= []\n",
    "    # Y = []\n",
    "    # for _ in range(10):\n",
    "    #     X.append(x0 + np.random.randn(4)*0.02);  Y.append(y0)\n",
    "    #     X.append(x1 + np.random.randn(4)*0.02);  Y.append(y1)\n",
    "    \n",
    "\n",
    "    \n",
    "    W1_0 = np.array([[0.21958991, 0.16223261, 0.02545166, 0.18849804, 0.09521701, 0.22744421, 0.05556097, 0.33130229, 0.03974721, 0.1968464],\n",
    "                [0.41958955, 0.47541312, 0.22287581, 0.69627866, 0.83639384, 0.79597959, 0.15029805, 0.126486, 0.18285382, 0.07470098],\n",
    "                [0.69559509, 0.41228614, 0.06028855, 0.51098037, 0.33730611, 1.17605488, 0.15405119, 0.28079173, 0.17365651, 0.23041775],\n",
    "                [0.79721356, 0.82210554, 0.15028745, 1.09421856, 0.68280376, 1.07577422, 0.16962136, 0.23838796, 0.0735181, 0.1719861],\n",
    "                [0.0631449, 0.10618091, 0.05791614, 0.0260418, -0.01797577, -0.1209534, 0.18702474, -0.01662061, -0.0683026, 0.05468931]])\n",
    "        #W1_0 = [[ 0.21958991,  0.16223261,  0.02545166  0.18849804  0.09521701  0.22744421, 0.05556097  0.33130229  0.03974721  0.1968464 ], [ 0.41958955  0.47541312  0.22287581  0.69627866  0.83639384  0.79597959, 0.15029805  0.126486    0.18285382  0.07470098], [ 0.69559509  0.41228614  0.06028855  0.51098037  0.33730611  1.17605488, 0.15405119  0.28079173  0.17365651  0.23041775], [ 0.79721356  0.82210554  0.15028745  1.09421856  0.68280376  1.07577422, 0.16962136  0.23838796  0.0735181   0.1719861 ], [ 0.0631449   0.10618091  0.05791614  0.0260418  -0.01797577 -0.1209534, 0.18702474 -0.01662061 -0.0683026   0.05468931]]\n",
    "        \n",
    "    \n",
    "        \n",
    "        #np.random.randn(4+1, 10)*0.1   # +1 for bias\n",
    "\n",
    "    \n",
    "    W2_0 = np.array([[ 0.38843549, -1.10085101,  0.38776897],\n",
    "                [ 0.35841177, -1.06985881,  0.40542767],\n",
    "                [ 0.40732835, -0.63317637,  0.59202003],\n",
    "                [ 0.32589618, -0.9691458,   0.48481597],\n",
    "                [ 0.26060079, -0.85267046,  0.75057083],\n",
    "                [ 0.31287437, -1.34806606,  0.44407244],\n",
    "                [ 0.1996638,  -0.65507816,  0.27473825],\n",
    "                [ 0.40169137, -0.9979045,   0.09884702],\n",
    "                [ 0.37579471, -0.62826323,  0.58035222],\n",
    "                [ 0.2191151,  -0.84980247,  0.14767976],\n",
    "                [ 0.11199583, -0.16498428,  0.00871235]])\n",
    "    \n",
    "    # will print out last times so DO NOT run the same the same expermeent to have a differnt outcoem\n",
    "\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=30, lr=0.5)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr) \n",
    "    print(\"Hidden times for x0:\", layer_forward(x0, W1_tr, 1))\n",
    "    print(\"Hidden times for x1:\", layer_forward(x1, W1_tr, 1))\n",
    "\n",
    "    # ── Now test on the same two patterns ──\n",
    "    print(\"\\n=== Test predictions ===\")\n",
    "    for xi, yi in zip(X, Y):\n",
    "        # call layer_forward(positionally) rather than with layer1_idx=\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "\n",
    "        pred_class = np.argmax(o_times)  # changed to argmin WHY???\n",
    "        true_class = np.argmax(yi)\n",
    "\n",
    "        print(f\"Input: {xi}\")\n",
    "        print(f\" Spike times: {o_times}\")\n",
    "        print(f\" Predicted class: {pred_class}, True class: {true_class}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95450212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_21960\\1577268996.py:157: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  L_non = 0.5 * λ * sum((o_times[j] - non_target_time)**2 for j in non_ids)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 — avg loss=0.1794\n",
      "             ‖W1‖=0.672, ‖W2‖=0.555\n",
      "             Accuracy: 0.500\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 259\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Training ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Train with more conservative parameters\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Final Test Results ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput pattern -> Output times -> Predicted class (True class)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 148\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max, non_target_time, λ)\u001b[0m\n\u001b[0;32m    144\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# — Forward pass —\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m layer_forward(h_times, W2, layer2_idx)\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# — Loss computation —\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 81\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m     78\u001b[0m G\u001b[38;5;241m.\u001b[39mscheduled_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e9\u001b[39m\u001b[38;5;241m*\u001b[39msecond\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# stim: now includes bias spike at t=0\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m stim \u001b[38;5;241m=\u001b[39m \u001b[43mSpikeGeneratorGroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_in_plus_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_in_plus_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maug_inputs\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m S \u001b[38;5;241m=\u001b[39m Synapses(stim, G, \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mw:1\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124m    layer:1\u001b[39m\u001b[38;5;124m'''\u001b[39m,\n\u001b[0;32m     87\u001b[0m     on_pre\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124m    scheduled_time = (sum/sr + layer)*ms\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m     92\u001b[0m S\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\input\\spikegeneratorgroup.py:81\u001b[0m, in \u001b[0;36mSpikeGeneratorGroup.__init__\u001b[1;34m(self, N, indices, times, dt, clock, period, when, order, sorted, name, codeobj_class)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, times\u001b[38;5;241m=\u001b[39msecond, period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     codeobj_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mGroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# We store the indices and times also directly in the Python object,\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# this way we can use them for checks in `before_run` even in standalone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this when the checks in `before_run` have been moved to the template\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m#: Array of spiking neuron indices.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neuron_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:77\u001b[0m, in \u001b[0;36mBrianObject.__init__\u001b[1;34m(self, dt, clock, when, order, namespace, name)\u001b[0m\n\u001b[0;32m     75\u001b[0m         base, _ \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(sys\u001b[38;5;241m.\u001b[39mmodules[modulename]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n\u001b[0;32m     76\u001b[0m         bases\u001b[38;5;241m.\u001b[39mappend(base)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname, linenum, funcname, line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(base \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m fname \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m bases):\n\u001b[0;32m     79\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  File \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlinenum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\traceback.py:228\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 228\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\traceback.py:390\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\traceback.py:429\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    425\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    426\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    427\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 429\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from brian2 import prefs, set_device\n",
    "\n",
    "# Tell Brian2 to use the Cython code generator:\n",
    "prefs.codegen.target = 'cython'\n",
    "set_device('runtime')\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative with improved numerical stability\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = np.clip(global_clock % 1, 1e-8, 1-1e-8)  # Avoid boundary issues\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = np.clip(global_clock % 1, 1e-8, 1-1e-8)  # Avoid boundary issues\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        log_x = np.log(np.maximum(x, eps))\n",
    "        return -np.power(x, (1 - w)) * log_x\n",
    "    else:\n",
    "        log_1_minus_x = np.log(np.maximum(1 - x, eps))\n",
    "        return -np.power((1 - x), (1 + w)) * log_1_minus_x\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Forward pass: 4->10->3 using two-stage mini_urd\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    \"\"\"\n",
    "    inputs: array of spike times (ms) from previous layer (shape: n_in,)\n",
    "    W: weight matrix shape (n_in+1, n_out)  ← note the extra bias row\n",
    "    layer_idx: integer layer number\n",
    "    returns: array of output spike times (ms)\n",
    "    \"\"\"\n",
    "    # 1) augment inputs with bias spike @ t=0\n",
    "    bias_time = 0.0\n",
    "    aug_inputs = np.concatenate((inputs, [bias_time]))  # shape (n_in+1,)\n",
    "\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    assert aug_inputs.size == n_in_plus_bias\n",
    "\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.001*ms\n",
    "\n",
    "        # single post‐synaptic neuron\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "\n",
    "        # init\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # stim: now includes bias spike at t=0\n",
    "        stim = SpikeGeneratorGroup(n_in_plus_bias,\n",
    "                                   indices=list(range(n_in_plus_bias)),\n",
    "                                   times=aug_inputs*ms)\n",
    "\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''',\n",
    "            on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "\n",
    "    return np.array(out_times)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop with corrected backprop for 4-10-3\n",
    "\n",
    "def train_snn_backprop(\n",
    "    X, Y,                # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.05,   # Reduced learning rate\n",
    "    max_grad=5.0,         # Reduced gradient clipping\n",
    "    w_min=-10.0, w_max=10.0,  # Reduced weight bounds\n",
    "    non_target_time=2.0,  # Non-target neurons should spike at 2.0ms\n",
    "    λ=0.2                 # Reduced penalty weight\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a 4→10→3 spiking network where:\n",
    "    - Target neuron should spike at specified time (2.95ms)\n",
    "    - Non-target neurons should spike at 2.0ms\n",
    "    - Classification uses argmax (latest spike wins)\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    W1 = W1_init.copy()      # shape (5,10) including bias row\n",
    "    W2 = W2_init.copy()      # shape (11,3) including bias row\n",
    "\n",
    "    # Momentum buffers\n",
    "    beta = 0.9\n",
    "    vW1 = np.zeros_like(W1)\n",
    "    vW2 = np.zeros_like(W2)\n",
    "\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Accumulators for this epoch\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            # — Forward pass —\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "\n",
    "            # — Loss computation —\n",
    "            target_idx = np.argmax(yi)\n",
    "            L_target = 0.5 * (o_times[target_idx] - yi[target_idx])**2\n",
    "            \n",
    "            # Non-target loss: push other neurons to fire at 2.0ms\n",
    "            non_ids = [j for j in range(len(o_times)) if j != target_idx]\n",
    "            L_non = 0.5 * λ * sum((o_times[j] - non_target_time)**2 for j in non_ids)\n",
    "            L = L_target + L_non\n",
    "            epoch_loss += L\n",
    "\n",
    "            # — Output layer gradients (W2) —\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (o_times[target_idx] - yi[target_idx])\n",
    "            for j in non_ids:\n",
    "                delta_o[j] = λ * (o_times[j] - non_target_time)\n",
    "\n",
    "            # Compute dW2 gradients\n",
    "            aug_h = np.concatenate((h_times, [0.0]))  # Add bias\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    if aug_h[k] > 0:  # Only compute if input exists\n",
    "                        dt_dw = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                        dW2[k, j] = delta_o[j] * dt_dw\n",
    "\n",
    "            # — Hidden layer gradients (backprop) —\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    if aug_h[k] > 0:  # Only backprop if hidden neuron fired\n",
    "                        # Chain rule: dL/dh_k = sum_j (dL/do_j * do_j/dh_k)\n",
    "                        # do_j/dh_k = sum_i (do_j/dw2_ij * dw2_ij/dh_k)\n",
    "                        # Since w2_ij is independent of h_k, we need do_j/dh_k directly\n",
    "                        \n",
    "                        # For spike timing, output depends on input time through the spike_timing function\n",
    "                        # This is a simplified approximation - in practice, this relationship is complex\n",
    "                        dt_dh = d_spike_timing_dw(W2[k, j], aug_h[k], layer2_idx, 0, 1)\n",
    "                        delta_h[k] += delta_o[j] * dt_dh\n",
    "\n",
    "            # Compute dW1 gradients\n",
    "            aug_xi = np.concatenate((xi, [0.0]))  # Add bias\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    if aug_xi[i] > 0:  # Only compute if input exists\n",
    "                        dt_dw = d_spike_timing_dw(W1[i, k], aug_xi[i], layer1_idx, 0, 1)\n",
    "                        dW1[i, k] = delta_h[k] * dt_dw\n",
    "\n",
    "            # — Accumulate gradients —\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        # — Average gradients over batch —\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "\n",
    "        # — Clip gradients —\n",
    "        acc_dW1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        acc_dW2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "\n",
    "        # — Momentum updates —\n",
    "        vW1 = beta * vW1 + (1 - beta) * acc_dW1\n",
    "        vW2 = beta * vW2 + (1 - beta) * acc_dW2\n",
    "\n",
    "        # — Apply weight updates & clamp —\n",
    "        W1 = np.clip(W1 - lr * vW1, w_min, w_max)\n",
    "        W2 = np.clip(W2 - lr * vW2, w_min, w_max)\n",
    "\n",
    "        # — Progress report —\n",
    "        if ep % 5 == 0 or ep == epochs - 1:\n",
    "            print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "            print(f\"             ‖W1‖={np.linalg.norm(W1):.3f}, ‖W2‖={np.linalg.norm(W2):.3f}\")\n",
    "            \n",
    "            # Quick accuracy check\n",
    "            correct = 0\n",
    "            for xi, yi in zip(X, Y):\n",
    "                h_times = layer_forward(xi, W1, layer1_idx)\n",
    "                o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "                pred_class = np.argmax(o_times)\n",
    "                true_class = np.argmax(yi)\n",
    "                if pred_class == true_class:\n",
    "                    correct += 1\n",
    "            accuracy = correct / len(X)\n",
    "            print(f\"             Accuracy: {accuracy:.3f}\")\n",
    "            print()\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage with fixed input/target pairs\n",
    "    x0 = np.array([0.8, 0.7, 0.3, 0.6])\n",
    "    x1 = np.array([0.9, 0.5, 0.5, 0.2])\n",
    "    X = [x0 if i % 2 == 0 else x1 for i in range(8)]\n",
    "    \n",
    "    # Target spike times: larger value = correct class\n",
    "    # Class 0: neuron 0 should spike at 2.95ms, others at 2.0ms\n",
    "    # Class 2: neuron 2 should spike at 2.95ms, others at 2.0ms\n",
    "    y0 = np.array([2.95, 2.0, 2.0])   # Class 0 target\n",
    "    y1 = np.array([2.0, 2.0, 2.95])   # Class 2 target\n",
    "    Y = [y0 if i % 2 == 0 else y1 for i in range(8)]\n",
    "    \n",
    "    # Smaller, more stable weight initialization\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    W1_0 = np.random.randn(5, 10) * 0.1   # Smaller initial weights\n",
    "    W2_0 = np.random.randn(11, 3) * 0.1\n",
    "    \n",
    "    print(\"=== Training ===\")\n",
    "    # Train with more conservative parameters\n",
    "    W1_tr, W2_tr = train_snn_backprop(X, Y, W1_0, W2_0,\n",
    "                                      epochs=50, lr=0.1)\n",
    "    \n",
    "    print(\"\\n=== Final Test Results ===\")\n",
    "    print(\"Input pattern -> Output times -> Predicted class (True class)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (xi, yi) in enumerate(zip(X, Y)):\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "        \n",
    "        pred_class = np.argmax(o_times)\n",
    "        true_class = np.argmax(yi)\n",
    "        \n",
    "        print(f\"Sample {i+1}: {xi}\")\n",
    "        print(f\"  Output times: [{o_times[0]:.3f}, {o_times[1]:.3f}, {o_times[2]:.3f}]\")\n",
    "        print(f\"  Predicted: {pred_class}, True: {true_class} {'✓' if pred_class == true_class else '✗'}\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate final accuracy\n",
    "    correct = 0\n",
    "    for xi, yi in zip(X, Y):\n",
    "        h_times = layer_forward(xi, W1_tr, 1)\n",
    "        o_times = layer_forward(h_times, W2_tr, 2)\n",
    "        pred_class = np.argmax(o_times)\n",
    "        true_class = np.argmax(yi)\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "    \n",
    "    final_accuracy = correct / len(X)\n",
    "    print(f\"Final Accuracy: {final_accuracy:.3f} ({correct}/{len(X)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba5326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Robust SNN Training with Enhanced Loss Function ===\n",
      "Starting robust training...\n",
      "Epoch 1/200 — avg loss=1.2006\n",
      "             Accuracy: 0.000\n",
      "             Sample output: [3.425, 3.561, 3.389]\n",
      "             Target: [2.950, 2.000, 2.000]\n",
      "\n",
      "Epoch 21/200 — avg loss=3.7211\n",
      "             Accuracy: 0.000\n",
      "\n",
      "Epoch 41/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "             Sample output: [3.332, 4.237, 3.332]\n",
      "             Target: [2.950, 2.000, 2.000]\n",
      "\n",
      "Epoch 61/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "\n",
      "Epoch 81/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "             Sample output: [3.332, 4.237, 3.332]\n",
      "             Target: [2.950, 2.000, 2.000]\n",
      "\n",
      "Epoch 101/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "\n",
      "Epoch 121/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "             Sample output: [3.332, 4.237, 3.332]\n",
      "             Target: [2.950, 2.000, 2.000]\n",
      "\n",
      "Epoch 141/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "\n",
      "Epoch 161/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "             Sample output: [3.332, 4.237, 3.332]\n",
      "             Target: [2.950, 2.000, 2.000]\n",
      "\n",
      "Epoch 181/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "\n",
      "Epoch 200/200 — avg loss=4.5120\n",
      "             Accuracy: 0.000\n",
      "\n",
      "\n",
      "=== Final Test Results ===\n",
      "Input pattern -> Output times -> Predicted class (True class)\n",
      "-----------------------------------------------------------------\n",
      "Sample 1: [0.8 0.7 0.3 0.6]\n",
      "  Output times: [3.332, 4.237, 3.332]\n",
      "  Target times: [2.950, 2.000, 2.000]\n",
      "  Predicted: 1, True: 0 ✗\n",
      "\n",
      "Sample 2: [0.9 0.5 0.5 0.2]\n",
      "  Output times: [3.277, 4.182, 3.277]\n",
      "  Target times: [2.000, 2.000, 2.950]\n",
      "  Predicted: 1, True: 2 ✗\n",
      "\n",
      "Sample 3: [0.8 0.7 0.3 0.6]\n",
      "  Output times: [3.332, 4.237, 3.332]\n",
      "  Target times: [2.950, 2.000, 2.000]\n",
      "  Predicted: 1, True: 0 ✗\n",
      "\n",
      "Sample 4: [0.9 0.5 0.5 0.2]\n",
      "  Output times: [3.277, 4.182, 3.277]\n",
      "  Target times: [2.000, 2.000, 2.950]\n",
      "  Predicted: 1, True: 2 ✗\n",
      "\n",
      "Sample 5: [0.8 0.7 0.3 0.6]\n",
      "  Output times: [3.332, 4.237, 3.332]\n",
      "  Target times: [2.950, 2.000, 2.000]\n",
      "  Predicted: 1, True: 0 ✗\n",
      "\n",
      "Sample 6: [0.9 0.5 0.5 0.2]\n",
      "  Output times: [3.277, 4.182, 3.277]\n",
      "  Target times: [2.000, 2.000, 2.950]\n",
      "  Predicted: 1, True: 2 ✗\n",
      "\n",
      "Sample 7: [0.8 0.7 0.3 0.6]\n",
      "  Output times: [3.332, 4.237, 3.332]\n",
      "  Target times: [2.950, 2.000, 2.000]\n",
      "  Predicted: 1, True: 0 ✗\n",
      "\n",
      "Sample 8: [0.9 0.5 0.5 0.2]\n",
      "  Output times: [3.277, 4.182, 3.277]\n",
      "  Target times: [2.000, 2.000, 2.950]\n",
      "  Predicted: 1, True: 2 ✗\n",
      "\n",
      "Final Accuracy: 0.000 (0/8)\n",
      "\n",
      "Weight statistics:\n",
      "W1 - mean: 3.000, std: 0.000, range: [3.000, 3.000]\n",
      "W2 - mean: -1.000, std: 2.828, range: [-3.000, 3.000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dummy dataset (8 samples, 5 features)\n",
    "X = np.array([\n",
    "    [1, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [1, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 2, 2])\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 3\n",
    "learning_rate = 0.5\n",
    "num_epochs = 200\n",
    "margin = 0.5\n",
    "layer_offset = 0.2\n",
    "\n",
    "def spike_timing(w, input_time, layer_offset):\n",
    "    return input_time + w + layer_offset\n",
    "\n",
    "class SpikingNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Parameter(torch.randn(input_size, hidden_size) * 0.8)\n",
    "        self.W2 = nn.Parameter(torch.randn(hidden_size + 1, output_size) * 0.8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1: compute spike times\n",
    "        x = x.unsqueeze(0) if len(x.shape) == 1 else x\n",
    "        x_times = x.clone()\n",
    "        h_times = spike_timing(self.W1.T, x_times, layer_offset)\n",
    "\n",
    "        # Add bias input\n",
    "        bias = torch.ones((x.shape[0], 1))\n",
    "        h_with_bias = torch.cat([h_times, bias], dim=1)\n",
    "\n",
    "        # Layer 2: output spike times\n",
    "        o_times = spike_timing(self.W2.T, h_with_bias, layer_offset)\n",
    "        return o_times\n",
    "\n",
    "def ranking_loss(o_times, target_class, margin):\n",
    "    correct_time = o_times[target_class]\n",
    "    loss = 0.0\n",
    "    for i, t in enumerate(o_times):\n",
    "        if i != target_class:\n",
    "            loss += torch.clamp(t - correct_time + margin, min=0)\n",
    "    return loss\n",
    "\n",
    "model = SpikingNet(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting robust training...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        x = torch.tensor(X[i])\n",
    "        target = y[i]\n",
    "\n",
    "        o_times = model(x)\n",
    "        loss = ranking_loss(o_times, target, margin)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = torch.argmin(o_times).item()\n",
    "        correct += (pred == target)\n",
    "\n",
    "    if epoch % 20 == 1 or epoch == num_epochs:\n",
    "        print(f\"Epoch {epoch}/{num_epochs} — avg loss={total_loss / len(X):.4f}\")\n",
    "        print(f\"             Accuracy: {correct / len(X):.3f}\")\n",
    "        print(f\"             Sample output: {o_times.detach().numpy().round(3)}\")\n",
    "        print(f\"             Target: {[o if i == target else '-' for i, o in enumerate(o_times.detach().numpy().round(3))]}\")\n",
    "\n",
    "# Final test\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    x = torch.tensor(X[i])\n",
    "    target = y[i]\n",
    "    o_times = model(x)\n",
    "    pred = torch.argmin(o_times).item()\n",
    "    print(f\"Sample {i}: Pred={pred}, True={target} {'✓' if pred == target else '✗'}\")\n",
    "    correct += (pred == target)\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {correct / len(X):.3f} ({correct}/{len(X)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8]), array([0.6, 0.2, 0.4, 0.8])]\n",
      "[array([0.8, 0.7, 0.3, 0.6]), array([0.9, 0.5, 0.5, 0.2]), array([0.8, 0.7, 0.3, 0.6]), array([0.9, 0.5, 0.5, 0.2]), array([0.8, 0.7, 0.3, 0.6]), array([0.9, 0.5, 0.5, 0.2]), array([0.8, 0.7, 0.3, 0.6]), array([0.9, 0.5, 0.5, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "block1 = np.array([0.8, 0.7, 0.3, 0.6])\n",
    "block2 = np.array([0.9, 0.5, 0.5, 0.2])\n",
    "it = [block1 if i % 2 == 0 else block2 for i in range(8)]\n",
    "\n",
    "\n",
    "\n",
    "X = [np.array([0.6, 0.2, 0.4, 0.8]) for _ in range(8)]\n",
    "\n",
    "print(X)\n",
    "print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Robust SNN Training with Enhanced Loss Function ===\n",
      "Starting robust training...\n",
      "Epoch 1/200 — avg loss=0.9566\n",
      "             Accuracy: 0.500\n",
      "             Sample output: [3.329 3.298 3.35 ]\n",
      "             Target: [2.95 2.   2.  ]\n",
      "\n",
      "Epoch 21/200 — avg loss=4.2276\n",
      "             Accuracy: 0.500\n",
      "Epoch 41/200 — avg loss=5.2386\n",
      "             Accuracy: 0.500\n",
      "             Sample output: [4.234 3.329 4.234]\n",
      "             Target: [2.95 2.   2.  ]\n",
      "\n",
      "Epoch 61/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "Epoch 81/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "             Sample output: [4.237 3.332 4.237]\n",
      "             Target: [2.95 2.   2.  ]\n",
      "\n",
      "Epoch 101/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "Epoch 121/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "             Sample output: [4.237 3.332 4.237]\n",
      "             Target: [2.95 2.   2.  ]\n",
      "\n",
      "Epoch 141/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "Epoch 161/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "             Sample output: [4.237 3.332 4.237]\n",
      "             Target: [2.95 2.   2.  ]\n",
      "\n",
      "Epoch 181/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "Epoch 200/200 — avg loss=5.2424\n",
      "             Accuracy: 0.500\n",
      "\n",
      "=== Final Test Results ===\n",
      "Sample 1: Pred=0, True=0 ✓\n",
      "Sample 2: Pred=0, True=2 ✗\n",
      "Sample 3: Pred=0, True=0 ✓\n",
      "Sample 4: Pred=0, True=2 ✗\n",
      "Sample 5: Pred=0, True=0 ✓\n",
      "Sample 6: Pred=0, True=2 ✗\n",
      "Sample 7: Pred=0, True=0 ✓\n",
      "Sample 8: Pred=0, True=2 ✗\n",
      "\n",
      "Final Accuracy: 0.500 (4/8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "np.seterr(over='ignore', under='ignore')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing functions (no Brian2 dependency)\n",
    "\n",
    "def spike_timing(w, input_time, layer_offset):\n",
    "    if input_time <= 0:\n",
    "        return 5.0  # No spike if no input\n",
    "    base_time = input_time\n",
    "    timing_factor = np.tanh(w * 0.5)\n",
    "    scaled_timing = base_time + timing_factor * 0.5\n",
    "    return scaled_timing + layer_offset\n",
    "\n",
    "def d_spike_timing_dw(w, input_time, layer_offset):\n",
    "    if input_time <= 0:\n",
    "        return 0.0\n",
    "    tanh_val = np.tanh(w * 0.5)\n",
    "    return 0.25 * (1 - tanh_val**2)\n",
    "    \n",
    "   \n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Efficient forward pass without Brian2\n",
    "\n",
    "def layer_forward_enhanced(inputs, W, layer_idx):\n",
    "    aug_inputs = np.concatenate((inputs, [0.0]))\n",
    "    n_in_plus_bias, n_out = W.shape\n",
    "    output_times = np.zeros(n_out)\n",
    "\n",
    "    for j in range(n_out):\n",
    "        weighted_sum = 0.0\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for i in range(n_in_plus_bias):\n",
    "            input_time = aug_inputs[i] if i < len(inputs) else 0.1\n",
    "            weight = W[i, j]\n",
    "            contribution = spike_timing(weight, input_time, 0)\n",
    "            weighted_sum += contribution * abs(weight)\n",
    "            total_weight += abs(weight)\n",
    "\n",
    "        if total_weight > 0:\n",
    "            output_times[j] = (weighted_sum / total_weight) + layer_idx\n",
    "        else:\n",
    "            output_times[j] = 2.0 + layer_idx\n",
    "\n",
    "    return output_times\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training with efficient forward pass\n",
    "\n",
    "def train_snn_robust(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=100, lr=0.3,\n",
    "    max_grad=5.0,\n",
    "    w_min=-3.0, w_max=3.0,\n",
    "    target_separation=0.5\n",
    "):\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "\n",
    "    beta1, beta2 = 0.9, 0.999\n",
    "    eps = 1e-8\n",
    "    m1_W1 = np.zeros_like(W1)\n",
    "    v1_W1 = np.zeros_like(W1)\n",
    "    m1_W2 = np.zeros_like(W2)\n",
    "    v1_W2 = np.zeros_like(W2)\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        acc_dW1 = np.zeros_like(W1)\n",
    "        acc_dW2 = np.zeros_like(W2)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for xi, yi in zip(X, Y):\n",
    "            h_times = layer_forward_enhanced(xi, W1, 1)\n",
    "            o_times = layer_forward_enhanced(h_times, W2, 2)\n",
    "\n",
    "            target_idx = np.argmax(yi)\n",
    "            target_time = o_times[target_idx]\n",
    "\n",
    "            separation_losses = []\n",
    "            for j in range(len(o_times)):\n",
    "                if j != target_idx:\n",
    "                    margin = o_times[j] - target_time\n",
    "                    if margin < target_separation:\n",
    "                        separation_losses.append((target_separation - margin) ** 2)\n",
    "                    else:\n",
    "                        separation_losses.append(0.0)\n",
    "\n",
    "            target_loss = 0.5 * (target_time - yi[target_idx]) ** 2\n",
    "            separation_loss = sum(separation_losses)\n",
    "            L = target_loss + 2.0 * separation_loss\n",
    "            epoch_loss += L\n",
    "\n",
    "            delta_o = np.zeros_like(o_times)\n",
    "            delta_o[target_idx] = (target_time - yi[target_idx])\n",
    "\n",
    "            for j in range(len(o_times)):\n",
    "                if j != target_idx:\n",
    "                    margin = o_times[j] - target_time\n",
    "                    if margin < target_separation:\n",
    "                        delta_o[j] = 2.0 * (target_separation - margin)\n",
    "                        delta_o[target_idx] -= 2.0 * (target_separation - margin)\n",
    "\n",
    "            aug_h = np.concatenate((h_times, [0.1]))\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    input_time = aug_h[k]\n",
    "                    dt_dw = d_spike_timing_dw(W2[k, j], input_time, 2)\n",
    "                    dW2[k, j] = delta_o[j] * dt_dw * 10.0\n",
    "\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    input_time = aug_h[k]\n",
    "                    dt_dw = d_spike_timing_dw(W2[k, j], input_time, 2)\n",
    "                    delta_h[k] += delta_o[j] * dt_dw * W2[k, j] * 0.1\n",
    "\n",
    "            aug_xi = np.concatenate((xi, [0.1]))\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    input_time = aug_xi[i]\n",
    "                    dt_dw = d_spike_timing_dw(W1[i, k], input_time, 1)\n",
    "                    dW1[i, k] = delta_h[k] * dt_dw * 10.0\n",
    "\n",
    "            acc_dW1 += dW1\n",
    "            acc_dW2 += dW2\n",
    "\n",
    "        acc_dW1 /= N\n",
    "        acc_dW2 /= N\n",
    "        acc_dW1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "        acc_dW2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "\n",
    "        m1_W1 = beta1 * m1_W1 + (1 - beta1) * acc_dW1\n",
    "        m1_W2 = beta1 * m1_W2 + (1 - beta1) * acc_dW2\n",
    "        v1_W1 = beta2 * v1_W1 + (1 - beta2) * (acc_dW1 ** 2)\n",
    "        v1_W2 = beta2 * v1_W2 + (1 - beta2) * (acc_dW2 ** 2)\n",
    "\n",
    "        m1_W1_corr = m1_W1 / (1 - beta1 ** (ep + 1))\n",
    "        m1_W2_corr = m1_W2 / (1 - beta1 ** (ep + 1))\n",
    "        v1_W1_corr = v1_W1 / (1 - beta2 ** (ep + 1))\n",
    "        v1_W2_corr = v1_W2 / (1 - beta2 ** (ep + 1))\n",
    "\n",
    "        W1 -= lr * m1_W1_corr / (np.sqrt(v1_W1_corr) + eps)\n",
    "        W2 -= lr * m1_W2_corr / (np.sqrt(v1_W2_corr) + eps)\n",
    "\n",
    "        W1 = np.clip(W1, w_min, w_max)\n",
    "        W2 = np.clip(W2, w_min, w_max)\n",
    "\n",
    "        if ep % 20 == 0 or ep == epochs - 1:\n",
    "            print(f\"Epoch {ep+1}/{epochs} — avg loss={epoch_loss/N:.4f}\")\n",
    "            correct = 0\n",
    "            for xi, yi in zip(X, Y):\n",
    "                h_times = layer_forward_enhanced(xi, W1, 1)\n",
    "                o_times = layer_forward_enhanced(h_times, W2, 2)\n",
    "                if np.argmax(o_times) == np.argmax(yi):\n",
    "                    correct += 1\n",
    "            print(f\"             Accuracy: {correct / len(X):.3f}\")\n",
    "            if ep % 40 == 0:\n",
    "                xi, yi = X[0], Y[0]\n",
    "                h_times = layer_forward_enhanced(xi, W1, 1)\n",
    "                o_times = layer_forward_enhanced(h_times, W2, 2)\n",
    "                print(f\"             Sample output: {o_times.round(3)}\")\n",
    "                print(f\"             Target: {yi.round(3)}\\n\")\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Main execution\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c27d8",
   "metadata": {},
   "source": [
    "Trained W1: [[ 4.42270137e+00  2.72902627e+00  4.52251250e+00 -2.66418654e-02\n",
    "  -1.32903905e-03  4.14133410e-02  4.61386303e+00  1.83720797e-01\n",
    "  -7.51569272e-02  3.51829401e+00]\n",
    " [ 5.39742143e+00  4.22116997e+00  5.31105386e+00 -4.65205424e-02\n",
    "  -1.70381678e-02  7.78778861e-02  5.08195082e+00  7.47894500e-03\n",
    "  -9.03152356e-03  5.05393082e+00]\n",
    " [ 4.43345211e+00  2.17641016e+00  4.37444012e+00  7.41919002e-01\n",
    "  -6.53028519e-02  9.41795776e-03  4.57591258e+00  1.17434939e-01\n",
    "  -2.42172258e-01  2.89698446e+00]\n",
    " [ 2.96324751e+00  1.34519281e+00  3.20728606e+00 -1.25186459e+00\n",
    "  -1.58785854e-03 -6.27490458e-02  4.34013077e+00 -6.69617214e-01\n",
    "   9.15434146e-02  1.46241605e+00]]\n",
    "Trained W2: [[-5.19009294e+00 -9.22196291e-01  1.06320763e-01]\n",
    " [-4.14733377e+00 -1.11731667e+00  7.87285482e-03]\n",
    " [-5.35983978e+00 -7.60399948e-01 -1.72060318e-03]\n",
    " [ 6.37550208e-01 -7.07602762e-01  1.22677933e-01]\n",
    " [-1.99769654e-02 -4.49491576e-01  9.30378392e-02]\n",
    " [ 3.01707533e-02 -1.04142396e+00  1.53061594e-01]\n",
    " [-6.27254161e+00 -5.01911927e-01  1.12362536e-02]\n",
    " [ 6.85429277e-01 -1.12952211e+00  6.65988918e-02]\n",
    " [ 1.42345211e-01 -4.74105845e-01  7.04198093e-02]\n",
    " [-4.10253561e+00 -5.98382591e-01  1.40683024e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8f61325",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W1_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m X_test[i]\n\u001b[1;32m---> 23\u001b[0m     h \u001b[38;5;241m=\u001b[39m relu(np\u001b[38;5;241m.\u001b[39mdot(x, \u001b[43mW1_tr\u001b[49m))         \u001b[38;5;66;03m# hidden layer\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(h, W2_tr)             \u001b[38;5;66;03m# output layer\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     pred_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(out)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W1_tr' is not defined"
     ]
    }
   ],
   "source": [
    "# assuming you have W1_tr, W2_tr from training\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# convert X_test from scaler, y_test from integer classes\n",
    "# use X_test from the same split\n",
    "# we still have y[indices] from before:\n",
    "_, X_test_raw, _, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# still assuming you have W1_tr, W2_tr, relu, X_test, y_test\n",
    "\n",
    "# initialize counts\n",
    "class_totals = {0: 0, 1: 0, 2: 0}\n",
    "class_correct = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "total = len(X_test)\n",
    "\n",
    "for i in range(total):\n",
    "    x = X_test[i]\n",
    "    h = relu(np.dot(x, W1_tr))         # hidden layer\n",
    "    out = np.dot(h, W2_tr)             # output layer\n",
    "\n",
    "    pred_class = np.argmax(out)\n",
    "    true_class = y_test[i]\n",
    "    \n",
    "    class_totals[true_class] += 1\n",
    "    if pred_class == true_class:\n",
    "        class_correct[true_class] += 1\n",
    "\n",
    "# print per-class accuracy\n",
    "for c in [0, 1, 2]:\n",
    "    total_c = class_totals[c]\n",
    "    correct_c = class_correct[c]\n",
    "    acc_c = correct_c / total_c if total_c > 0 else 0\n",
    "    print(f\"Class {c}: {correct_c}/{total_c} correct ({acc_c:.2%})\")\n",
    "\n",
    "# also global accuracy\n",
    "overall_correct = sum(class_correct.values())\n",
    "# accuracy = overall_correct / total\n",
    "# print(f\"Overall accuracy: {accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, Y_encoded\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\n\u001b[0;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, epochs, lr, max_grad, w_min, w_max)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[1;32m---> 94\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m layer_forward(h_times, W2, layer2_idx)\n\u001b[0;32m     96\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum((o_times \u001b[38;5;241m-\u001b[39m yi)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 72\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m     67\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n\u001b[0;32m     71\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n\u001b[1;32m---> 72\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     74\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m5.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1230\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m active_objects:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_clock \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[1;32m-> 1230\u001b[0m             \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[0;32m   1233\u001b[0m     timestep, t, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_variables[c]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:236\u001b[0m, in \u001b[0;36mBrianObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m codeobj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_objects:\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mcodeobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:131\u001b[0m, in \u001b[0;36mCodeObject.__call__\u001b[1;34m(self, **kwds)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_namespace()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:159\u001b[0m, in \u001b[0;36mCodeObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Runs the main code in the namespace.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        defined during the call of `CodeGenerator.code_object`.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\runtime\\numpy_rt\\numpy_rt.py:281\u001b[0m, in \u001b[0;36mNumpyCodeObject.run_block\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     exec(compiled_code, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    283\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode, block)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Layer forward pass\n",
    "\n",
    "def layer_forward(inputs, W, layer_idx):\n",
    "    n_in, n_out = W.shape\n",
    "    out_times = []\n",
    "    for j in range(n_out):\n",
    "        start_scope()\n",
    "        defaultclock.dt = 0.0001*ms\n",
    "        G = NeuronGroup(1, '''\n",
    "            v : 1\n",
    "            sum : 1\n",
    "            sr : 1\n",
    "            scheduled_time : second\n",
    "            global_clock : 1\n",
    "        ''', threshold='v>1', reset='v=0', method='exact')\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "        stim = SpikeGeneratorGroup(n_in, indices=list(range(n_in)), times=inputs*ms)\n",
    "        S = Synapses(stim, G, '''w:1\n",
    "            layer:1''', on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[:, j]\n",
    "        S.layer = layer_idx\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else float(5.0)\n",
    "        out_times.append(t0)\n",
    "    return np.array(out_times)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "\n",
    "def train_snn_backprop(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1, max_grad=20.0, w_min=-20.0, w_max=20.0\n",
    "):\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    layer1_idx = 1\n",
    "    layer2_idx = 2\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        print(f\"Epoch {ep+1}/{epochs}\")\n",
    "        for xi, yi in zip(X, Y):\n",
    "            h_times = layer_forward(xi, W1, layer1_idx)\n",
    "            o_times = layer_forward(h_times, W2, layer2_idx)\n",
    "            L = 0.5 * np.sum((o_times - yi)**2)\n",
    "            dW2 = np.zeros_like(W2)\n",
    "            delta_o = (o_times - yi)\n",
    "            for k in range(W2.shape[0]):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dW2[k,j] = delta_o[j] * d_spike_timing_dw(\n",
    "                        W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "            delta_h = np.zeros_like(h_times)\n",
    "            for k in range(len(h_times)):\n",
    "                for j in range(W2.shape[1]):\n",
    "                    dt_dw = d_spike_timing_dw(W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                    delta_h[k] += delta_o[j] * W2[k,j] * dt_dw\n",
    "            dW1 = np.zeros_like(W1)\n",
    "            for i in range(W1.shape[0]):\n",
    "                for k in range(W1.shape[1]):\n",
    "                    dW1[i,k] = delta_h[k] * d_spike_timing_dw(\n",
    "                        W1[i,k], xi[i], layer1_idx, 0, 1)\n",
    "            dW1 = np.clip(dW1, -max_grad, max_grad)\n",
    "            dW2 = np.clip(dW2, -max_grad, max_grad)\n",
    "            W1 = np.clip(W1 - lr * dW1, w_min, w_max)\n",
    "            W2 = np.clip(W2 - lr * dW2, w_min, w_max)\n",
    "        print(f\" End Epoch {ep+1}: W1 norm={np.linalg.norm(W1):.3f}, W2 norm={np.linalg.norm(W2):.3f}\\n\")\n",
    "    return W1, W2\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Data preparation for Iris\n",
    "\n",
    "def scale_inputs(X, low=0.05, high=0.095):\n",
    "    # min-max scale each feature to [low, high]\n",
    "    min_vals = X.min(axis=0)\n",
    "    max_vals = X.max(axis=0)\n",
    "    return low + (X - min_vals) * (high - low) / (max_vals - min_vals)\n",
    "\n",
    "\n",
    "def encode_targets(y, class_count=3, spike_low=2.05, spike_high=2.95):\n",
    "    # one-hot-like mapping: for each sample, array of length class_count\n",
    "    Y = []\n",
    "    for label in y:\n",
    "        arr = np.full(class_count, spike_low)\n",
    "        arr[label] = spike_high\n",
    "        Y.append(arr)\n",
    "    return np.array(Y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load and shuffle Iris\n",
    "    iris = datasets.load_iris()\n",
    "    X_raw, y_raw = shuffle(iris.data, iris.target, random_state=42)\n",
    "\n",
    "    # scale inputs and encode targets\n",
    "    X_scaled = scale_inputs(X_raw)\n",
    "    Y_encoded = encode_targets(y_raw)\n",
    "\n",
    "    # split 80/20\n",
    "    split = int(0.8 * len(X_scaled))\n",
    "    X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
    "    Y_train, Y_test = Y_encoded[:split], Y_encoded[split:]\n",
    "\n",
    "    # initialize weights\n",
    "    W1_0 = np.random.randn(X_scaled.shape[1], 10) * 0.1\n",
    "    W2_0 = np.random.randn(10, Y_encoded.shape[1]) * 0.1\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(\n",
    "        X_train, Y_train, W1_0, W2_0,\n",
    "        epochs=20, lr=0.4\n",
    "    )\n",
    "\n",
    "    # evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    for xi, yi_true in zip(X_test, Y_test):\n",
    "        o_times = layer_forward(xi, W2_tr, layer_idx=2)\n",
    "        print(f\"Input: {xi}, Pred times: {o_times}, True times: {yi_true}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85dc5a",
   "metadata": {},
   "source": [
    "\n",
    "        ---     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ccf28a",
   "metadata": {},
   "source": [
    "Old scratch below urd 2x2 model and such"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb70d6",
   "metadata": {},
   "source": [
    "            ---         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421ba054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights - W1 norm: 0.7618281992028296\n",
      "Initial weights - W2 norm: 0.5620876397440523\n",
      "Sample W1 values: [[-0.12467743  0.01261521  0.00491398]\n",
      " [ 0.0071659   0.07718885  0.11401133]]\n",
      "Sample W2 values: [[-0.08962412  0.05624603  0.26515587]\n",
      " [-0.12227563  0.00762061  0.04028084]\n",
      " [ 0.07210636 -0.01546034  0.13874489]]\n",
      "Epoch 1/5\n",
      "  Sample 0: Input: [0.6 0.2 0.4 0.8], Pred: [2.494 2.501 2.514], Target: [2.1 2.9 2.1], Loss: 0.2429\n",
      "  Sample 1: Input: [0.6 0.2 0.4 0.8], Pred: [2.494 2.501 2.514], Target: [2.1 2.9 2.1], Loss: 0.2429\n",
      "  Sample 2: Input: [0.6 0.2 0.4 0.8], Pred: [2.494 2.501 2.514], Target: [2.1 2.9 2.1], Loss: 0.2429\n",
      "  Sample 3: Input: [0.6 0.2 0.4 0.8], Pred: [2.494 2.501 2.514], Target: [2.1 2.9 2.1], Loss: 0.2429\n",
      "  Batch 1: Avg Loss: 0.2429\n",
      "  Sample 4: Input: [0.6 0.2 0.4 0.8], Pred: [2.472 2.522 2.491], Target: [2.1 2.9 2.1], Loss: 0.2171\n",
      "  Sample 5: Input: [0.6 0.2 0.4 0.8], Pred: [2.472 2.522 2.491], Target: [2.1 2.9 2.1], Loss: 0.2171\n",
      "  Sample 6: Input: [0.6 0.2 0.4 0.8], Pred: [2.472 2.522 2.491], Target: [2.1 2.9 2.1], Loss: 0.2171\n",
      "  Sample 7: Input: [0.6 0.2 0.4 0.8], Pred: [2.472 2.522 2.491], Target: [2.1 2.9 2.1], Loss: 0.2171\n",
      "  Batch 2: Avg Loss: 0.2171\n",
      " End Epoch 1: W1 norm=0.762, W2 norm=0.815\n",
      "\n",
      "Epoch 2/5\n",
      "  Sample 0: Input: [0.6 0.2 0.4 0.8], Pred: [2.451 2.543 2.471], Target: [2.1 2.9 2.1], Loss: 0.1941\n",
      "  Sample 1: Input: [0.6 0.2 0.4 0.8], Pred: [2.451 2.543 2.471], Target: [2.1 2.9 2.1], Loss: 0.1941\n",
      "  Sample 2: Input: [0.6 0.2 0.4 0.8], Pred: [2.451 2.543 2.471], Target: [2.1 2.9 2.1], Loss: 0.1941\n",
      "  Sample 3: Input: [0.6 0.2 0.4 0.8], Pred: [2.451 2.543 2.471], Target: [2.1 2.9 2.1], Loss: 0.1941\n",
      "  Batch 1: Avg Loss: 0.1941\n",
      "  Sample 4: Input: [0.6 0.2 0.4 0.8], Pred: [2.432 2.566 2.452], Target: [2.1 2.9 2.1], Loss: 0.1728\n",
      "  Sample 5: Input: [0.6 0.2 0.4 0.8], Pred: [2.432 2.566 2.452], Target: [2.1 2.9 2.1], Loss: 0.1728\n",
      "  Sample 6: Input: [0.6 0.2 0.4 0.8], Pred: [2.432 2.566 2.452], Target: [2.1 2.9 2.1], Loss: 0.1728\n",
      "  Sample 7: Input: [0.6 0.2 0.4 0.8], Pred: [2.432 2.566 2.452], Target: [2.1 2.9 2.1], Loss: 0.1728\n",
      "  Batch 2: Avg Loss: 0.1728\n",
      " End Epoch 2: W1 norm=0.762, W2 norm=1.318\n",
      "\n",
      "Epoch 3/5\n",
      "  Sample 0: Input: [0.6 0.2 0.4 0.8], Pred: [2.414 2.589 2.434], Target: [2.1 2.9 2.1], Loss: 0.1534\n",
      "  Sample 1: Input: [0.6 0.2 0.4 0.8], Pred: [2.414 2.589 2.434], Target: [2.1 2.9 2.1], Loss: 0.1534\n",
      "  Sample 2: Input: [0.6 0.2 0.4 0.8], Pred: [2.414 2.589 2.434], Target: [2.1 2.9 2.1], Loss: 0.1534\n",
      "  Sample 3: Input: [0.6 0.2 0.4 0.8], Pred: [2.414 2.589 2.434], Target: [2.1 2.9 2.1], Loss: 0.1534\n",
      "  Batch 1: Avg Loss: 0.1534\n",
      "  Sample 4: Input: [0.6 0.2 0.4 0.8], Pred: [2.395 2.614 2.416], Target: [2.1 2.9 2.1], Loss: 0.1343\n",
      "  Sample 5: Input: [0.6 0.2 0.4 0.8], Pred: [2.395 2.614 2.416], Target: [2.1 2.9 2.1], Loss: 0.1343\n",
      "  Sample 6: Input: [0.6 0.2 0.4 0.8], Pred: [2.395 2.614 2.416], Target: [2.1 2.9 2.1], Loss: 0.1343\n",
      "  Sample 7: Input: [0.6 0.2 0.4 0.8], Pred: [2.395 2.614 2.416], Target: [2.1 2.9 2.1], Loss: 0.1343\n",
      "  Batch 2: Avg Loss: 0.1343\n",
      " End Epoch 3: W1 norm=0.791, W2 norm=1.845\n",
      "\n",
      "Epoch 4/5\n",
      "  Sample 0: Input: [0.6 0.2 0.4 0.8], Pred: [2.377 2.638 2.398], Target: [2.1 2.9 2.1], Loss: 0.1171\n",
      "  Sample 1: Input: [0.6 0.2 0.4 0.8], Pred: [2.377 2.638 2.398], Target: [2.1 2.9 2.1], Loss: 0.1171\n",
      "  Sample 2: Input: [0.6 0.2 0.4 0.8], Pred: [2.377 2.638 2.398], Target: [2.1 2.9 2.1], Loss: 0.1171\n",
      "  Sample 3: Input: [0.6 0.2 0.4 0.8], Pred: [2.377 2.638 2.398], Target: [2.1 2.9 2.1], Loss: 0.1171\n",
      "  Batch 1: Avg Loss: 0.1171\n",
      "  Sample 4: Input: [0.6 0.2 0.4 0.8], Pred: [2.358 2.661 2.38 ], Target: [2.1 2.9 2.1], Loss: 0.1010\n",
      "  Sample 5: Input: [0.6 0.2 0.4 0.8], Pred: [2.358 2.661 2.38 ], Target: [2.1 2.9 2.1], Loss: 0.1010\n",
      "  Sample 6: Input: [0.6 0.2 0.4 0.8], Pred: [2.358 2.661 2.38 ], Target: [2.1 2.9 2.1], Loss: 0.1010\n",
      "  Sample 7: Input: [0.6 0.2 0.4 0.8], Pred: [2.358 2.661 2.38 ], Target: [2.1 2.9 2.1], Loss: 0.1010\n",
      "  Batch 2: Avg Loss: 0.1010\n",
      " End Epoch 4: W1 norm=0.879, W2 norm=2.354\n",
      "\n",
      "Epoch 5/5\n",
      "  Sample 0: Input: [0.6 0.2 0.4 0.8], Pred: [2.339 2.684 2.361], Target: [2.1 2.9 2.1], Loss: 0.0859\n",
      "  Sample 1: Input: [0.6 0.2 0.4 0.8], Pred: [2.339 2.684 2.361], Target: [2.1 2.9 2.1], Loss: 0.0859\n",
      "  Sample 2: Input: [0.6 0.2 0.4 0.8], Pred: [2.339 2.684 2.361], Target: [2.1 2.9 2.1], Loss: 0.0859\n",
      "  Sample 3: Input: [0.6 0.2 0.4 0.8], Pred: [2.339 2.684 2.361], Target: [2.1 2.9 2.1], Loss: 0.0859\n",
      "  Batch 1: Avg Loss: 0.0859\n",
      "  Sample 4: Input: [0.6 0.2 0.4 0.8], Pred: [2.32  2.706 2.342], Target: [2.1 2.9 2.1], Loss: 0.0723\n",
      "  Sample 5: Input: [0.6 0.2 0.4 0.8], Pred: [2.32  2.706 2.342], Target: [2.1 2.9 2.1], Loss: 0.0723\n",
      "  Sample 6: Input: [0.6 0.2 0.4 0.8], Pred: [2.32  2.706 2.342], Target: [2.1 2.9 2.1], Loss: 0.0723\n",
      "  Sample 7: Input: [0.6 0.2 0.4 0.8], Pred: [2.32  2.706 2.342], Target: [2.1 2.9 2.1], Loss: 0.0723\n",
      "  Batch 2: Avg Loss: 0.0723\n",
      " End Epoch 5: W1 norm=1.040, W2 norm=2.829\n",
      "\n",
      "Trained W1: [[-0.00835384  0.14288309  0.06749924 -0.01145776  0.20918957 -0.04869656\n",
      "   0.07533394  0.2044784   0.26596998 -0.0549491 ]\n",
      " [ 0.11064481  0.24491657  0.19554785 -0.05064137  0.17523084 -0.07021603\n",
      "   0.16702122  0.18697763 -0.00660806 -0.08343643]\n",
      " [ 0.43872639 -0.02602465  0.25838206  0.34359977  0.03167968  0.1627447\n",
      "   0.17943226  0.13726565  0.04545888  0.08550479]\n",
      " [ 0.29184399  0.06814981  0.08245378  0.03232432  0.13532286  0.0288005\n",
      "   0.16052154  0.16000859 -0.12789016 -0.14022609]]\n",
      "Trained W2: [[-0.65619713  0.52065918 -0.22760147]\n",
      " [-0.67244641  0.48125798 -0.48392604]\n",
      " [-0.41272314  0.43578099 -0.36706335]\n",
      " [-0.59923461  0.43998696 -0.58418941]\n",
      " [-0.61823491  0.4854081  -0.51517816]\n",
      " [-0.37697033  0.3251454  -0.65807375]\n",
      " [-0.76686965  0.36005556 -0.67287324]\n",
      " [-0.49064586  0.69848123 -0.57438626]\n",
      " [-0.33798523  0.59417077 -0.43605511]\n",
      " [-0.30853289  0.51195349 -0.36558078]]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Batched forward pass: process multiple samples simultaneously\n",
    "\n",
    "def layer_forward_batched(inputs_batch, W, layer_idx, batch_size):\n",
    "    \"\"\"\n",
    "    inputs_batch: list of arrays, each of shape (n_in,) - spike times from previous layer\n",
    "    W: weight matrix shape (n_in, n_out)\n",
    "    layer_idx: integer layer number\n",
    "    batch_size: number of samples to process\n",
    "    returns: list of arrays, each of shape (n_out,) - output spike times\n",
    "    \"\"\"\n",
    "    n_in, n_out = W.shape\n",
    "    results = []\n",
    "    \n",
    "    # Process each sample individually but collect results for batching\n",
    "    for batch_idx, inputs in enumerate(inputs_batch):\n",
    "        sample_results = []\n",
    "        \n",
    "        for j in range(n_out):\n",
    "            start_scope()\n",
    "            defaultclock.dt = 0.0001*ms\n",
    "            \n",
    "            # Create single neuron for this output\n",
    "            G = NeuronGroup(1, '''\n",
    "                v : 1\n",
    "                sum : 1\n",
    "                sr : 1\n",
    "                scheduled_time : second\n",
    "                global_clock : 1\n",
    "            ''', threshold='v>1', reset='v=0', method='exact')\n",
    "            \n",
    "            G.v = G.sum = G.sr = 0\n",
    "            G.global_clock = 0\n",
    "            G.scheduled_time = 1e9*second\n",
    "            \n",
    "            # Create spike inputs for this sample\n",
    "            stim = SpikeGeneratorGroup(n_in, indices=list(range(n_in)), times=inputs*ms)\n",
    "            \n",
    "            # Create synapses\n",
    "            S = Synapses(stim, G, '''\n",
    "                w : 1\n",
    "                layer : 1\n",
    "            ''', on_pre='''\n",
    "                sr += 1\n",
    "                sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "                scheduled_time = (sum/sr + layer)*ms\n",
    "            ''')\n",
    "            \n",
    "            S.connect(True)\n",
    "            S.w = W[:, j]\n",
    "            S.layer = layer_idx\n",
    "            \n",
    "            # Update global clock and voltage\n",
    "            G.run_regularly('''\n",
    "                v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "                global_clock += 0.001\n",
    "            ''', dt=0.001*ms)\n",
    "            \n",
    "            # Monitor spikes\n",
    "            mon = SpikeMonitor(G)\n",
    "            \n",
    "            # Run simulation\n",
    "            run(5*ms)\n",
    "            \n",
    "            # Extract spike time\n",
    "            ts = mon.spike_trains()[0]\n",
    "            t0 = float(ts[0]/ms) if len(ts) > 0 else float(5.0)\n",
    "            sample_results.append(t0)\n",
    "        \n",
    "        results.append(np.array(sample_results))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Batched training loop\n",
    "\n",
    "def train_snn_backprop_batched(\n",
    "    X, Y,        # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1, max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    batch_size=4\n",
    "):\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    layer1_idx = 1\n",
    "    layer2_idx = 2\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        print(f\"Epoch {ep+1}/{epochs}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, n_samples, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, n_samples)\n",
    "            current_batch_size = batch_end - batch_start\n",
    "            \n",
    "            # Get batch data\n",
    "            X_batch = X[batch_start:batch_end]\n",
    "            Y_batch = Y[batch_start:batch_end]\n",
    "            \n",
    "            # Forward pass through both layers\n",
    "            h_times_batch = layer_forward_batched(X_batch, W1, layer1_idx, current_batch_size)\n",
    "            o_times_batch = layer_forward_batched(h_times_batch, W2, layer2_idx, current_batch_size)\n",
    "            \n",
    "            # Accumulate gradients for this batch\n",
    "            dW1_batch = np.zeros_like(W1)\n",
    "            dW2_batch = np.zeros_like(W2)\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for i, (xi, yi, h_times, o_times) in enumerate(zip(X_batch, Y_batch, h_times_batch, o_times_batch)):\n",
    "                # Loss for this sample\n",
    "                L = 0.5 * np.sum((o_times - yi)**2)\n",
    "                total_loss += L\n",
    "                \n",
    "                # Gradients for W2\n",
    "                dW2_sample = np.zeros_like(W2)\n",
    "                delta_o = (o_times - yi)\n",
    "                \n",
    "                for k in range(W2.shape[0]):\n",
    "                    for j in range(W2.shape[1]):\n",
    "                        dW2_sample[k,j] = delta_o[j] * d_spike_timing_dw(\n",
    "                            W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                \n",
    "                # Hidden deltas\n",
    "                delta_h = np.zeros_like(h_times)\n",
    "                for k in range(len(h_times)):\n",
    "                    for j in range(W2.shape[1]):\n",
    "                        dt_dw = d_spike_timing_dw(W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                        delta_h[k] += delta_o[j] * W2[k,j] * dt_dw\n",
    "                \n",
    "                # Gradients for W1\n",
    "                dW1_sample = np.zeros_like(W1)\n",
    "                for ii in range(W1.shape[0]):\n",
    "                    for k in range(W1.shape[1]):\n",
    "                        dW1_sample[ii,k] = delta_h[k] * d_spike_timing_dw(\n",
    "                            W1[ii,k], xi[ii], layer1_idx, 0, 1)\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                dW1_batch += dW1_sample\n",
    "                dW2_batch += dW2_sample\n",
    "                \n",
    "                # Print individual sample results\n",
    "                print(f\"  Sample {batch_start+i}: Input: {xi}, Pred: {o_times}, Target: {yi}, Loss: {L:.4f}\")\n",
    "            \n",
    "            # Average gradients over batch\n",
    "            dW1_batch /= current_batch_size\n",
    "            dW2_batch /= current_batch_size\n",
    "            \n",
    "            # Clip and update weights\n",
    "            dW1_batch = np.clip(dW1_batch, -max_grad, max_grad)\n",
    "            dW2_batch = np.clip(dW2_batch, -max_grad, max_grad)\n",
    "            W1 = np.clip(W1 - lr * dW1_batch, w_min, w_max)\n",
    "            W2 = np.clip(W2 - lr * dW2_batch, w_min, w_max)\n",
    "            \n",
    "            avg_loss = total_loss / current_batch_size\n",
    "            print(f\"  Batch {batch_start//batch_size + 1}: Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(f\" End Epoch {ep+1}: W1 norm={np.linalg.norm(W1):.3f}, W2 norm={np.linalg.norm(W2):.3f}\\n\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # example usage with fixed input/target pairs\n",
    "    # 4 inputs per sample, constant across 8 samples\n",
    "    X = [np.array([0.6, 0.2, 0.4, 0.8]) for _ in range(8)]\n",
    "    # 3-targets (network outputs 3 values): use desired spike times [2.1, 2.0, 1.0]\n",
    "    Y = [np.array([2.1, 2.9, 2.1]) for _ in range(8)]\n",
    "    # initialize weights\n",
    "    W1_0 = np.random.randn(4, 10) * 0.1\n",
    "    W2_0 = np.random.randn(10, 3) * 0.1\n",
    "    \n",
    "    print(\"Initial weights - W1 norm:\", np.linalg.norm(W1_0))\n",
    "    print(\"Initial weights - W2 norm:\", np.linalg.norm(W2_0))\n",
    "    print(\"Sample W1 values:\", W1_0[:2, :3])\n",
    "    print(\"Sample W2 values:\", W2_0[:3, :])\n",
    "    \n",
    "    # train with batching\n",
    "    W1_tr, W2_tr = train_snn_backprop_batched(X, Y, W1_0, W2_0,\n",
    "                                             epochs=5, lr=0.4, batch_size=4)\n",
    "    print(\"Trained W1:\", W1_tr)\n",
    "    print(\"Trained W2:\", W2_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449a7dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing Iris dataset...\n",
      "Original dataset shape: (150, 4)\n",
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "Target distribution: [50 50 50]\n",
      "Training set size: 120\n",
      "Test set size: 30\n",
      "Input range: [0.050, 0.950]\n",
      "\n",
      "Initial weights - W1 shape: (4, 10), W2 shape: (10, 3)\n",
      "Training samples: 120\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.202 2.184 2.188], Target: [2. 3. 2.], Loss: 0.3710\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.452 2.429 2.435], Target: [3. 2. 2.], Loss: 0.3368\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.576 2.553 2.559], Target: [2. 2. 3.], Loss: 0.4160\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.247 2.228 2.233], Target: [2. 3. 2.], Loss: 0.3556\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.435 2.412 2.418], Target: [2. 2. 3.], Loss: 0.3488\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.571 2.548 2.555], Target: [3. 2. 2.], Loss: 0.3962\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.466 2.443 2.449], Target: [2. 2. 3.], Loss: 0.3585\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.567 2.544 2.55 ], Target: [3. 2. 2.], Loss: 0.3930\n",
      "  Batch 1: Avg Loss: 0.3720\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.769 2.745 2.757], Target: [3. 2. 2.], Loss: 0.5907\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.851 2.831 2.842], Target: [3. 2. 2.], Loss: 0.7109\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.764 2.739 2.752], Target: [3. 2. 2.], Loss: 0.5837\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.534 2.504 2.518], Target: [2. 2. 3.], Loss: 0.3857\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.308 2.282 2.293], Target: [2. 2. 3.], Loss: 0.3371\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.445 2.417 2.429], Target: [2. 2. 3.], Loss: 0.3490\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.451 2.422 2.435], Target: [2. 2. 3.], Loss: 0.3504\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.255 2.232 2.242], Target: [2. 3. 2.], Loss: 0.3567\n",
      "  Batch 2: Avg Loss: 0.4580\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.334 2.303 2.328], Target: [2. 3. 2.], Loss: 0.3525\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.605 2.569 2.598], Target: [3. 2. 2.], Loss: 0.4187\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.63  2.594 2.623], Target: [3. 2. 2.], Loss: 0.4389\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.326 2.295 2.32 ], Target: [2. 3. 2.], Loss: 0.3529\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.522 2.486 2.515], Target: [2. 2. 3.], Loss: 0.3720\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.328 2.298 2.322], Target: [2. 3. 2.], Loss: 0.3520\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.83  2.802 2.825], Target: [3. 2. 2.], Loss: 0.6764\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.249 2.223 2.244], Target: [2. 3. 2.], Loss: 0.3626\n",
      "  Batch 3: Avg Loss: 0.4157\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.373 2.344 2.36 ], Target: [2. 2. 3.], Loss: 0.3335\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.606 2.572 2.591], Target: [3. 2. 2.], Loss: 0.4159\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.592 2.558 2.577], Target: [3. 2. 2.], Loss: 0.4054\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.301 2.275 2.289], Target: [2. 3. 2.], Loss: 0.3499\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.548 2.515 2.533], Target: [3. 2. 2.], Loss: 0.3768\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.289 2.264 2.278], Target: [2. 3. 2.], Loss: 0.3513\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.33  2.303 2.318], Target: [2. 3. 2.], Loss: 0.3479\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.483 2.451 2.469], Target: [2. 2. 3.], Loss: 0.3593\n",
      "  Batch 4: Avg Loss: 0.3675\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.53  2.495 2.51 ], Target: [2. 2. 3.], Loss: 0.3830\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.427 2.395 2.409], Target: [2. 3. 2.], Loss: 0.3578\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.677 2.64  2.657], Target: [3. 2. 2.], Loss: 0.4728\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.68  2.644 2.661], Target: [3. 2. 2.], Loss: 0.4770\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.394 2.363 2.377], Target: [2. 2. 3.], Loss: 0.3376\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.423 2.391 2.405], Target: [2. 2. 3.], Loss: 0.3429\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.73  2.695 2.712], Target: [3. 2. 2.], Loss: 0.5314\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.578 2.542 2.558], Target: [2. 2. 3.], Loss: 0.4116\n",
      "  Batch 5: Avg Loss: 0.4143\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.25  2.222 2.243], Target: [2. 3. 2.], Loss: 0.3634\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.597 2.549 2.582], Target: [2. 2. 3.], Loss: 0.4163\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.35  2.313 2.339], Target: [2. 3. 2.], Loss: 0.3547\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.682 2.633 2.667], Target: [3. 2. 2.], Loss: 0.4734\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.224 2.197 2.217], Target: [2. 3. 2.], Loss: 0.3710\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.291 2.258 2.282], Target: [2. 3. 2.], Loss: 0.3574\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.541 2.494 2.527], Target: [3. 2. 2.], Loss: 0.3662\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.24  2.213 2.233], Target: [2. 3. 2.], Loss: 0.3656\n",
      "  Batch 6: Avg Loss: 0.3835\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.136 2.124 2.131], Target: [2. 3. 2.], Loss: 0.4015\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.204 2.186 2.196], Target: [2. 3. 2.], Loss: 0.3713\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.224 2.205 2.215], Target: [2. 3. 2.], Loss: 0.3642\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.434 2.401 2.418], Target: [2. 2. 3.], Loss: 0.3439\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.709 2.669 2.688], Target: [3. 2. 2.], Loss: 0.5028\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.464 2.43  2.447], Target: [2. 2. 3.], Loss: 0.3530\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.277 2.254 2.266], Target: [2. 3. 2.], Loss: 0.3520\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.561 2.522 2.541], Target: [3. 2. 2.], Loss: 0.3789\n",
      "  Batch 7: Avg Loss: 0.3835\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.447 2.415 2.43 ], Target: [2. 2. 3.], Loss: 0.3485\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.69  2.652 2.669], Target: [3. 2. 2.], Loss: 0.4844\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.216 2.201 2.208], Target: [2. 3. 2.], Loss: 0.3642\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.533 2.498 2.514], Target: [3. 2. 2.], Loss: 0.3651\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.294 2.272 2.283], Target: [2. 3. 2.], Loss: 0.3483\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.385 2.357 2.37 ], Target: [2. 2. 3.], Loss: 0.3363\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.514 2.48  2.496], Target: [3. 2. 2.], Loss: 0.3563\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.265 2.246 2.255], Target: [2. 3. 2.], Loss: 0.3519\n",
      "  Batch 8: Avg Loss: 0.3694\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.518 2.479 2.494], Target: [2. 2. 3.], Loss: 0.3769\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.418 2.385 2.397], Target: [2. 2. 3.], Loss: 0.3433\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.725 2.681 2.698], Target: [3. 2. 2.], Loss: 0.5133\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.49  2.453 2.467], Target: [2. 2. 3.], Loss: 0.3647\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.529 2.49  2.505], Target: [2. 2. 3.], Loss: 0.3825\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.657 2.614 2.631], Target: [3. 2. 2.], Loss: 0.4464\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.268 2.246 2.254], Target: [2. 3. 2.], Loss: 0.3524\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.295 2.271 2.28 ], Target: [2. 3. 2.], Loss: 0.3484\n",
      "  Batch 9: Avg Loss: 0.3910\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.27  2.246 2.264], Target: [2. 3. 2.], Loss: 0.3556\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.451 2.412 2.439], Target: [3. 2. 2.], Loss: 0.3319\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.419 2.382 2.408], Target: [2. 2. 3.], Loss: 0.3360\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.634 2.586 2.618], Target: [3. 2. 2.], Loss: 0.4296\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.362 2.329 2.352], Target: [2. 2. 3.], Loss: 0.3296\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.645 2.597 2.629], Target: [3. 2. 2.], Loss: 0.4390\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.537 2.493 2.523], Target: [3. 2. 2.], Loss: 0.3655\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.55  2.505 2.536], Target: [2. 2. 3.], Loss: 0.3864\n",
      "  Batch 10: Avg Loss: 0.3717\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.225 2.194 2.217], Target: [2. 3. 2.], Loss: 0.3737\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.581 2.515 2.559], Target: [3. 2. 2.], Loss: 0.3766\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.447 2.39  2.429], Target: [2. 2. 3.], Loss: 0.3390\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.307 2.265 2.295], Target: [2. 3. 2.], Loss: 0.3607\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.697 2.626 2.673], Target: [3. 2. 2.], Loss: 0.4683\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.321 2.278 2.308], Target: [2. 3. 2.], Loss: 0.3596\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.786 2.717 2.763], Target: [3. 2. 2.], Loss: 0.5710\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.32  2.277 2.308], Target: [2. 2. 3.], Loss: 0.3290\n",
      "  Batch 11: Avg Loss: 0.3972\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.564 2.493 2.536], Target: [2. 2. 3.], Loss: 0.3882\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.222 2.189 2.21 ], Target: [2. 3. 2.], Loss: 0.3756\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.485 2.42  2.459], Target: [2. 2. 3.], Loss: 0.3522\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.621 2.547 2.591], Target: [3. 2. 2.], Loss: 0.3961\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.33  2.283 2.312], Target: [2. 3. 2.], Loss: 0.3602\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.153 2.131 2.146], Target: [2. 3. 2.], Loss: 0.3999\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.674 2.597 2.643], Target: [3. 2. 2.], Loss: 0.4381\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.735 2.658 2.705], Target: [3. 2. 2.], Loss: 0.5001\n",
      "  Batch 12: Avg Loss: 0.4013\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.643 2.558 2.607], Target: [3. 2. 2.], Loss: 0.4036\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.438 2.37  2.409], Target: [2. 2. 3.], Loss: 0.3390\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.537 2.458 2.503], Target: [3. 2. 2.], Loss: 0.3386\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.348 2.293 2.324], Target: [2. 3. 2.], Loss: 0.3630\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.695 2.608 2.658], Target: [3. 2. 2.], Loss: 0.4478\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.45  2.381 2.42 ], Target: [2. 2. 3.], Loss: 0.3420\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.677 2.59  2.64 ], Target: [3. 2. 2.], Loss: 0.4310\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.676 2.59  2.639], Target: [3. 2. 2.], Loss: 0.4307\n",
      "  Batch 13: Avg Loss: 0.3870\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.303 2.235 2.27 ], Target: [2. 3. 2.], Loss: 0.3750\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.5   2.399 2.45 ], Target: [2. 2. 3.], Loss: 0.3559\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.48  2.381 2.431], Target: [2. 2. 3.], Loss: 0.3497\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.456 2.361 2.409], Target: [2. 2. 3.], Loss: 0.3438\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.455 2.36  2.408], Target: [2. 2. 3.], Loss: 0.3435\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.548 2.441 2.495], Target: [2. 2. 3.], Loss: 0.3749\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.28  2.217 2.249], Target: [2. 3. 2.], Loss: 0.3767\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.611 2.497 2.555], Target: [3. 2. 2.], Loss: 0.3532\n",
      "  Batch 14: Avg Loss: 0.3591\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.365 2.285 2.344], Target: [2. 2. 3.], Loss: 0.3224\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.483 2.383 2.456], Target: [2. 2. 3.], Loss: 0.3380\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.243 2.188 2.23 ], Target: [2. 3. 2.], Loss: 0.3856\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.25  2.193 2.236], Target: [2. 3. 2.], Loss: 0.3847\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.209 2.161 2.199], Target: [2. 3. 2.], Loss: 0.3936\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.345 2.269 2.326], Target: [2. 3. 2.], Loss: 0.3798\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.575 2.463 2.545], Target: [2. 2. 3.], Loss: 0.3760\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.202 2.156 2.192], Target: [2. 3. 2.], Loss: 0.3950\n",
      "  Batch 15: Avg Loss: 0.3719\n",
      " End Epoch 1: W1 norm=0.748, W2 norm=1.331\n",
      "\n",
      "Epoch 2/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.164 2.134 2.163], Target: [2. 3. 2.], Loss: 0.4017\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.396 2.327 2.387], Target: [3. 2. 2.], Loss: 0.3108\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.517 2.433 2.506], Target: [2. 2. 3.], Loss: 0.3494\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.204 2.168 2.202], Target: [2. 3. 2.], Loss: 0.3873\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.379 2.313 2.372], Target: [2. 2. 3.], Loss: 0.3180\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.512 2.429 2.501], Target: [3. 2. 2.], Loss: 0.3366\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.41  2.339 2.401], Target: [2. 2. 3.], Loss: 0.3209\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.508 2.425 2.497], Target: [3. 2. 2.], Loss: 0.3348\n",
      "  Batch 1: Avg Loss: 0.3449\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.721 2.608 2.706], Target: [3. 2. 2.], Loss: 0.4730\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.812 2.702 2.798], Target: [3. 2. 2.], Loss: 0.5825\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.716 2.603 2.701], Target: [3. 2. 2.], Loss: 0.4678\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.48  2.388 2.469], Target: [2. 2. 3.], Loss: 0.3315\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.263 2.208 2.259], Target: [2. 2. 3.], Loss: 0.3308\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.393 2.315 2.385], Target: [2. 2. 3.], Loss: 0.3159\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.398 2.319 2.39 ], Target: [2. 2. 3.], Loss: 0.3161\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.215 2.17  2.212], Target: [2. 3. 2.], Loss: 0.3900\n",
      "  Batch 2: Avg Loss: 0.4010\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.292 2.218 2.289], Target: [2. 3. 2.], Loss: 0.3902\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.555 2.429 2.545], Target: [3. 2. 2.], Loss: 0.3395\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.581 2.451 2.57 ], Target: [3. 2. 2.], Loss: 0.3519\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.285 2.212 2.282], Target: [2. 3. 2.], Loss: 0.3908\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.473 2.361 2.465], Target: [2. 2. 3.], Loss: 0.3201\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.286 2.213 2.283], Target: [2. 3. 2.], Loss: 0.3906\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.79  2.648 2.778], Target: [3. 2. 2.], Loss: 0.5346\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.215 2.159 2.214], Target: [2. 3. 2.], Loss: 0.3997\n",
      "  Batch 3: Avg Loss: 0.3897\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.334 2.249 2.32 ], Target: [2. 2. 3.], Loss: 0.3180\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.561 2.431 2.537], Target: [3. 2. 2.], Loss: 0.3334\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.546 2.419 2.523], Target: [3. 2. 2.], Loss: 0.3276\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.266 2.197 2.255], Target: [2. 3. 2.], Loss: 0.3903\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.503 2.383 2.481], Target: [3. 2. 2.], Loss: 0.3125\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.254 2.188 2.244], Target: [2. 3. 2.], Loss: 0.3917\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.292 2.217 2.28 ], Target: [2. 3. 2.], Loss: 0.3884\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.44  2.332 2.421], Target: [2. 2. 3.], Loss: 0.3195\n",
      "  Batch 4: Avg Loss: 0.3477\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.489 2.367 2.46 ], Target: [2. 2. 3.], Loss: 0.3327\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.386 2.286 2.362], Target: [2. 3. 2.], Loss: 0.3949\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.636 2.49  2.602], Target: [3. 2. 2.], Loss: 0.3675\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.64  2.493 2.605], Target: [3. 2. 2.], Loss: 0.3693\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.358 2.264 2.336], Target: [2. 2. 3.], Loss: 0.3194\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.386 2.286 2.362], Target: [2. 2. 3.], Loss: 0.3189\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.69  2.539 2.656], Target: [3. 2. 2.], Loss: 0.4085\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.537 2.406 2.506], Target: [2. 2. 3.], Loss: 0.3486\n",
      "  Batch 5: Avg Loss: 0.3575\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.227 2.155 2.215], Target: [2. 3. 2.], Loss: 0.4059\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.559 2.401 2.531], Target: [2. 2. 3.], Loss: 0.3466\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.318 2.22  2.301], Target: [2. 3. 2.], Loss: 0.4001\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.643 2.469 2.612], Target: [3. 2. 2.], Loss: 0.3610\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.203 2.138 2.193], Target: [2. 3. 2.], Loss: 0.4108\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.264 2.181 2.25 ], Target: [2. 3. 2.], Loss: 0.4015\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.504 2.357 2.478], Target: [3. 2. 2.], Loss: 0.3010\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.218 2.149 2.207], Target: [2. 3. 2.], Loss: 0.4073\n",
      "  Batch 6: Avg Loss: 0.3793\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.125 2.089 2.117], Target: [2. 3. 2.], Loss: 0.4296\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.186 2.133 2.174], Target: [2. 3. 2.], Loss: 0.4083\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.204 2.146 2.191], Target: [2. 3. 2.], Loss: 0.4037\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.403 2.291 2.376], Target: [2. 2. 3.], Loss: 0.3182\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.671 2.506 2.633], Target: [3. 2. 2.], Loss: 0.3825\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.432 2.312 2.402], Target: [2. 2. 3.], Loss: 0.3208\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.252 2.18  2.235], Target: [2. 3. 2.], Loss: 0.3956\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.524 2.384 2.491], Target: [3. 2. 2.], Loss: 0.3076\n",
      "  Batch 7: Avg Loss: 0.3708\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.416 2.301 2.387], Target: [2. 2. 3.], Loss: 0.3197\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.654 2.491 2.615], Target: [3. 2. 2.], Loss: 0.3695\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.199 2.144 2.185], Target: [2. 3. 2.], Loss: 0.4033\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.499 2.365 2.465], Target: [3. 2. 2.], Loss: 0.3002\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.27  2.194 2.251], Target: [2. 3. 2.], Loss: 0.3928\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.357 2.258 2.332], Target: [2. 2. 3.], Loss: 0.3201\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.48  2.35  2.447], Target: [3. 2. 2.], Loss: 0.2964\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.244 2.176 2.226], Target: [2. 3. 2.], Loss: 0.3948\n",
      "  Batch 8: Avg Loss: 0.3496\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.488 2.349 2.446], Target: [2. 2. 3.], Loss: 0.3334\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.392 2.278 2.357], Target: [2. 2. 3.], Loss: 0.3222\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.691 2.513 2.642], Target: [3. 2. 2.], Loss: 0.3854\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.461 2.329 2.422], Target: [2. 2. 3.], Loss: 0.3274\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.499 2.358 2.457], Target: [2. 2. 3.], Loss: 0.3360\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.624 2.457 2.577], Target: [3. 2. 2.], Loss: 0.3416\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.249 2.176 2.226], Target: [2. 3. 2.], Loss: 0.3960\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.275 2.194 2.25 ], Target: [2. 3. 2.], Loss: 0.3939\n",
      "  Batch 9: Avg Loss: 0.3545\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.251 2.172 2.236], Target: [2. 3. 2.], Loss: 0.4021\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.424 2.292 2.398], Target: [3. 2. 2.], Loss: 0.2877\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.394 2.271 2.37 ], Target: [2. 2. 3.], Loss: 0.3128\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.601 2.424 2.568], Target: [3. 2. 2.], Loss: 0.3308\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.34  2.232 2.319], Target: [2. 2. 3.], Loss: 0.3166\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.613 2.432 2.579], Target: [3. 2. 2.], Loss: 0.3358\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.508 2.352 2.478], Target: [3. 2. 2.], Loss: 0.2972\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.52  2.362 2.489], Target: [2. 2. 3.], Loss: 0.3313\n",
      "  Batch 10: Avg Loss: 0.3268\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.215 2.134 2.198], Target: [2. 3. 2.], Loss: 0.4177\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.553 2.355 2.513], Target: [3. 2. 2.], Loss: 0.2945\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.423 2.267 2.39 ], Target: [2. 2. 3.], Loss: 0.3112\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.29  2.181 2.267], Target: [2. 3. 2.], Loss: 0.4131\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.668 2.439 2.624], Target: [3. 2. 2.], Loss: 0.3462\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.3   2.189 2.275], Target: [2. 3. 2.], Loss: 0.4117\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.758 2.51  2.714], Target: [3. 2. 2.], Loss: 0.4142\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.305 2.19  2.28 ], Target: [2. 2. 3.], Loss: 0.3238\n",
      "  Batch 11: Avg Loss: 0.3665\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.54  2.334 2.49 ], Target: [2. 2. 3.], Loss: 0.3316\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.213 2.129 2.191], Target: [2. 3. 2.], Loss: 0.4202\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.463 2.283 2.418], Target: [2. 2. 3.], Loss: 0.3166\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.595 2.371 2.542], Target: [3. 2. 2.], Loss: 0.2977\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.313 2.191 2.281], Target: [2. 3. 2.], Loss: 0.4157\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.148 2.091 2.133], Target: [2. 3. 2.], Loss: 0.4329\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.647 2.407 2.592], Target: [3. 2. 2.], Loss: 0.3204\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.706 2.449 2.651], Target: [3. 2. 2.], Loss: 0.3559\n",
      "  Batch 12: Avg Loss: 0.3614\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.619 2.364 2.556], Target: [3. 2. 2.], Loss: 0.2934\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.419 2.242 2.371], Target: [2. 2. 3.], Loss: 0.3149\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.515 2.299 2.458], Target: [3. 2. 2.], Loss: 0.2672\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.331 2.192 2.291], Target: [2. 3. 2.], Loss: 0.4236\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.67  2.397 2.606], Target: [3. 2. 2.], Loss: 0.3169\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.432 2.249 2.382], Target: [2. 2. 3.], Loss: 0.3153\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.652 2.386 2.588], Target: [3. 2. 2.], Loss: 0.3079\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.651 2.385 2.588], Target: [3. 2. 2.], Loss: 0.3079\n",
      "  Batch 13: Avg Loss: 0.3184\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.293 2.145 2.242], Target: [2. 3. 2.], Loss: 0.4377\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.486 2.238 2.409], Target: [2. 2. 3.], Loss: 0.3211\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.466 2.227 2.391], Target: [2. 2. 3.], Loss: 0.3198\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.443 2.216 2.371], Target: [2. 2. 3.], Loss: 0.3193\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.442 2.216 2.37 ], Target: [2. 2. 3.], Loss: 0.3195\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.531 2.261 2.45 ], Target: [2. 2. 3.], Loss: 0.3263\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.272 2.134 2.224], Target: [2. 3. 2.], Loss: 0.4371\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.592 2.29  2.506], Target: [3. 2. 2.], Loss: 0.2533\n",
      "  Batch 14: Avg Loss: 0.3417\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.354 2.169 2.318], Target: [2. 2. 3.], Loss: 0.3095\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.467 2.222 2.421], Target: [2. 2. 3.], Loss: 0.3013\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.237 2.116 2.212], Target: [2. 3. 2.], Loss: 0.4413\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.241 2.119 2.216], Target: [2. 3. 2.], Loss: 0.4404\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.205 2.1   2.184], Target: [2. 3. 2.], Loss: 0.4429\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.327 2.164 2.293], Target: [2. 3. 2.], Loss: 0.4458\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.555 2.265 2.503], Target: [2. 2. 3.], Loss: 0.3126\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.199 2.097 2.179], Target: [2. 3. 2.], Loss: 0.4435\n",
      "  Batch 15: Avg Loss: 0.3922\n",
      " End Epoch 2: W1 norm=1.026, W2 norm=2.438\n",
      "\n",
      "Epoch 3/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.16  2.092 2.153], Target: [2. 3. 2.], Loss: 0.4367\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.379 2.209 2.358], Target: [3. 2. 2.], Loss: 0.2787\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.495 2.272 2.468], Target: [2. 2. 3.], Loss: 0.3010\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.198 2.113 2.188], Target: [2. 3. 2.], Loss: 0.4307\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.364 2.201 2.344], Target: [2. 2. 3.], Loss: 0.3016\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.491 2.268 2.464], Target: [3. 2. 2.], Loss: 0.2731\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.393 2.217 2.371], Target: [2. 2. 3.], Loss: 0.2986\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.487 2.267 2.46 ], Target: [3. 2. 2.], Loss: 0.2730\n",
      "  Batch 1: Avg Loss: 0.3242\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.696 2.342 2.664], Target: [3. 2. 2.], Loss: 0.3251\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.788 2.379 2.755], Target: [3. 2. 2.], Loss: 0.3793\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.691 2.34  2.658], Target: [3. 2. 2.], Loss: 0.3220\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.461 2.238 2.436], Target: [2. 2. 3.], Loss: 0.2936\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.255 2.135 2.241], Target: [2. 2. 3.], Loss: 0.3297\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.38  2.197 2.359], Target: [2. 2. 3.], Loss: 0.2970\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.384 2.199 2.363], Target: [2. 2. 3.], Loss: 0.2964\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.209 2.113 2.198], Target: [2. 3. 2.], Loss: 0.4348\n",
      "  Batch 2: Avg Loss: 0.3348\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.28  2.125 2.266], Target: [2. 3. 2.], Loss: 0.4574\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.537 2.189 2.51 ], Target: [3. 2. 2.], Loss: 0.2551\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.561 2.2   2.534], Target: [3. 2. 2.], Loss: 0.2589\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.276 2.119 2.263], Target: [2. 3. 2.], Loss: 0.4608\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.459 2.172 2.436], Target: [2. 2. 3.], Loss: 0.2792\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.274 2.123 2.261], Target: [2. 3. 2.], Loss: 0.4562\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.767 2.2   2.735], Target: [3. 2. 2.], Loss: 0.3173\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.213 2.09  2.204], Target: [2. 3. 2.], Loss: 0.4575\n",
      "  Batch 3: Avg Loss: 0.3678\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.328 2.135 2.3  ], Target: [2. 2. 3.], Loss: 0.3079\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.543 2.198 2.499], Target: [3. 2. 2.], Loss: 0.2485\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.529 2.192 2.486], Target: [3. 2. 2.], Loss: 0.2475\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.258 2.12  2.235], Target: [2. 3. 2.], Loss: 0.4481\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.487 2.187 2.446], Target: [3. 2. 2.], Loss: 0.2485\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.246 2.116 2.224], Target: [2. 3. 2.], Loss: 0.4461\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.28  2.132 2.256], Target: [2. 3. 2.], Loss: 0.4487\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.427 2.172 2.39 ], Target: [2. 2. 3.], Loss: 0.2920\n",
      "  Batch 4: Avg Loss: 0.3359\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.474 2.188 2.425], Target: [2. 2. 3.], Loss: 0.2953\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.362 2.18  2.323], Target: [2. 3. 2.], Loss: 0.4539\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.617 2.201 2.559], Target: [3. 2. 2.], Loss: 0.2498\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.618 2.215 2.56 ], Target: [3. 2. 2.], Loss: 0.2529\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.351 2.146 2.314], Target: [2. 2. 3.], Loss: 0.3076\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.377 2.156 2.337], Target: [2. 2. 3.], Loss: 0.3030\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.669 2.197 2.61 ], Target: [3. 2. 2.], Loss: 0.2602\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.519 2.201 2.467], Target: [2. 2. 3.], Loss: 0.2969\n",
      "  Batch 5: Avg Loss: 0.3025\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.223 2.087 2.202], Target: [2. 3. 2.], Loss: 0.4621\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.543 2.104 2.495], Target: [2. 2. 3.], Loss: 0.2803\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.304 2.123 2.275], Target: [2. 3. 2.], Loss: 0.4686\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.63  1.94  2.578], Target: [3. 2. 2.], Loss: 0.2373\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.205 2.071 2.186], Target: [2. 3. 2.], Loss: 0.4698\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.257 2.099 2.233], Target: [2. 3. 2.], Loss: 0.4661\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.495 2.071 2.45 ], Target: [3. 2. 2.], Loss: 0.2313\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.216 2.08  2.196], Target: [2. 3. 2.], Loss: 0.4657\n",
      "  Batch 6: Avg Loss: 0.3851\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.129 2.069 2.115], Target: [2. 3. 2.], Loss: 0.4483\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.182 2.102 2.161], Target: [2. 3. 2.], Loss: 0.4327\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.2   2.11  2.177], Target: [2. 3. 2.], Loss: 0.4317\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.388 2.185 2.344], Target: [2. 2. 3.], Loss: 0.3076\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.647 2.231 2.584], Target: [3. 2. 2.], Loss: 0.2595\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.416 2.193 2.369], Target: [2. 2. 3.], Loss: 0.3042\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.24  2.135 2.213], Target: [2. 3. 2.], Loss: 0.4256\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.504 2.219 2.45 ], Target: [3. 2. 2.], Loss: 0.2482\n",
      "  Batch 7: Avg Loss: 0.3572\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.401 2.194 2.355], Target: [2. 2. 3.], Loss: 0.3072\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.629 2.24  2.565], Target: [3. 2. 2.], Loss: 0.2572\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.193 2.117 2.17 ], Target: [2. 3. 2.], Loss: 0.4229\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.481 2.21  2.428], Target: [3. 2. 2.], Loss: 0.2483\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.257 2.151 2.226], Target: [2. 3. 2.], Loss: 0.4190\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.347 2.168 2.306], Target: [2. 2. 3.], Loss: 0.3151\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.464 2.201 2.412], Target: [3. 2. 2.], Loss: 0.2487\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.233 2.139 2.205], Target: [2. 3. 2.], Loss: 0.4188\n",
      "  Batch 8: Avg Loss: 0.3297\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.474 2.182 2.412], Target: [2. 2. 3.], Loss: 0.3018\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.383 2.167 2.331], Target: [2. 2. 3.], Loss: 0.3111\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.675 2.061 2.6  ], Target: [3. 2. 2.], Loss: 0.2347\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.446 2.194 2.387], Target: [2. 2. 3.], Loss: 0.3062\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.484 2.189 2.421], Target: [2. 2. 3.], Loss: 0.3026\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.604 2.193 2.531], Target: [3. 2. 2.], Loss: 0.2380\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.24  2.139 2.206], Target: [2. 3. 2.], Loss: 0.4207\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.265 2.148 2.228], Target: [2. 3. 2.], Loss: 0.4241\n",
      "  Batch 9: Avg Loss: 0.3174\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.241 2.135 2.216], Target: [2. 3. 2.], Loss: 0.4265\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.426 2.    2.385], Target: [3. 2. 2.], Loss: 0.2389\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.388 2.117 2.349], Target: [2. 2. 3.], Loss: 0.2940\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.586 2.042 2.532], Target: [3. 2. 2.], Loss: 0.2281\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.341 2.089 2.307], Target: [2. 2. 3.], Loss: 0.3022\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.598 2.006 2.543], Target: [3. 2. 2.], Loss: 0.2282\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.498 2.068 2.45 ], Target: [3. 2. 2.], Loss: 0.2296\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.5   2.164 2.451], Target: [2. 2. 3.], Loss: 0.2891\n",
      "  Batch 10: Avg Loss: 0.2796\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.214 2.094 2.189], Target: [2. 3. 2.], Loss: 0.4512\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.547 5.    2.488], Target: [3. 2. 2.], Loss: 4.7217\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.441 5.    2.394], Target: [2. 2. 3.], Loss: 4.7809\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.28  2.129 2.246], Target: [2. 3. 2.], Loss: 0.4488\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.654 5.    2.588], Target: [3. 2. 2.], Loss: 4.7327\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.281 2.154 2.246], Target: [2. 3. 2.], Loss: 0.4276\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.74  5.    2.672], Target: [3. 2. 2.], Loss: 4.7596\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.312 2.035 2.275], Target: [2. 2. 3.], Loss: 0.3121\n",
      "  Batch 11: Avg Loss: 2.5793\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.514 2.214 2.441], Target: [2. 2. 3.], Loss: 0.3112\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.203 2.11  2.173], Target: [2. 3. 2.], Loss: 0.4316\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.44  2.191 2.376], Target: [2. 2. 3.], Loss: 0.3097\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.568 2.23  2.49 ], Target: [3. 2. 2.], Loss: 0.2398\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.295 2.144 2.251], Target: [2. 3. 2.], Loss: 0.4414\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.141 2.088 2.121], Target: [2. 3. 2.], Loss: 0.4331\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.619 2.244 2.536], Target: [3. 2. 2.], Loss: 0.2460\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.677 2.26  2.592], Target: [3. 2. 2.], Loss: 0.2612\n",
      "  Batch 12: Avg Loss: 0.3343\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.592 2.221 2.501], Target: [3. 2. 2.], Loss: 0.2332\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.398 2.172 2.331], Target: [2. 2. 3.], Loss: 0.3178\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.491 2.197 2.411], Target: [3. 2. 2.], Loss: 0.2334\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.312 2.149 2.258], Target: [2. 3. 2.], Loss: 0.4441\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.643 2.231 2.548], Target: [3. 2. 2.], Loss: 0.2406\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.411 2.175 2.342], Target: [2. 2. 3.], Loss: 0.3163\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.625 2.228 2.531], Target: [3. 2. 2.], Loss: 0.2373\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.624 2.227 2.531], Target: [3. 2. 2.], Loss: 0.2374\n",
      "  Batch 13: Avg Loss: 0.2825\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.278 2.119 2.213], Target: [2. 3. 2.], Loss: 0.4494\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.465 2.154 2.363], Target: [2. 2. 3.], Loss: 0.3229\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.446 2.15  2.347], Target: [2. 2. 3.], Loss: 0.3239\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.424 2.147 2.329], Target: [2. 2. 3.], Loss: 0.3258\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.423 2.147 2.329], Target: [2. 2. 3.], Loss: 0.3254\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.509 2.161 2.399], Target: [2. 2. 3.], Loss: 0.3231\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.259 2.114 2.198], Target: [2. 3. 2.], Loss: 0.4456\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.569 2.166 2.45 ], Target: [3. 2. 2.], Loss: 0.2079\n",
      "  Batch 14: Avg Loss: 0.3405\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.334 2.132 2.286], Target: [2. 2. 3.], Loss: 0.3194\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.444 2.151 2.38 ], Target: [2. 2. 3.], Loss: 0.3022\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.224 2.109 2.192], Target: [2. 3. 2.], Loss: 0.4405\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.227 2.11  2.195], Target: [2. 3. 2.], Loss: 0.4408\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.194 2.1   2.167], Target: [2. 3. 2.], Loss: 0.4378\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.307 2.132 2.262], Target: [2. 3. 2.], Loss: 0.4582\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.529 2.161 2.456], Target: [2. 2. 3.], Loss: 0.3008\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.189 2.099 2.163], Target: [2. 3. 2.], Loss: 0.4370\n",
      "  Batch 15: Avg Loss: 0.3921\n",
      " End Epoch 3: W1 norm=12.028, W2 norm=8.375\n",
      "\n",
      "Epoch 4/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.151 2.117 2.141], Target: [2. 3. 2.], Loss: 0.4112\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.356 2.176 2.325], Target: [3. 2. 2.], Loss: 0.2757\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.469 2.202 2.427], Target: [2. 2. 3.], Loss: 0.2945\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.186 2.129 2.172], Target: [2. 3. 2.], Loss: 0.4114\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.343 2.172 2.313], Target: [2. 2. 3.], Loss: 0.3096\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.464 2.201 2.423], Target: [3. 2. 2.], Loss: 0.2533\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.37  2.179 2.338], Target: [2. 2. 3.], Loss: 0.3036\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.46 2.2  2.42], Target: [3. 2. 2.], Loss: 0.2540\n",
      "  Batch 1: Avg Loss: 0.3142\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.669 2.203 2.615], Target: [3. 2. 2.], Loss: 0.2645\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.762 2.187 2.706], Target: [3. 2. 2.], Loss: 0.2950\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.664 2.203 2.61 ], Target: [3. 2. 2.], Loss: 0.2631\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.437 2.185 2.4  ], Target: [2. 2. 3.], Loss: 0.2926\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.238 2.143 2.22 ], Target: [2. 2. 3.], Loss: 0.3427\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.358 2.17  2.329], Target: [2. 2. 3.], Loss: 0.3037\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.363 2.171 2.333], Target: [2. 2. 3.], Loss: 0.3029\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.196 2.133 2.182], Target: [2. 3. 2.], Loss: 0.4116\n",
      "  Batch 2: Avg Loss: 0.3095\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.262 2.135 2.243], Target: [2. 3. 2.], Loss: 0.4380\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.51  2.142 2.467], Target: [3. 2. 2.], Loss: 0.2392\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.534 2.142 2.489], Target: [3. 2. 2.], Loss: 0.2382\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.26  2.132 2.241], Target: [2. 3. 2.], Loss: 0.4396\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.434 2.143 2.398], Target: [2. 2. 3.], Loss: 0.2856\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.257 2.134 2.238], Target: [2. 3. 2.], Loss: 0.4363\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.741 2.067 2.684], Target: [3. 2. 2.], Loss: 0.2697\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.2   2.12  2.187], Target: [2. 3. 2.], Loss: 0.4247\n",
      "  Batch 3: Avg Loss: 0.3464\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.308 2.162 2.27 ], Target: [2. 2. 3.], Loss: 0.3270\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.517 2.179 2.452], Target: [3. 2. 2.], Loss: 0.2348\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.503 2.178 2.44 ], Target: [3. 2. 2.], Loss: 0.2361\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.243 2.156 2.213], Target: [2. 3. 2.], Loss: 0.4084\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.462 2.178 2.404], Target: [3. 2. 2.], Loss: 0.2422\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.231 2.155 2.203], Target: [2. 3. 2.], Loss: 0.4043\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.264 2.161 2.231], Target: [2. 3. 2.], Loss: 0.4135\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.404 2.174 2.353], Target: [2. 2. 3.], Loss: 0.3061\n",
      "  Batch 4: Avg Loss: 0.3215\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.452 2.189 2.385], Target: [2. 2. 3.], Loss: 0.3091\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.342 2.192 2.29 ], Target: [2. 3. 2.], Loss: 0.4270\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.592 2.186 2.508], Target: [3. 2. 2.], Loss: 0.2296\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.593 2.19  2.509], Target: [3. 2. 2.], Loss: 0.2304\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.333 2.177 2.283], Target: [2. 2. 3.], Loss: 0.3282\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.357 2.18  2.304], Target: [2. 2. 3.], Loss: 0.3221\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.644 2.179 2.556], Target: [3. 2. 2.], Loss: 0.2340\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.496 2.192 2.423], Target: [2. 2. 3.], Loss: 0.3079\n",
      "  Batch 5: Avg Loss: 0.2985\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.21  2.153 2.184], Target: [2. 3. 2.], Loss: 0.3977\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.519 2.145 2.45 ], Target: [2. 2. 3.], Loss: 0.2964\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.287 2.163 2.249], Target: [2. 3. 2.], Loss: 0.4225\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.602 2.109 2.525], Target: [3. 2. 2.], Loss: 0.2230\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.192 2.15  2.169], Target: [2. 3. 2.], Loss: 0.3940\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.243 2.156 2.212], Target: [2. 3. 2.], Loss: 0.4082\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.468 2.146 2.406], Target: [3. 2. 2.], Loss: 0.2346\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.203 2.152 2.179], Target: [2. 3. 2.], Loss: 0.3962\n",
      "  Batch 6: Avg Loss: 0.3466\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.126 2.198 2.109], Target: [2. 3. 2.], Loss: 0.3355\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.177 2.2   2.152], Target: [2. 3. 2.], Loss: 0.3472\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.194 2.197 2.166], Target: [2. 3. 2.], Loss: 0.3550\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.374 2.2   2.317], Target: [2. 2. 3.], Loss: 0.3232\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.627 2.179 2.537], Target: [3. 2. 2.], Loss: 0.2298\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.402 2.199 2.34 ], Target: [2. 2. 3.], Loss: 0.3184\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.233 2.202 2.197], Target: [2. 3. 2.], Loss: 0.3650\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.487 2.2   2.413], Target: [3. 2. 2.], Loss: 0.2369\n",
      "  Batch 7: Avg Loss: 0.3139\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.393 2.214 2.331], Target: [2. 2. 3.], Loss: 0.3239\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.612 2.195 2.522], Target: [3. 2. 2.], Loss: 0.2305\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.192 2.232 2.163], Target: [2. 3. 2.], Loss: 0.3266\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.469 2.21  2.396], Target: [3. 2. 2.], Loss: 0.2414\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.253 2.23  2.213], Target: [2. 3. 2.], Loss: 0.3511\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.339 2.214 2.286], Target: [2. 2. 3.], Loss: 0.3353\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.451 2.211 2.381], Target: [3. 2. 2.], Loss: 0.2455\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.231 2.23  2.195], Target: [2. 3. 2.], Loss: 0.3421\n",
      "  Batch 8: Avg Loss: 0.2996\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.466 2.2   2.385], Target: [2. 2. 3.], Loss: 0.3177\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.379 2.207 2.312], Target: [2. 2. 3.], Loss: 0.3299\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.664 2.124 2.56 ], Target: [3. 2. 2.], Loss: 0.2209\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.439 2.208 2.362], Target: [2. 2. 3.], Loss: 0.3215\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.477 2.199 2.394], Target: [2. 2. 3.], Loss: 0.3172\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.591 2.181 2.492], Target: [3. 2. 2.], Loss: 0.2211\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.243 2.228 2.2  ], Target: [2. 3. 2.], Loss: 0.3475\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.267 2.223 2.22 ], Target: [2. 3. 2.], Loss: 0.3617\n",
      "  Batch 9: Avg Loss: 0.3047\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.248 2.211 2.215], Target: [2. 3. 2.], Loss: 0.3651\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.41  2.167 2.356], Target: [3. 2. 2.], Loss: 0.2514\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.384 2.179 2.333], Target: [2. 2. 3.], Loss: 0.3122\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.578 2.115 2.502], Target: [3. 2. 2.], Loss: 0.2217\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.336 2.183 2.292], Target: [2. 2. 3.], Loss: 0.3238\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.582 2.111 2.506], Target: [3. 2. 2.], Loss: 0.2215\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.486 2.152 2.422], Target: [3. 2. 2.], Loss: 0.2327\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.492 2.171 2.426], Target: [2. 2. 3.], Loss: 0.3004\n",
      "  Batch 10: Avg Loss: 0.2786\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.23  2.177 2.196], Target: [2. 3. 2.], Loss: 0.3843\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.531 2.039 2.451], Target: [3. 2. 2.], Loss: 0.2124\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.42  2.085 2.358], Target: [2. 2. 3.], Loss: 0.2979\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.289 2.181 2.245], Target: [2. 3. 2.], Loss: 0.4072\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.641 1.914 2.549], Target: [3. 2. 2.], Loss: 0.2188\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.294 2.188 2.249], Target: [2. 3. 2.], Loss: 0.4039\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.727 5.    2.628], Target: [3. 2. 2.], Loss: 4.7345\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.31  2.151 2.265], Target: [2. 2. 3.], Loss: 0.3296\n",
      "  Batch 11: Avg Loss: 0.8736\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.491 2.227 2.396], Target: [2. 2. 3.], Loss: 0.3287\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.245 2.121 2.2  ], Target: [2. 3. 2.], Loss: 0.4363\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.455 2.227 2.359], Target: [2. 2. 3.], Loss: 0.3347\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.528 2.232 2.425], Target: [3. 2. 2.], Loss: 0.2286\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.374 5.    2.314], Target: [2. 3. 2.], Loss: 2.1192\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.212 5.    2.18 ], Target: [2. 3. 2.], Loss: 2.0387\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.569 2.225 2.46 ], Target: [3. 2. 2.], Loss: 0.2240\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.68  2.207 2.551], Target: [3. 2. 2.], Loss: 0.2244\n",
      "  Batch 12: Avg Loss: 0.7418\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.597 2.102 2.464], Target: [3. 2. 2.], Loss: 0.1941\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.418 2.165 2.32 ], Target: [2. 2. 3.], Loss: 0.3322\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.5   2.147 2.384], Target: [3. 2. 2.], Loss: 0.2095\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.347 2.169 2.266], Target: [2. 3. 2.], Loss: 0.4409\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.643 2.07  2.503], Target: [3. 2. 2.], Loss: 0.1927\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.429 2.164 2.328], Target: [2. 2. 3.], Loss: 0.3313\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.633 2.077 2.496], Target: [3. 2. 2.], Loss: 0.1933\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.629 2.079 2.492], Target: [3. 2. 2.], Loss: 0.1930\n",
      "  Batch 13: Avg Loss: 0.2609\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.339 2.142 2.245], Target: [2. 3. 2.], Loss: 0.4556\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.484 2.123 2.342], Target: [2. 2. 3.], Loss: 0.3412\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.467 2.128 2.33 ], Target: [2. 2. 3.], Loss: 0.3417\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.444 2.138 2.312], Target: [2. 2. 3.], Loss: 0.3448\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.441 2.141 2.31 ], Target: [2. 2. 3.], Loss: 0.3452\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.525 2.107 2.373], Target: [2. 2. 3.], Loss: 0.3401\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.321 2.142 2.231], Target: [2. 3. 2.], Loss: 0.4463\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.573 2.082 2.409], Target: [3. 2. 2.], Loss: 0.1782\n",
      "  Batch 14: Avg Loss: 0.3491\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.361 2.149 2.292], Target: [2. 2. 3.], Loss: 0.3269\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.454 2.129 2.366], Target: [2. 2. 3.], Loss: 0.3124\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.314 2.13  2.277], Target: [2. 3. 2.], Loss: 0.4661\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.209 2.008 2.18 ], Target: [2. 3. 2.], Loss: 0.5301\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.28  2.131 2.241], Target: [2. 3. 2.], Loss: 0.4458\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.263 2.011 2.213], Target: [2. 3. 2.], Loss: 0.5463\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.54  2.084 2.438], Target: [2. 2. 3.], Loss: 0.3072\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.274 2.131 2.235], Target: [2. 3. 2.], Loss: 0.4427\n",
      "  Batch 15: Avg Loss: 0.4222\n",
      " End Epoch 4: W1 norm=20.137, W2 norm=12.979\n",
      "\n",
      "Epoch 5/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.144 2.019 2.132], Target: [2. 3. 2.], Loss: 0.5003\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.341 2.033 2.299], Target: [3. 2. 2.], Loss: 0.2624\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.45  2.023 2.391], Target: [2. 2. 3.], Loss: 0.2870\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.177 2.025 2.161], Target: [2. 3. 2.], Loss: 0.5039\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.328 2.032 2.288], Target: [2. 2. 3.], Loss: 0.3078\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.445 2.024 2.387], Target: [3. 2. 2.], Loss: 0.2292\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.354 2.032 2.31 ], Target: [2. 2. 3.], Loss: 0.3012\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.442 2.024 2.384], Target: [3. 2. 2.], Loss: 0.2297\n",
      "  Batch 1: Avg Loss: 0.3277\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.59  1.956 2.515], Target: [3. 2. 2.], Loss: 0.2176\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.692 5.    2.607], Target: [3. 2. 2.], Loss: 4.7317\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.641 1.959 2.561], Target: [3. 2. 2.], Loss: 0.2226\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.416 2.044 2.365], Target: [2. 2. 3.], Loss: 0.2891\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.223 2.036 2.202], Target: [2. 2. 3.], Loss: 0.3439\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.34  2.045 2.301], Target: [2. 2. 3.], Loss: 0.3031\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.344 2.045 2.304], Target: [2. 2. 3.], Loss: 0.3024\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.184 2.031 2.168], Target: [2. 3. 2.], Loss: 0.5005\n",
      "  Batch 2: Avg Loss: 0.8639\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.26  1.765 2.234], Target: [2. 3. 2.], Loss: 0.8238\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.513 5.    2.45 ], Target: [3. 2. 2.], Loss: 4.7198\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.532 5.    2.466], Target: [3. 2. 2.], Loss: 4.7181\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.26  1.723 2.235], Target: [2. 3. 2.], Loss: 0.8768\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.439 5.    2.387], Target: [2. 2. 3.], Loss: 4.7842\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.254 1.778 2.229], Target: [2. 3. 2.], Loss: 0.8051\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.732 5.    2.642], Target: [3. 2. 2.], Loss: 4.7420\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.206 1.785 2.189], Target: [2. 3. 2.], Loss: 0.7772\n",
      "  Batch 3: Avg Loss: 2.7809\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.332 1.679 2.265], Target: [2. 2. 3.], Loss: 0.3767\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.533 5.    2.425], Target: [3. 2. 2.], Loss: 4.6994\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.52  1.704 2.414], Target: [3. 2. 2.], Loss: 0.2447\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.26  1.796 2.21 ], Target: [2. 3. 2.], Loss: 0.7807\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.479 5.    2.381], Target: [3. 2. 2.], Loss: 4.7083\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.247 1.814 2.2  ], Target: [2. 3. 2.], Loss: 0.7538\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.278 1.783 2.225], Target: [2. 3. 2.], Loss: 0.8045\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.425 5.    2.338], Target: [2. 2. 3.], Loss: 4.8094\n",
      "  Batch 4: Avg Loss: 2.1472\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.38  1.804 2.367], Target: [2. 2. 3.], Loss: 0.2918\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.348 5.    2.29 ], Target: [2. 3. 2.], Loss: 2.1026\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.492 5.    2.463], Target: [3. 2. 2.], Loss: 4.7362\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.429 1.772 2.399], Target: [3. 2. 2.], Loss: 0.2686\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.293 1.872 2.288], Target: [2. 2. 3.], Loss: 0.3046\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.309 1.86  2.304], Target: [2. 2. 3.], Loss: 0.2997\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.567 5.    2.489], Target: [3. 2. 2.], Loss: 4.7133\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.604 1.772 2.548], Target: [2. 2. 3.], Loss: 0.3106\n",
      "  Batch 5: Avg Loss: 1.6284\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.268 5.    2.238], Target: [2. 3. 2.], Loss: 2.0642\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.605 5.    2.581], Target: [2. 2. 3.], Loss: 4.7708\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.321 5.    2.305], Target: [2. 3. 2.], Loss: 2.0980\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.698 5.    2.661], Target: [3. 2. 2.], Loss: 4.7641\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.265 1.547 2.257], Target: [2. 3. 2.], Loss: 1.1237\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.325 5.    2.337], Target: [2. 3. 2.], Loss: 2.1096\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.583 5.    2.561], Target: [3. 2. 2.], Loss: 4.7443\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.291 5.    2.306], Target: [2. 3. 2.], Loss: 2.0892\n",
      "  Batch 6: Avg Loss: 2.9705\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.286 1.639 2.133], Target: [2. 3. 2.], Loss: 0.9759\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.236 1.83  2.112], Target: [2. 3. 2.], Loss: 0.7186\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.251 5.    2.136], Target: [2. 3. 2.], Loss: 2.0407\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.374 2.101 2.233], Target: [2. 2. 3.], Loss: 0.3692\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.39  5.    2.287], Target: [3. 2. 2.], Loss: 4.7272\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.399 2.109 2.251], Target: [2. 2. 3.], Loss: 0.3660\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.234 2.059 2.14 ], Target: [2. 3. 2.], Loss: 0.4799\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.365 2.141 2.316], Target: [3. 2. 2.], Loss: 0.2615\n",
      "  Batch 7: Avg Loss: 1.2424\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.397 2.033 2.253], Target: [2. 2. 3.], Loss: 0.3584\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.629 2.063 2.435], Target: [3. 2. 2.], Loss: 0.1654\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.263 1.812 2.126], Target: [2. 3. 2.], Loss: 0.7482\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.476 2.042 2.311], Target: [3. 2. 2.], Loss: 0.1865\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.17  2.206 2.099], Target: [2. 3. 2.], Loss: 0.3346\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.34  2.027 2.213], Target: [2. 2. 3.], Loss: 0.3678\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.457 2.039 2.297], Target: [3. 2. 2.], Loss: 0.1923\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.167 2.219 2.096], Target: [2. 3. 2.], Loss: 0.3235\n",
      "  Batch 8: Avg Loss: 0.3346\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.461 2.061 2.296], Target: [2. 2. 3.], Loss: 0.3559\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.37  2.046 2.231], Target: [2. 2. 3.], Loss: 0.3652\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.655 2.103 2.453], Target: [3. 2. 2.], Loss: 0.1674\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.437 2.057 2.278], Target: [2. 2. 3.], Loss: 0.3578\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.473 2.063 2.305], Target: [2. 2. 3.], Loss: 0.3554\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.598 2.089 2.404], Target: [3. 2. 2.], Loss: 0.1664\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.238 2.027 2.143], Target: [2. 3. 2.], Loss: 0.5119\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.26  2.03  2.157], Target: [2. 3. 2.], Loss: 0.5166\n",
      "  Batch 9: Avg Loss: 0.3496\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.234 2.032 2.154], Target: [2. 3. 2.], Loss: 0.5077\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.332 2.372 2.203], Target: [3. 2. 2.], Loss: 0.3129\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.368 2.054 2.25 ], Target: [2. 2. 3.], Loss: 0.3504\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.564 2.096 2.405], Target: [3. 2. 2.], Loss: 0.1817\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.316 2.045 2.212], Target: [2. 2. 3.], Loss: 0.3614\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.582 2.101 2.421], Target: [3. 2. 2.], Loss: 0.1811\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.476 2.076 2.332], Target: [3. 2. 2.], Loss: 0.1953\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.487 2.078 2.341], Target: [2. 2. 3.], Loss: 0.3388\n",
      "  Batch 10: Avg Loss: 0.3037\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.174 1.947 2.093], Target: [2. 3. 2.], Loss: 0.5739\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.524 2.075 2.37 ], Target: [3. 2. 2.], Loss: 0.1846\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.335 2.366 2.202], Target: [2. 2. 3.], Loss: 0.4415\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.189 1.97  2.109], Target: [2. 3. 2.], Loss: 0.5543\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.563 5.    2.382], Target: [3. 2. 2.], Loss: 4.6684\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.278 2.034 2.183], Target: [2. 3. 2.], Loss: 0.5220\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.729 2.128 2.556], Target: [3. 2. 2.], Loss: 0.1995\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.284 2.035 2.188], Target: [2. 2. 3.], Loss: 0.3706\n",
      "  Batch 11: Avg Loss: 0.9393\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.52  1.965 2.355], Target: [2. 2. 3.], Loss: 0.3438\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.189 1.921 2.097], Target: [2. 3. 2.], Loss: 0.6047\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.446 1.972 2.298], Target: [2. 2. 3.], Loss: 0.3463\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.576 1.959 2.401], Target: [3. 2. 2.], Loss: 0.1711\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.299 1.983 2.191], Target: [2. 3. 2.], Loss: 0.5801\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.396 0.855 2.199], Target: [2. 3. 2.], Loss: 2.3987\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.626 1.953 2.444], Target: [3. 2. 2.], Loss: 0.1696\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.681 1.945 2.495], Target: [3. 2. 2.], Loss: 0.1749\n",
      "  Batch 12: Avg Loss: 0.5987\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.592 2.027 2.408], Target: [3. 2. 2.], Loss: 0.1668\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.394 2.015 2.254], Target: [2. 2. 3.], Loss: 0.3560\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.489 2.02  2.325], Target: [3. 2. 2.], Loss: 0.1836\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.309 2.011 2.194], Target: [2. 3. 2.], Loss: 0.5556\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.57  1.89  2.333], Target: [3. 2. 2.], Loss: 0.1539\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.409 2.016 2.265], Target: [2. 2. 3.], Loss: 0.3539\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.559 1.889 2.323], Target: [3. 2. 2.], Loss: 0.1556\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.599 1.678 2.347], Target: [3. 2. 2.], Loss: 0.1924\n",
      "  Batch 13: Avg Loss: 0.2647\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.271 2.026 2.157], Target: [2. 3. 2.], Loss: 0.5234\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.456 2.049 2.28 ], Target: [2. 2. 3.], Loss: 0.3644\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.435 2.046 2.266], Target: [2. 2. 3.], Loss: 0.3650\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.414 2.043 2.251], Target: [2. 2. 3.], Loss: 0.3671\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.414 2.043 2.251], Target: [2. 2. 3.], Loss: 0.3671\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.502 2.056 2.314], Target: [2. 2. 3.], Loss: 0.3629\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.252 2.024 2.145], Target: [2. 3. 2.], Loss: 0.5186\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.561 2.066 2.359], Target: [3. 2. 2.], Loss: 0.1630\n",
      "  Batch 14: Avg Loss: 0.3789\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.32  2.042 2.222], Target: [2. 2. 3.], Loss: 0.3547\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.43  2.061 2.306], Target: [2. 2. 3.], Loss: 0.3351\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.215 2.027 2.145], Target: [2. 3. 2.], Loss: 0.5070\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.219 2.027 2.148], Target: [2. 3. 2.], Loss: 0.5083\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.184 2.023 2.124], Target: [2. 3. 2.], Loss: 0.5019\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.298 2.039 2.205], Target: [2. 3. 2.], Loss: 0.5272\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.518 2.078 2.377], Target: [2. 2. 3.], Loss: 0.3313\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.18  2.022 2.121], Target: [2. 3. 2.], Loss: 0.5018\n",
      "  Batch 15: Avg Loss: 0.4459\n",
      " End Epoch 5: W1 norm=52.440, W2 norm=20.625\n",
      "\n",
      "Epoch 6/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.142 2.029 2.104], Target: [2. 3. 2.], Loss: 0.4869\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.342 2.078 2.261], Target: [3. 2. 2.], Loss: 0.2536\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.455 2.111 2.355], Target: [2. 2. 3.], Loss: 0.3177\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.177 2.037 2.131], Target: [2. 3. 2.], Loss: 0.4879\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.327 2.074 2.249], Target: [2. 2. 3.], Loss: 0.3382\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.45  2.109 2.35 ], Target: [3. 2. 2.], Loss: 0.2184\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.355 2.081 2.272], Target: [2. 2. 3.], Loss: 0.3313\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.445 2.108 2.347], Target: [3. 2. 2.], Loss: 0.2200\n",
      "  Batch 1: Avg Loss: 0.3318\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.618 1.74  2.384], Target: [3. 2. 2.], Loss: 0.1805\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.752 2.238 2.642], Target: [3. 2. 2.], Loss: 0.2652\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.613 1.74  2.379], Target: [3. 2. 2.], Loss: 0.1805\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.42  2.101 2.331], Target: [2. 2. 3.], Loss: 0.3171\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.221 2.048 2.169], Target: [2. 2. 3.], Loss: 0.3709\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.341 2.078 2.265], Target: [2. 2. 3.], Loss: 0.3313\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.346 2.08  2.269], Target: [2. 2. 3.], Loss: 0.3302\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.185 2.039 2.14 ], Target: [2. 3. 2.], Loss: 0.4887\n",
      "  Batch 2: Avg Loss: 0.3080\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.247 2.059 2.192], Target: [2. 3. 2.], Loss: 0.4917\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.49  2.135 2.396], Target: [3. 2. 2.], Loss: 0.2176\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.515 2.144 2.419], Target: [3. 2. 2.], Loss: 0.2158\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.243 2.058 2.188], Target: [2. 3. 2.], Loss: 0.4909\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.413 2.108 2.33 ], Target: [2. 2. 3.], Loss: 0.3156\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.243 2.058 2.189], Target: [2. 3. 2.], Loss: 0.4911\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.729 2.245 2.624], Target: [3. 2. 2.], Loss: 0.2614\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.184 2.043 2.142], Target: [2. 3. 2.], Loss: 0.4849\n",
      "  Batch 3: Avg Loss: 0.3711\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.288 2.08  2.212], Target: [2. 2. 3.], Loss: 0.3551\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.498 2.157 2.384], Target: [3. 2. 2.], Loss: 0.2121\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.486 2.152 2.373], Target: [3. 2. 2.], Loss: 0.2132\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.226 2.061 2.165], Target: [2. 3. 2.], Loss: 0.4800\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.444 2.135 2.338], Target: [3. 2. 2.], Loss: 0.2208\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.216 2.058 2.157], Target: [2. 3. 2.], Loss: 0.4793\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.248 2.068 2.182], Target: [2. 3. 2.], Loss: 0.4816\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.385 2.113 2.289], Target: [2. 2. 3.], Loss: 0.3333\n",
      "  Batch 4: Avg Loss: 0.3469\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.432 2.141 2.321], Target: [2. 2. 3.], Loss: 0.3338\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.33  2.102 2.24 ], Target: [2. 3. 2.], Loss: 0.4865\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.556 2.012 2.307], Target: [3. 2. 2.], Loss: 0.1458\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.579 2.208 2.447], Target: [3. 2. 2.], Loss: 0.2102\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.31  2.095 2.225], Target: [2. 2. 3.], Loss: 0.3529\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.336 2.104 2.244], Target: [2. 2. 3.], Loss: 0.3476\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.539 2.05  2.315], Target: [3. 2. 2.], Loss: 0.1571\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.478 2.161 2.359], Target: [2. 2. 3.], Loss: 0.3326\n",
      "  Batch 5: Avg Loss: 0.2958\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.193 2.055 2.142], Target: [2. 3. 2.], Loss: 0.4752\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.5   2.168 2.391], Target: [2. 2. 3.], Loss: 0.3246\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.271 2.08  2.203], Target: [2. 3. 2.], Loss: 0.4805\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.583 2.207 2.465], Target: [3. 2. 2.], Loss: 0.2165\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.173 2.049 2.127], Target: [2. 3. 2.], Loss: 0.4752\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.225 2.065 2.167], Target: [2. 3. 2.], Loss: 0.4764\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.446 2.145 2.345], Target: [3. 2. 2.], Loss: 0.2235\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.185 2.053 2.136], Target: [2. 3. 2.], Loss: 0.4748\n",
      "  Batch 6: Avg Loss: 0.3933\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.103 2.034 2.073], Target: [2. 3. 2.], Loss: 0.4745\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.159 2.053 2.114], Target: [2. 3. 2.], Loss: 0.4675\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.173 2.058 2.125], Target: [2. 3. 2.], Loss: 0.4665\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.351 2.128 2.262], Target: [2. 2. 3.], Loss: 0.3421\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.614 2.26  2.486], Target: [3. 2. 2.], Loss: 0.2264\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.377 2.139 2.282], Target: [2. 2. 3.], Loss: 0.3385\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.215 2.074 2.156], Target: [2. 3. 2.], Loss: 0.4640\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.466 2.18  2.355], Target: [3. 2. 2.], Loss: 0.2218\n",
      "  Batch 7: Avg Loss: 0.3752\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.364 2.14  2.272], Target: [2. 2. 3.], Loss: 0.3410\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.596 2.261 2.471], Target: [3. 2. 2.], Loss: 0.2266\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.169 2.06  2.122], Target: [2. 3. 2.], Loss: 0.4635\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.443 2.178 2.337], Target: [3. 2. 2.], Loss: 0.2278\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.232 2.084 2.169], Target: [2. 3. 2.], Loss: 0.4607\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.309 2.116 2.228], Target: [2. 2. 3.], Loss: 0.3525\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.425 2.169 2.322], Target: [3. 2. 2.], Loss: 0.2314\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.209 2.075 2.152], Target: [2. 3. 2.], Loss: 0.4612\n",
      "  Batch 8: Avg Loss: 0.3456\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.433 2.175 2.321], Target: [2. 2. 3.], Loss: 0.3396\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.342 2.133 2.249], Target: [2. 2. 3.], Loss: 0.3493\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.637 2.291 2.5  ], Target: [3. 2. 2.], Loss: 0.2332\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.408 2.163 2.301], Target: [2. 2. 3.], Loss: 0.3408\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.445 2.181 2.332], Target: [2. 2. 3.], Loss: 0.3385\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.57  2.249 2.438], Target: [3. 2. 2.], Loss: 0.2194\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.214 2.078 2.152], Target: [2. 3. 2.], Loss: 0.4595\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.236 2.087 2.168], Target: [2. 3. 2.], Loss: 0.4587\n",
      "  Batch 9: Avg Loss: 0.3424\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.215 2.077 2.163], Target: [2. 3. 2.], Loss: 0.4624\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.31  2.909 2.188], Target: [3. 2. 2.], Loss: 0.6689\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.344 2.13  2.266], Target: [2. 2. 3.], Loss: 0.3370\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.545 2.23  2.438], Target: [3. 2. 2.], Loss: 0.2259\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.294 2.109 2.225], Target: [2. 2. 3.], Loss: 0.3495\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.559 2.237 2.451], Target: [3. 2. 2.], Loss: 0.2270\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.453 2.181 2.357], Target: [3. 2. 2.], Loss: 0.2297\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.464 2.186 2.366], Target: [2. 2. 3.], Loss: 0.3259\n",
      "  Batch 10: Avg Loss: 0.3533\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.184 2.054 2.136], Target: [2. 3. 2.], Loss: 0.4736\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.504 2.174 2.396], Target: [3. 2. 2.], Loss: 0.2166\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.312 2.86  2.187], Target: [2. 2. 3.], Loss: 0.7490\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.25  2.076 2.187], Target: [2. 3. 2.], Loss: 0.4756\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.564 2.032 2.319], Target: [3. 2. 2.], Loss: 0.1464\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.262 2.08  2.196], Target: [2. 3. 2.], Loss: 0.4767\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.714 2.29  2.594], Target: [3. 2. 2.], Loss: 0.2594\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.264 2.08  2.198], Target: [2. 2. 3.], Loss: 0.3596\n",
      "  Batch 11: Avg Loss: 0.3946\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.492 2.165 2.375], Target: [2. 2. 3.], Loss: 0.3300\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.184 2.053 2.131], Target: [2. 3. 2.], Loss: 0.4739\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.417 2.133 2.312], Target: [2. 2. 3.], Loss: 0.3325\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.548 2.19  2.423], Target: [3. 2. 2.], Loss: 0.2097\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.274 2.082 2.199], Target: [2. 3. 2.], Loss: 0.4787\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.128 2.036 2.091], Target: [2. 3. 2.], Loss: 0.4770\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.538 2.033 2.294], Target: [3. 2. 2.], Loss: 0.1505\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.702 1.659 2.379], Target: [3. 2. 2.], Loss: 0.1744\n",
      "  Batch 12: Avg Loss: 0.3283\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.571 2.226 2.44 ], Target: [3. 2. 2.], Loss: 0.2144\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.372 2.131 2.273], Target: [2. 2. 3.], Loss: 0.3420\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.467 2.173 2.35 ], Target: [3. 2. 2.], Loss: 0.2183\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.289 2.098 2.209], Target: [2. 3. 2.], Loss: 0.4704\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.576 2.059 2.297], Target: [3. 2. 2.], Loss: 0.1357\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.385 2.136 2.283], Target: [2. 2. 3.], Loss: 0.3404\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.566 2.054 2.288], Target: [3. 2. 2.], Loss: 0.1371\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.607 2.035 2.301], Target: [3. 2. 2.], Loss: 0.1231\n",
      "  Batch 13: Avg Loss: 0.2477\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.256 2.08  2.172], Target: [2. 3. 2.], Loss: 0.4708\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.438 2.15  2.309], Target: [2. 2. 3.], Loss: 0.3459\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.418 2.141 2.293], Target: [2. 2. 3.], Loss: 0.3472\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.395 2.132 2.276], Target: [2. 2. 3.], Loss: 0.3488\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.395 2.132 2.275], Target: [2. 2. 3.], Loss: 0.3495\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.484 2.17  2.346], Target: [2. 2. 3.], Loss: 0.3454\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.236 2.073 2.159], Target: [2. 3. 2.], Loss: 0.4702\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.546 2.199 2.397], Target: [3. 2. 2.], Loss: 0.2017\n",
      "  Batch 14: Avg Loss: 0.3599\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.294 2.902 2.185], Target: [2. 2. 3.], Loss: 0.7821\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.417 2.146 2.328], Target: [2. 2. 3.], Loss: 0.3234\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.203 2.064 2.154], Target: [2. 3. 2.], Loss: 0.4705\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.208 2.066 2.158], Target: [2. 3. 2.], Loss: 0.4703\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.174 2.054 2.131], Target: [2. 3. 2.], Loss: 0.4712\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.288 2.095 2.221], Target: [2. 3. 2.], Loss: 0.4754\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.507 2.187 2.406], Target: [2. 2. 3.], Loss: 0.3224\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.168 2.052 2.127], Target: [2. 3. 2.], Loss: 0.4715\n",
      "  Batch 15: Avg Loss: 0.4734\n",
      " End Epoch 6: W1 norm=79.413, W2 norm=26.402\n",
      "\n",
      "Epoch 7/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.135 2.048 2.11 ], Target: [2. 3. 2.], Loss: 0.4683\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.335 2.131 2.281], Target: [3. 2. 2.], Loss: 0.2692\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.448 2.185 2.381], Target: [2. 2. 3.], Loss: 0.3090\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.169 2.061 2.139], Target: [2. 3. 2.], Loss: 0.4648\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.319 2.124 2.267], Target: [2. 2. 3.], Loss: 0.3272\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.443 2.182 2.376], Target: [3. 2. 2.], Loss: 0.2424\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.347 2.136 2.291], Target: [2. 2. 3.], Loss: 0.3208\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.439 2.18  2.372], Target: [3. 2. 2.], Loss: 0.2428\n",
      "  Batch 1: Avg Loss: 0.3306\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.649 2.051 2.341], Target: [3. 2. 2.], Loss: 0.1210\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.752 2.369 2.679], Target: [3. 2. 2.], Loss: 0.3294\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.645 2.049 2.337], Target: [3. 2. 2.], Loss: 0.1210\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.414 2.162 2.353], Target: [2. 2. 3.], Loss: 0.3081\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.216 2.077 2.18 ], Target: [2. 2. 3.], Loss: 0.3625\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.334 2.126 2.282], Target: [2. 2. 3.], Loss: 0.3215\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.339 2.128 2.286], Target: [2. 2. 3.], Loss: 0.3206\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.178 2.063 2.148], Target: [2. 3. 2.], Loss: 0.4658\n",
      "  Batch 2: Avg Loss: 0.2937\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.243 2.081 2.206], Target: [2. 3. 2.], Loss: 0.4730\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.489 2.184 2.427], Target: [3. 2. 2.], Loss: 0.2387\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.513 2.196 2.45 ], Target: [3. 2. 2.], Loss: 0.2390\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.237 2.079 2.202], Target: [2. 3. 2.], Loss: 0.4726\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.411 2.148 2.355], Target: [2. 2. 3.], Loss: 0.3034\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.154 2.033 2.104], Target: [2. 3. 2.], Loss: 0.4848\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.731 2.328 2.664], Target: [3. 2. 2.], Loss: 0.3104\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.124 2.022 2.077], Target: [2. 3. 2.], Loss: 0.4889\n",
      "  Batch 3: Avg Loss: 0.3764\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.285 2.102 2.231], Target: [2. 2. 3.], Loss: 0.3415\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.498 2.199 2.417], Target: [3. 2. 2.], Loss: 0.2327\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.485 2.192 2.405], Target: [3. 2. 2.], Loss: 0.2331\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.223 2.078 2.179], Target: [2. 3. 2.], Loss: 0.4659\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.443 2.171 2.367], Target: [3. 2. 2.], Loss: 0.2371\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.213 2.074 2.17 ], Target: [2. 3. 2.], Loss: 0.4659\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.246 2.087 2.198], Target: [2. 3. 2.], Loss: 0.4666\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.382 2.144 2.314], Target: [2. 2. 3.], Loss: 0.3186\n",
      "  Batch 4: Avg Loss: 0.3452\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.431 2.172 2.349], Target: [2. 2. 3.], Loss: 0.3196\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.329 2.125 2.261], Target: [2. 3. 2.], Loss: 0.4710\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.577 2.251 2.48 ], Target: [3. 2. 2.], Loss: 0.2362\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.579 2.252 2.482], Target: [3. 2. 2.], Loss: 0.2365\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.309 2.116 2.245], Target: [2. 2. 3.], Loss: 0.3395\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.334 2.127 2.266], Target: [2. 2. 3.], Loss: 0.3332\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.585 2.072 2.289], Target: [3. 2. 2.], Loss: 0.1305\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.477 2.195 2.389], Target: [2. 2. 3.], Loss: 0.3194\n",
      "  Batch 5: Avg Loss: 0.2982\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.447 1.915 2.104], Target: [2. 3. 2.], Loss: 0.6939\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.502 2.197 2.421], Target: [2. 2. 3.], Loss: 0.3130\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.443 1.957 2.131], Target: [2. 3. 2.], Loss: 0.6506\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.587 2.243 2.5  ], Target: [3. 2. 2.], Loss: 0.2398\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.448 1.903 2.098], Target: [2. 3. 2.], Loss: 0.7069\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.426 1.944 2.112], Target: [2. 3. 2.], Loss: 0.6546\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.421 2.04  2.197], Target: [3. 2. 2.], Loss: 0.1878\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.443 1.914 2.101], Target: [2. 3. 2.], Loss: 0.6929\n",
      "  Batch 6: Avg Loss: 0.5174\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.433 2.194 2.044], Target: [2. 3. 2.], Loss: 0.4195\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.391 2.157 2.065], Target: [2. 3. 2.], Loss: 0.4339\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.386 2.153 2.07 ], Target: [2. 3. 2.], Loss: 0.4357\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.346 2.149 2.284], Target: [2. 2. 3.], Loss: 0.3273\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.609 2.3   2.521], Target: [3. 2. 2.], Loss: 0.2572\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.373 2.163 2.307], Target: [2. 2. 3.], Loss: 0.3230\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.211 2.086 2.17 ], Target: [2. 3. 2.], Loss: 0.4544\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.462 2.21  2.385], Target: [3. 2. 2.], Loss: 0.2409\n",
      "  Batch 7: Avg Loss: 0.3615\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.358 2.16  2.296], Target: [2. 2. 3.], Loss: 0.3247\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.591 2.296 2.506], Target: [3. 2. 2.], Loss: 0.2555\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.364 2.187 2.063], Target: [2. 3. 2.], Loss: 0.3987\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.437 2.202 2.365], Target: [3. 2. 2.], Loss: 0.2455\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.227 2.096 2.184], Target: [2. 3. 2.], Loss: 0.4513\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.304 2.132 2.249], Target: [2. 2. 3.], Loss: 0.3369\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.42  2.193 2.35 ], Target: [3. 2. 2.], Loss: 0.2481\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.204 2.085 2.164], Target: [2. 3. 2.], Loss: 0.4529\n",
      "  Batch 8: Avg Loss: 0.3392\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.429 2.197 2.349], Target: [2. 2. 3.], Loss: 0.3233\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.348 2.17  2.133], Target: [2. 2. 3.], Loss: 0.4508\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.522 2.274 2.266], Target: [3. 2. 2.], Loss: 0.1872\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.403 2.183 2.327], Target: [2. 2. 3.], Loss: 0.3244\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.426 2.213 2.177], Target: [2. 2. 3.], Loss: 0.4521\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.564 2.278 2.472], Target: [3. 2. 2.], Loss: 0.2451\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.209 2.088 2.165], Target: [2. 3. 2.], Loss: 0.4513\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.23  2.097 2.182], Target: [2. 3. 2.], Loss: 0.4507\n",
      "  Batch 9: Avg Loss: 0.3606\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.211 2.086 2.178], Target: [2. 3. 2.], Loss: 0.4558\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.33  2.656 2.154], Target: [3. 2. 2.], Loss: 0.4515\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.344 2.162 2.154], Target: [2. 2. 3.], Loss: 0.4301\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.543 2.257 2.477], Target: [3. 2. 2.], Loss: 0.2512\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.29  2.122 2.246], Target: [2. 2. 3.], Loss: 0.3338\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.555 2.265 2.488], Target: [3. 2. 2.], Loss: 0.2532\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.45  2.203 2.39 ], Target: [3. 2. 2.], Loss: 0.2479\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.46  2.209 2.399], Target: [2. 2. 3.], Loss: 0.3082\n",
      "  Batch 10: Avg Loss: 0.3415\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.377 2.185 2.098], Target: [2. 3. 2.], Loss: 0.4080\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.499 2.201 2.429], Target: [3. 2. 2.], Loss: 0.2377\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.308 2.773 2.197], Target: [2. 2. 3.], Loss: 0.6686\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.247 2.088 2.205], Target: [2. 3. 2.], Loss: 0.4674\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.615 2.266 2.539], Target: [3. 2. 2.], Loss: 0.2548\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.36  2.914 2.111], Target: [2. 3. 2.], Loss: 0.0747\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.71  2.33  2.633], Target: [3. 2. 2.], Loss: 0.2968\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.346 2.155 2.124], Target: [2. 2. 3.], Loss: 0.4556\n",
      "  Batch 11: Avg Loss: 0.3579\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.487 2.175 2.408], Target: [2. 2. 3.], Loss: 0.3091\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.379 2.197 2.105], Target: [2. 3. 2.], Loss: 0.3997\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.379 2.163 2.187], Target: [2. 2. 3.], Loss: 0.4156\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.543 2.201 2.459], Target: [3. 2. 2.], Loss: 0.2300\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.269 2.086 2.218], Target: [2. 3. 2.], Loss: 0.4776\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.401 2.229 2.093], Target: [2. 3. 2.], Loss: 0.3819\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.595 2.229 2.508], Target: [3. 2. 2.], Loss: 0.2373\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.6   2.289 2.315], Target: [3. 2. 2.], Loss: 0.1714\n",
      "  Batch 12: Avg Loss: 0.3278\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.568 2.207 2.479], Target: [3. 2. 2.], Loss: 0.2295\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.388 2.184 2.17 ], Target: [2. 2. 3.], Loss: 0.4367\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.429 2.199 2.21 ], Target: [3. 2. 2.], Loss: 0.2049\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.286 2.089 2.23 ], Target: [2. 3. 2.], Loss: 0.4823\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.62  2.234 2.528], Target: [3. 2. 2.], Loss: 0.2390\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.382 2.124 2.311], Target: [2. 2. 3.], Loss: 0.3180\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.602 2.224 2.51 ], Target: [3. 2. 2.], Loss: 0.2343\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.602 2.224 2.51 ], Target: [3. 2. 2.], Loss: 0.2343\n",
      "  Batch 13: Avg Loss: 0.2974\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.253 2.064 2.188], Target: [2. 3. 2.], Loss: 0.4877\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.382 2.157 2.187], Target: [2. 2. 3.], Loss: 0.4158\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.416 2.115 2.318], Target: [2. 2. 3.], Loss: 0.3257\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.37  2.156 2.171], Target: [2. 2. 3.], Loss: 0.4242\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.378 2.161 2.172], Target: [2. 2. 3.], Loss: 0.4272\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.482 2.139 2.374], Target: [2. 2. 3.], Loss: 0.3218\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.234 2.059 2.174], Target: [2. 3. 2.], Loss: 0.4853\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.47  2.2   2.238], Target: [3. 2. 2.], Loss: 0.1888\n",
      "  Batch 14: Avg Loss: 0.3846\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.394 2.187 2.183], Target: [2. 2. 3.], Loss: 0.4288\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.416 2.12  2.357], Target: [2. 2. 3.], Loss: 0.3005\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.342 2.171 2.137], Target: [2. 3. 2.], Loss: 0.4115\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.206 2.053 2.172], Target: [2. 3. 2.], Loss: 0.4844\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.528 2.478 2.284], Target: [2. 3. 2.], Loss: 0.3160\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.499 2.277 2.216], Target: [2. 3. 2.], Loss: 0.4092\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.506 2.154 2.439], Target: [2. 2. 3.], Loss: 0.2972\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.386 2.213 2.143], Target: [2. 3. 2.], Loss: 0.3944\n",
      "  Batch 15: Avg Loss: 0.3802\n",
      " End Epoch 7: W1 norm=88.992, W2 norm=31.034\n",
      "\n",
      "Epoch 8/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.384 2.27  2.139], Target: [2. 3. 2.], Loss: 0.3498\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.329 2.111 2.302], Target: [3. 2. 2.], Loss: 0.2769\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.382 2.196 2.237], Target: [2. 2. 3.], Loss: 0.3833\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.358 2.237 2.139], Target: [2. 3. 2.], Loss: 0.3648\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.337 2.185 2.181], Target: [2. 2. 3.], Loss: 0.4093\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.437 2.156 2.404], Target: [3. 2. 2.], Loss: 0.2523\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.342 2.116 2.314], Target: [2. 2. 3.], Loss: 0.3005\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.377 2.194 2.232], Target: [3. 2. 2.], Loss: 0.2398\n",
      "  Batch 1: Avg Loss: 0.3221\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.65  2.254 2.618], Target: [3. 2. 2.], Loss: 0.2845\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.747 2.318 2.716], Target: [3. 2. 2.], Loss: 0.3389\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.643 2.25  2.611], Target: [3. 2. 2.], Loss: 0.2816\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.408 2.136 2.381], Target: [2. 2. 3.], Loss: 0.2841\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.386 2.263 2.175], Target: [2. 2. 3.], Loss: 0.4494\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.335 2.187 2.194], Target: [2. 2. 3.], Loss: 0.3984\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.333 2.107 2.31 ], Target: [2. 2. 3.], Loss: 0.2992\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.337 2.229 2.146], Target: [2. 3. 2.], Loss: 0.3647\n",
      "  Batch 2: Avg Loss: 0.3376\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.239 2.059 2.222], Target: [2. 3. 2.], Loss: 0.4959\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.482 2.136 2.455], Target: [3. 2. 2.], Loss: 0.2469\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.507 2.145 2.478], Target: [3. 2. 2.], Loss: 0.2463\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.234 2.057 2.218], Target: [2. 3. 2.], Loss: 0.4958\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.404 2.108 2.379], Target: [2. 2. 3.], Loss: 0.2803\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.234 2.057 2.218], Target: [2. 3. 2.], Loss: 0.4958\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.724 2.248 2.694], Target: [3. 2. 2.], Loss: 0.3097\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.352 2.245 2.179], Target: [2. 3. 2.], Loss: 0.3630\n",
      "  Batch 3: Avg Loss: 0.3667\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.356 2.228 2.197], Target: [2. 2. 3.], Loss: 0.4118\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.489 2.155 2.442], Target: [3. 2. 2.], Loss: 0.2403\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.417 2.229 2.274], Target: [3. 2. 2.], Loss: 0.2337\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.218 2.059 2.192], Target: [2. 3. 2.], Loss: 0.4849\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.383 2.209 2.249], Target: [3. 2. 2.], Loss: 0.2432\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.208 2.056 2.183], Target: [2. 3. 2.], Loss: 0.4839\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.24  2.066 2.212], Target: [2. 3. 2.], Loss: 0.4874\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.331 2.179 2.213], Target: [2. 2. 3.], Loss: 0.3805\n",
      "  Batch 4: Avg Loss: 0.3707\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.424 2.133 2.374], Target: [2. 2. 3.], Loss: 0.2947\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.445 2.293 2.26 ], Target: [2. 3. 2.], Loss: 0.3827\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.569 2.196 2.511], Target: [3. 2. 2.], Loss: 0.2426\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.571 2.197 2.512], Target: [3. 2. 2.], Loss: 0.2425\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.347 2.209 2.206], Target: [2. 2. 3.], Loss: 0.3973\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.328 2.098 2.286], Target: [2. 2. 3.], Loss: 0.3135\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.624 2.224 2.564], Target: [3. 2. 2.], Loss: 0.2548\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.469 2.152 2.416], Target: [2. 2. 3.], Loss: 0.2921\n",
      "  Batch 5: Avg Loss: 0.3025\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.321 2.224 2.178], Target: [2. 3. 2.], Loss: 0.3685\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.494 2.145 2.445], Target: [2. 2. 3.], Loss: 0.2865\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.264 2.068 2.233], Target: [2. 3. 2.], Loss: 0.4963\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.473 2.25  2.338], Target: [3. 2. 2.], Loss: 0.2272\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.367 2.274 2.2  ], Target: [2. 3. 2.], Loss: 0.3509\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.219 2.055 2.192], Target: [2. 3. 2.], Loss: 0.4889\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.396 2.216 2.268], Target: [3. 2. 2.], Loss: 0.2416\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.344 2.248 2.189], Target: [2. 3. 2.], Loss: 0.3598\n",
      "  Batch 6: Avg Loss: 0.3525\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.416 2.367 2.203], Target: [2. 3. 2.], Loss: 0.3075\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.371 2.302 2.181], Target: [2. 3. 2.], Loss: 0.3288\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.367 2.294 2.181], Target: [2. 3. 2.], Loss: 0.3329\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.344 2.108 2.302], Target: [2. 2. 3.], Loss: 0.3086\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.605 2.223 2.548], Target: [3. 2. 2.], Loss: 0.2530\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.371 2.118 2.327], Target: [2. 2. 3.], Loss: 0.3022\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.209 2.062 2.182], Target: [2. 3. 2.], Loss: 0.4783\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.459 2.153 2.408], Target: [3. 2. 2.], Loss: 0.2413\n",
      "  Batch 7: Avg Loss: 0.3191\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.356 2.118 2.312], Target: [2. 2. 3.], Loss: 0.3070\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.587 2.223 2.53 ], Target: [3. 2. 2.], Loss: 0.2506\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.342 2.296 2.157], Target: [2. 3. 2.], Loss: 0.3186\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.394 2.26  2.246], Target: [3. 2. 2.], Loss: 0.2477\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.224 2.07  2.195], Target: [2. 3. 2.], Loss: 0.4766\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.335 2.242 2.188], Target: [2. 2. 3.], Loss: 0.4151\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.42  2.292 2.251], Target: [3. 2. 2.], Loss: 0.2423\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.318 2.257 2.156], Target: [2. 3. 2.], Loss: 0.3388\n",
      "  Batch 8: Avg Loss: 0.3246\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.368 2.238 2.23 ], Target: [2. 2. 3.], Loss: 0.3925\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.338 2.236 2.194], Target: [2. 2. 3.], Loss: 0.4098\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.495 2.313 2.336], Target: [3. 2. 2.], Loss: 0.2329\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.35  2.227 2.216], Target: [2. 2. 3.], Loss: 0.3943\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.398 2.265 2.243], Target: [2. 2. 3.], Loss: 0.4008\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.56  2.204 2.497], Target: [3. 2. 2.], Loss: 0.2411\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.316 2.257 2.153], Target: [2. 3. 2.], Loss: 0.3377\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.327 2.261 2.162], Target: [2. 3. 2.], Loss: 0.3396\n",
      "  Batch 9: Avg Loss: 0.3436\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.21  2.057 2.191], Target: [2. 3. 2.], Loss: 0.4849\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.321 2.368 2.243], Target: [3. 2. 2.], Loss: 0.3278\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.34  2.098 2.31 ], Target: [2. 2. 3.], Loss: 0.3007\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.542 2.176 2.502], Target: [3. 2. 2.], Loss: 0.2464\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.337 2.257 2.214], Target: [2. 2. 3.], Loss: 0.3987\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.553 2.182 2.514], Target: [3. 2. 2.], Loss: 0.2486\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.449 2.138 2.413], Target: [3. 2. 2.], Loss: 0.2466\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.458 2.141 2.422], Target: [2. 2. 3.], Loss: 0.2819\n",
      "  Batch 10: Avg Loss: 0.3169\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.339 2.297 2.204], Target: [2. 3. 2.], Loss: 0.3254\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.403 2.243 2.296], Target: [3. 2. 2.], Loss: 0.2515\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.333 2.371 2.215], Target: [2. 2. 3.], Loss: 0.4324\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.246 2.058 2.217], Target: [2. 3. 2.], Loss: 0.4975\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.613 2.182 2.562], Target: [3. 2. 2.], Loss: 0.2494\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.254 2.06  2.225], Target: [2. 3. 2.], Loss: 0.4994\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.707 2.228 2.656], Target: [3. 2. 2.], Loss: 0.2841\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.34  2.267 2.217], Target: [2. 2. 3.], Loss: 0.4000\n",
      "  Batch 11: Avg Loss: 0.3674\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.486 2.126 2.427], Target: [2. 2. 3.], Loss: 0.2902\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.337 2.297 2.217], Target: [2. 3. 2.], Loss: 0.3274\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.337 2.204 2.248], Target: [2. 2. 3.], Loss: 0.3603\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.541 2.146 2.479], Target: [3. 2. 2.], Loss: 0.2307\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.268 2.061 2.23 ], Target: [2. 3. 2.], Loss: 0.5032\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.35  2.333 2.223], Target: [2. 3. 2.], Loss: 0.3086\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.594 2.167 2.53 ], Target: [3. 2. 2.], Loss: 0.2368\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.545 2.343 2.417], Target: [3. 2. 2.], Loss: 0.2493\n",
      "  Batch 12: Avg Loss: 0.3133\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.568 2.146 2.496], Target: [3. 2. 2.], Loss: 0.2270\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.342 2.232 2.238], Target: [2. 2. 3.], Loss: 0.3757\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.463 2.111 2.398], Target: [3. 2. 2.], Loss: 0.2295\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.284 2.061 2.239], Target: [2. 3. 2.], Loss: 0.5097\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.62  2.167 2.547], Target: [3. 2. 2.], Loss: 0.2357\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.381 2.087 2.325], Target: [2. 2. 3.], Loss: 0.3042\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.601 2.159 2.528], Target: [3. 2. 2.], Loss: 0.2316\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.602 2.16  2.529], Target: [3. 2. 2.], Loss: 0.2319\n",
      "  Batch 13: Avg Loss: 0.2932\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.252 2.044 2.195], Target: [2. 3. 2.], Loss: 0.5077\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.347 2.201 2.245], Target: [2. 2. 3.], Loss: 0.3654\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.415 2.081 2.33 ], Target: [2. 2. 3.], Loss: 0.3138\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.393 2.075 2.311], Target: [2. 2. 3.], Loss: 0.3174\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.338 2.21  2.233], Target: [2. 2. 3.], Loss: 0.3733\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.481 2.098 2.388], Target: [2. 2. 3.], Loss: 0.3078\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.234 2.041 2.18 ], Target: [2. 3. 2.], Loss: 0.5034\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.428 2.248 2.309], Target: [3. 2. 2.], Loss: 0.2421\n",
      "  Batch 14: Avg Loss: 0.3664\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.339 2.249 2.248], Target: [2. 2. 3.], Loss: 0.3712\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.416 2.084 2.365], Target: [2. 2. 3.], Loss: 0.2917\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.298 2.25  2.209], Target: [2. 3. 2.], Loss: 0.3475\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.207 2.037 2.178], Target: [2. 3. 2.], Loss: 0.5010\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.321 2.289 2.223], Target: [2. 3. 2.], Loss: 0.3291\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.439 2.364 2.321], Target: [2. 3. 2.], Loss: 0.3501\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.506 2.108 2.449], Target: [2. 2. 3.], Loss: 0.2857\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.334 2.304 2.232], Target: [2. 3. 2.], Loss: 0.3249\n",
      "  Batch 15: Avg Loss: 0.3501\n",
      " End Epoch 8: W1 norm=95.466, W2 norm=33.358\n",
      "\n",
      "Epoch 9/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.349 2.371 2.253], Target: [2. 3. 2.], Loss: 0.2907\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.331 2.084 2.308], Target: [3. 2. 2.], Loss: 0.2747\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.444 2.12  2.416], Target: [2. 2. 3.], Loss: 0.2763\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.332 2.341 2.242], Target: [2. 3. 2.], Loss: 0.3015\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.314 2.255 2.245], Target: [2. 2. 3.], Loss: 0.3668\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.439 2.118 2.411], Target: [3. 2. 2.], Loss: 0.2488\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.344 2.087 2.32 ], Target: [2. 2. 3.], Loss: 0.2942\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.435 2.117 2.408], Target: [3. 2. 2.], Loss: 0.2497\n",
      "  Batch 1: Avg Loss: 0.2878\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.452 2.287 2.393], Target: [3. 2. 2.], Loss: 0.2686\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.747 2.246 2.718], Target: [3. 2. 2.], Loss: 0.3200\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.449 2.286 2.39 ], Target: [3. 2. 2.], Loss: 0.2687\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.33  2.24  2.273], Target: [2. 2. 3.], Loss: 0.3475\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.323 2.328 2.246], Target: [2. 2. 3.], Loss: 0.3902\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.317 2.264 2.254], Target: [2. 2. 3.], Loss: 0.3634\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.31  2.252 2.25 ], Target: [2. 2. 3.], Loss: 0.3611\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.317 2.337 2.238], Target: [2. 3. 2.], Loss: 0.2984\n",
      "  Batch 2: Avg Loss: 0.3272\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.242 2.035 2.226], Target: [2. 3. 2.], Loss: 0.5204\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.49  2.083 2.463], Target: [3. 2. 2.], Loss: 0.2407\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.514 2.089 2.487], Target: [3. 2. 2.], Loss: 0.2406\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.29  2.269 2.25 ], Target: [2. 3. 2.], Loss: 0.3405\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.412 2.066 2.388], Target: [2. 2. 3.], Loss: 0.2743\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.237 2.034 2.222], Target: [2. 3. 2.], Loss: 0.5193\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.73  2.155 2.703], Target: [3. 2. 2.], Loss: 0.2956\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.323 2.338 2.276], Target: [2. 3. 2.], Loss: 0.3094\n",
      "  Batch 3: Avg Loss: 0.3426\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.311 2.292 2.262], Target: [2. 2. 3.], Loss: 0.3633\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.496 2.107 2.45 ], Target: [3. 2. 2.], Loss: 0.2340\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.483 2.104 2.438], Target: [3. 2. 2.], Loss: 0.2350\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.289 2.296 2.241], Target: [2. 3. 2.], Loss: 0.3186\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.378 2.296 2.325], Target: [3. 2. 2.], Loss: 0.2901\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.278 2.288 2.232], Target: [2. 3. 2.], Loss: 0.3190\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.243 2.045 2.216], Target: [2. 3. 2.], Loss: 0.5089\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.38  2.076 2.341], Target: [2. 2. 3.], Loss: 0.2922\n",
      "  Batch 4: Avg Loss: 0.3201\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.429 2.094 2.377], Target: [2. 2. 3.], Loss: 0.2905\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.38  2.366 2.321], Target: [2. 3. 2.], Loss: 0.3247\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.575 2.14  2.515], Target: [3. 2. 2.], Loss: 0.2327\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.576 2.14  2.516], Target: [3. 2. 2.], Loss: 0.2328\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.308 2.284 2.258], Target: [2. 2. 3.], Loss: 0.3630\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.333 2.069 2.29 ], Target: [2. 2. 3.], Loss: 0.3099\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.63  2.16  2.568], Target: [3. 2. 2.], Loss: 0.2426\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.348 2.244 2.299], Target: [2. 2. 3.], Loss: 0.3360\n",
      "  Batch 5: Avg Loss: 0.2915\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.326 2.375 2.297], Target: [2. 3. 2.], Loss: 0.2926\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.499 2.097 2.448], Target: [2. 2. 3.], Loss: 0.2816\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.268 2.045 2.235], Target: [2. 3. 2.], Loss: 0.5195\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.586 2.122 2.531], Target: [3. 2. 2.], Loss: 0.2341\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.336 2.394 2.307], Target: [2. 3. 2.], Loss: 0.2872\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.319 2.352 2.289], Target: [2. 3. 2.], Loss: 0.3026\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.446 2.083 2.397], Target: [3. 2. 2.], Loss: 0.2357\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.33  2.382 2.301], Target: [2. 3. 2.], Loss: 0.2907\n",
      "  Batch 6: Avg Loss: 0.3055\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.323 2.44  2.294], Target: [2. 3. 2.], Loss: 0.2522\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.321 2.422 2.29 ], Target: [2. 3. 2.], Loss: 0.2606\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.315 2.411 2.284], Target: [2. 3. 2.], Loss: 0.2634\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.287 2.277 2.253], Target: [2. 2. 3.], Loss: 0.3586\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.611 2.165 2.546], Target: [3. 2. 2.], Loss: 0.2383\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.289 2.26  2.254], Target: [2. 2. 3.], Loss: 0.3538\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.306 2.383 2.273], Target: [2. 3. 2.], Loss: 0.2744\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.366 2.328 2.324], Target: [3. 2. 2.], Loss: 0.3073\n",
      "  Batch 7: Avg Loss: 0.2886\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.267 2.258 2.239], Target: [2. 2. 3.], Loss: 0.3585\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.595 2.152 2.528], Target: [3. 2. 2.], Loss: 0.2330\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.294 2.422 2.275], Target: [2. 3. 2.], Loss: 0.2481\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.443 2.101 2.385], Target: [3. 2. 2.], Loss: 0.2343\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.281 2.378 2.259], Target: [2. 3. 2.], Loss: 0.2665\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.254 2.28  2.228], Target: [2. 2. 3.], Loss: 0.3695\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.425 2.096 2.369], Target: [3. 2. 2.], Loss: 0.2380\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.291 2.402 2.27 ], Target: [2. 3. 2.], Loss: 0.2576\n",
      "  Batch 8: Avg Loss: 0.2757\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.434 2.096 2.367], Target: [2. 2. 3.], Loss: 0.2991\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.343 2.072 2.286], Target: [2. 2. 3.], Loss: 0.3163\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.641 2.166 2.561], Target: [3. 2. 2.], Loss: 0.2356\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.408 2.089 2.344], Target: [2. 2. 3.], Loss: 0.3024\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.445 2.099 2.377], Target: [2. 2. 3.], Loss: 0.2980\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.569 2.138 2.491], Target: [3. 2. 2.], Loss: 0.2229\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.266 2.408 2.272], Target: [2. 3. 2.], Loss: 0.2476\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.263 2.392 2.266], Target: [2. 3. 2.], Loss: 0.2548\n",
      "  Batch 9: Avg Loss: 0.2721\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.253 2.413 2.265], Target: [2. 3. 2.], Loss: 0.2394\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.323 2.348 2.239], Target: [3. 2. 2.], Loss: 0.3183\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.245 2.289 2.239], Target: [2. 2. 3.], Loss: 0.3613\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.546 2.127 2.488], Target: [3. 2. 2.], Loss: 0.2302\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.234 2.317 2.233], Target: [2. 2. 3.], Loss: 0.3718\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.558 2.131 2.499], Target: [3. 2. 2.], Loss: 0.2308\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.305 2.316 2.295], Target: [3. 2. 2.], Loss: 0.3350\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.461 2.101 2.408], Target: [2. 2. 3.], Loss: 0.2866\n",
      "  Batch 10: Avg Loss: 0.2967\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.251 2.425 2.284], Target: [2. 3. 2.], Loss: 0.2371\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.504 2.082 2.438], Target: [3. 2. 2.], Loss: 0.2223\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.328 2.335 2.237], Target: [2. 2. 3.], Loss: 0.4010\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.249 2.382 2.27 ], Target: [2. 3. 2.], Loss: 0.2584\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.621 2.111 2.549], Target: [3. 2. 2.], Loss: 0.2287\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.258 2.035 2.217], Target: [2. 3. 2.], Loss: 0.5224\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.46  2.397 2.46 ], Target: [3. 2. 2.], Loss: 0.3304\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.227 2.327 2.239], Target: [2. 2. 3.], Loss: 0.3688\n",
      "  Batch 11: Avg Loss: 0.3211\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.495 2.067 2.415], Target: [2. 2. 3.], Loss: 0.2959\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.243 2.425 2.273], Target: [2. 3. 2.], Loss: 0.2321\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.42  2.054 2.348], Target: [2. 2. 3.], Loss: 0.3022\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.551 2.078 2.466], Target: [3. 2. 2.], Loss: 0.2124\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.249 2.374 2.262], Target: [2. 3. 2.], Loss: 0.2613\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.251 2.456 2.292], Target: [2. 3. 2.], Loss: 0.2221\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.604 2.09  2.517], Target: [3. 2. 2.], Loss: 0.2161\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.67  2.107 2.581], Target: [3. 2. 2.], Loss: 0.2290\n",
      "  Batch 12: Avg Loss: 0.2464\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.577 2.088 2.479], Target: [3. 2. 2.], Loss: 0.2081\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.376 2.049 2.301], Target: [2. 2. 3.], Loss: 0.3162\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.471 2.066 2.383], Target: [3. 2. 2.], Loss: 0.2154\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.232 2.381 2.244], Target: [2. 3. 2.], Loss: 0.2483\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.629 2.1   2.529], Target: [3. 2. 2.], Loss: 0.2137\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.389 2.051 2.312], Target: [2. 2. 3.], Loss: 0.3136\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.61  2.096 2.511], Target: [3. 2. 2.], Loss: 0.2112\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.611 2.096 2.512], Target: [3. 2. 2.], Loss: 0.2113\n",
      "  Batch 13: Avg Loss: 0.2422\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.226 2.414 2.241], Target: [2. 3. 2.], Loss: 0.2263\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.443 2.051 2.332], Target: [2. 2. 3.], Loss: 0.3225\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.423 2.048 2.316], Target: [2. 2. 3.], Loss: 0.3245\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.236 2.239 2.206], Target: [2. 2. 3.], Loss: 0.3716\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.398 2.045 2.296], Target: [2. 2. 3.], Loss: 0.3280\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.488 2.059 2.37 ], Target: [2. 2. 3.], Loss: 0.3193\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.222 2.422 2.241], Target: [2. 3. 2.], Loss: 0.2207\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.323 2.298 2.288], Target: [3. 2. 2.], Loss: 0.3150\n",
      "  Batch 14: Avg Loss: 0.3035\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.193 2.25  2.187], Target: [2. 2. 3.], Loss: 0.3804\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.424 2.046 2.351], Target: [2. 2. 3.], Loss: 0.3015\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.208 2.438 2.243], Target: [2. 3. 2.], Loss: 0.2091\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.213 2.444 2.25 ], Target: [2. 3. 2.], Loss: 0.2085\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.201 2.447 2.242], Target: [2. 3. 2.], Loss: 0.2024\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.226 2.403 2.246], Target: [2. 3. 2.], Loss: 0.2340\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.514 2.059 2.432], Target: [2. 2. 3.], Loss: 0.2952\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.199 2.448 2.241], Target: [2. 3. 2.], Loss: 0.2012\n",
      "  Batch 15: Avg Loss: 0.2540\n",
      " End Epoch 9: W1 norm=94.188, W2 norm=33.371\n",
      "\n",
      "Epoch 10/10\n",
      "  Sample 0: Input: [0.075      0.3875     0.11101695 0.0875    ], Pred: [2.114 2.472 2.21 ], Target: [2. 3. 2.], Loss: 0.1679\n",
      "  Sample 1: Input: [0.2        0.2375     0.58389831 0.65      ], Pred: [2.341 2.05  2.298], Target: [3. 2. 2.], Loss: 0.2628\n",
      "  Sample 2: Input: [0.675      0.35       0.62966102 0.5375    ], Pred: [2.456 2.073 2.403], Target: [2. 2. 3.], Loss: 0.2848\n",
      "  Sample 3: Input: [0.2        0.4625     0.12627119 0.05      ], Pred: [2.127 2.466 2.215], Target: [2. 3. 2.], Loss: 0.1738\n",
      "  Sample 4: Input: [0.35       0.2375     0.50762712 0.5       ], Pred: [2.325 2.048 2.283], Target: [2. 2. 3.], Loss: 0.3110\n",
      "  Sample 5: Input: [0.55       0.2375     0.66016949 0.725     ], Pred: [2.242 2.345 2.261], Target: [3. 2. 2.], Loss: 0.3809\n",
      "  Sample 6: Input: [0.375      0.3125     0.53813559 0.5       ], Pred: [2.353 2.052 2.308], Target: [2. 2. 3.], Loss: 0.3031\n",
      "  Sample 7: Input: [0.55       0.35       0.67542373 0.575     ], Pred: [2.446 2.07  2.393], Target: [3. 2. 2.], Loss: 0.2331\n",
      "  Batch 1: Avg Loss: 0.2647\n",
      "  Sample 8: Input: [0.9       0.425     0.8279661 0.875    ], Pred: [2.669 2.115 2.609], Target: [3. 2. 2.], Loss: 0.2468\n",
      "  Sample 9: Input: [0.9        0.725      0.91949153 0.8375    ], Pred: [2.761 2.146 2.703], Target: [3. 2. 2.], Loss: 0.2863\n",
      "  Sample 10: Input: [0.875      0.425      0.90423729 0.8       ], Pred: [2.662 2.113 2.602], Target: [3. 2. 2.], Loss: 0.2447\n",
      "  Sample 11: Input: [0.475      0.3875     0.58389831 0.575     ], Pred: [2.423 2.059 2.373], Target: [2. 2. 3.], Loss: 0.2878\n",
      "  Sample 12: Input: [0.225      0.05       0.43135593 0.3875    ], Pred: [2.127 2.297 2.132], Target: [2. 2. 3.], Loss: 0.4289\n",
      "  Sample 13: Input: [0.425      0.3125     0.52288136 0.3875    ], Pred: [2.185 2.264 2.179], Target: [2. 2. 3.], Loss: 0.3890\n",
      "  Sample 14: Input: [0.425      0.275      0.50762712 0.4625    ], Pred: [2.346 2.046 2.303], Target: [2. 2. 3.], Loss: 0.3038\n",
      "  Sample 15: Input: [0.15       0.5        0.14152542 0.0875    ], Pred: [2.139 2.463 2.176], Target: [2. 3. 2.], Loss: 0.1693\n",
      "  Batch 2: Avg Loss: 0.2946\n",
      "  Sample 16: Input: [0.25       0.725      0.12627119 0.125     ], Pred: [2.143 2.444 2.224], Target: [2. 3. 2.], Loss: 0.1899\n",
      "  Sample 17: Input: [0.65       0.2375     0.78220339 0.6875    ], Pred: [2.501 2.05  2.445], Target: [3. 2. 2.], Loss: 0.2248\n",
      "  Sample 18: Input: [0.575      0.4625     0.73644068 0.6875    ], Pred: [2.27  2.315 2.287], Target: [3. 2. 2.], Loss: 0.3572\n",
      "  Sample 19: Input: [0.325      0.575      0.12627119 0.1625    ], Pred: [2.138 2.416 2.204], Target: [2. 3. 2.], Loss: 0.2009\n",
      "  Sample 20: Input: [0.55       0.2375     0.64491525 0.575     ], Pred: [2.421 2.04  2.371], Target: [2. 2. 3.], Loss: 0.2872\n",
      "  Sample 21: Input: [0.25       0.725      0.14152542 0.0875    ], Pred: [2.141 2.452 2.228], Target: [2. 3. 2.], Loss: 0.1861\n",
      "  Sample 22: Input: [0.95       0.725      0.87372881 0.7625    ], Pred: [2.741 2.095 2.682], Target: [3. 2. 2.], Loss: 0.2706\n",
      "  Sample 23: Input: [0.225      0.425      0.14152542 0.0875    ], Pred: [2.112 2.446 2.2  ], Target: [2. 3. 2.], Loss: 0.1797\n",
      "  Batch 3: Avg Loss: 0.2371\n",
      "  Sample 24: Input: [0.35       0.2        0.46186441 0.3875    ], Pred: [2.147 2.257 2.141], Target: [2. 2. 3.], Loss: 0.4128\n",
      "  Sample 25: Input: [0.55       0.3875     0.75169492 0.6875    ], Pred: [2.257 2.349 2.247], Target: [3. 2. 2.], Loss: 0.3674\n",
      "  Sample 26: Input: [0.575     0.3125    0.7059322 0.725    ], Pred: [2.249 2.326 2.237], Target: [3. 2. 2.], Loss: 0.3632\n",
      "  Sample 27: Input: [0.225      0.575      0.14152542 0.1625    ], Pred: [2.116 2.439 2.149], Target: [2. 3. 2.], Loss: 0.1752\n",
      "  Sample 28: Input: [0.375      0.35       0.64491525 0.7625    ], Pred: [2.455 2.051 2.383], Target: [3. 2. 2.], Loss: 0.2232\n",
      "  Sample 29: Input: [0.225      0.6125     0.09576271 0.125     ], Pred: [2.111 2.457 2.153], Target: [2. 3. 2.], Loss: 0.1653\n",
      "  Sample 30: Input: [0.25       0.6875     0.12627119 0.1625    ], Pred: [2.127 2.443 2.159], Target: [2. 3. 2.], Loss: 0.1758\n",
      "  Sample 31: Input: [0.5        0.35       0.50762712 0.5       ], Pred: [2.394 2.042 2.328], Target: [2. 2. 3.], Loss: 0.3043\n",
      "  Batch 4: Avg Loss: 0.2734\n",
      "  Sample 32: Input: [0.5        0.425      0.59915254 0.5375    ], Pred: [2.449 2.042 2.368], Target: [2. 2. 3.], Loss: 0.3014\n",
      "  Sample 33: Input: [0.4        0.95       0.12627119 0.1625    ], Pred: [2.183 2.34  2.15 ], Target: [2. 3. 2.], Loss: 0.2458\n",
      "  Sample 34: Input: [0.7        0.4625     0.72118644 0.8       ], Pred: [2.598 2.064 2.505], Target: [3. 2. 2.], Loss: 0.2104\n",
      "  Sample 35: Input: [0.525      0.575      0.72118644 0.875     ], Pred: [2.314 2.37  2.268], Target: [3. 2. 2.], Loss: 0.3397\n",
      "  Sample 36: Input: [0.4        0.275      0.43135593 0.3875    ], Pred: [2.324 2.028 2.26 ], Target: [2. 2. 3.], Loss: 0.3267\n",
      "  Sample 37: Input: [0.275      0.3125     0.49237288 0.5375    ], Pred: [2.351 2.031 2.283], Target: [2. 2. 3.], Loss: 0.3191\n",
      "  Sample 38: Input: [0.65       0.5375     0.76694915 0.95      ], Pred: [2.653 2.074 2.558], Target: [3. 2. 2.], Loss: 0.2186\n",
      "  Sample 39: Input: [0.45       0.5        0.62966102 0.6875    ], Pred: [2.496 2.049 2.41 ], Target: [2. 2. 3.], Loss: 0.2983\n",
      "  Batch 5: Avg Loss: 0.2825\n",
      "  Sample 40: Input: [0.125      0.575      0.11101695 0.125     ], Pred: [2.141 2.407 2.089], Target: [2. 3. 2.], Loss: 0.1897\n",
      "  Sample 41: Input: [0.725      0.5        0.61440678 0.5375    ], Pred: [2.522 2.041 2.441], Target: [2. 2. 3.], Loss: 0.2933\n",
      "  Sample 42: Input: [0.325      0.7625     0.09576271 0.1625    ], Pred: [2.159 2.268 2.118], Target: [2. 3. 2.], Loss: 0.2875\n",
      "  Sample 43: Input: [0.825     0.35      0.8279661 0.725    ], Pred: [2.611 2.052 2.525], Target: [3. 2. 2.], Loss: 0.2148\n",
      "  Sample 44: Input: [0.2        0.425      0.11101695 0.0875    ], Pred: [2.135 2.421 2.082], Target: [2. 3. 2.], Loss: 0.1801\n",
      "  Sample 45: Input: [0.25       0.6125     0.11101695 0.125     ], Pred: [2.154 2.392 2.102], Target: [2. 3. 2.], Loss: 0.2019\n",
      "  Sample 46: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.468 2.035 2.392], Target: [3. 2. 2.], Loss: 0.2190\n",
      "  Sample 47: Input: [0.225      0.5        0.08050847 0.0875    ], Pred: [2.143 2.425 2.087], Target: [2. 3. 2.], Loss: 0.1793\n",
      "  Batch 6: Avg Loss: 0.2207\n",
      "  Sample 48: Input: [0.1        0.1625     0.09576271 0.125     ], Pred: [2.063 2.405 2.028], Target: [2. 3. 2.], Loss: 0.1794\n",
      "  Sample 49: Input: [0.175      0.425      0.11101695 0.05      ], Pred: [2.095 2.463 2.044], Target: [2. 3. 2.], Loss: 0.1497\n",
      "  Sample 50: Input: [0.175      0.425      0.11101695 0.125     ], Pred: [2.099 2.408 2.057], Target: [2. 3. 2.], Loss: 0.1818\n",
      "  Sample 51: Input: [0.375      0.3875     0.44661017 0.5       ], Pred: [2.368 2.045 2.298], Target: [2. 2. 3.], Loss: 0.3151\n",
      "  Sample 52: Input: [0.55       0.5375     0.81271186 0.95      ], Pred: [2.323 2.384 2.261], Target: [3. 2. 2.], Loss: 0.3370\n",
      "  Sample 53: Input: [0.4        0.35       0.58389831 0.5       ], Pred: [2.396 2.049 2.322], Target: [2. 2. 3.], Loss: 0.3095\n",
      "  Sample 54: Input: [0.25       0.6125     0.11101695 0.0875    ], Pred: [2.122 2.44  2.072], Target: [2. 3. 2.], Loss: 0.1668\n",
      "  Sample 55: Input: [0.45       0.425      0.67542373 0.6875    ], Pred: [2.488 2.064 2.403], Target: [3. 2. 2.], Loss: 0.2143\n",
      "  Batch 7: Avg Loss: 0.2317\n",
      "  Sample 56: Input: [0.4        0.35       0.52288136 0.5       ], Pred: [2.384 2.046 2.313], Target: [2. 2. 3.], Loss: 0.3108\n",
      "  Sample 57: Input: [0.55       0.575      0.75169492 0.9125    ], Pred: [2.317 2.386 2.238], Target: [3. 2. 2.], Loss: 0.3361\n",
      "  Sample 58: Input: [0.125      0.5        0.11101695 0.0875    ], Pred: [2.094 2.231 2.059], Target: [2. 3. 2.], Loss: 0.3018\n",
      "  Sample 59: Input: [0.425      0.3125     0.67542373 0.725     ], Pred: [2.468 2.06  2.387], Target: [3. 2. 2.], Loss: 0.2182\n",
      "  Sample 60: Input: [0.35       0.6125     0.09576271 0.0875    ], Pred: [2.124 2.126 2.092], Target: [2. 3. 2.], Loss: 0.3939\n",
      "  Sample 61: Input: [0.35       0.1625     0.50762712 0.5       ], Pred: [2.327 2.038 2.264], Target: [2. 2. 3.], Loss: 0.3250\n",
      "  Sample 62: Input: [0.4        0.2375     0.66016949 0.7625    ], Pred: [2.449 2.057 2.369], Target: [3. 2. 2.], Loss: 0.2215\n",
      "  Sample 63: Input: [0.175      0.575      0.18728814 0.0875    ], Pred: [2.113 2.208 2.077], Target: [2. 3. 2.], Loss: 0.3230\n",
      "  Batch 8: Avg Loss: 0.3038\n",
      "  Sample 64: Input: [0.475      0.3125     0.67542373 0.6125    ], Pred: [2.46  2.062 2.373], Target: [2. 2. 3.], Loss: 0.3043\n",
      "  Sample 65: Input: [0.35       0.275      0.56864407 0.4625    ], Pred: [2.364 2.046 2.29 ], Target: [2. 2. 3.], Loss: 0.3194\n",
      "  Sample 66: Input: [0.9        0.35       0.91949153 0.7625    ], Pred: [2.672 2.11  2.57 ], Target: [3. 2. 2.], Loss: 0.2223\n",
      "  Sample 67: Input: [0.375      0.425      0.58389831 0.575     ], Pred: [2.432 2.057 2.348], Target: [2. 2. 3.], Loss: 0.3075\n",
      "  Sample 68: Input: [0.6        0.35       0.59915254 0.575     ], Pred: [2.472 2.064 2.383], Target: [2. 2. 3.], Loss: 0.3038\n",
      "  Sample 69: Input: [0.575     0.5       0.7059322 0.875    ], Pred: [2.314 2.351 2.221], Target: [3. 2. 2.], Loss: 0.3213\n",
      "  Sample 70: Input: [0.25       0.575      0.12627119 0.0875    ], Pred: [2.145 2.157 2.106], Target: [2. 3. 2.], Loss: 0.3715\n",
      "  Sample 71: Input: [0.25       0.5375     0.15677966 0.2       ], Pred: [2.251 2.03  2.197], Target: [2. 3. 2.], Loss: 0.5214\n",
      "  Batch 9: Avg Loss: 0.3339\n",
      "  Sample 72: Input: [0.225      0.65       0.11101695 0.0875    ], Pred: [2.147 2.165 2.111], Target: [2. 3. 2.], Loss: 0.3656\n",
      "  Sample 73: Input: [0.475      0.125      0.66016949 0.575     ], Pred: [2.352 2.352 2.254], Target: [3. 2. 2.], Loss: 0.3042\n",
      "  Sample 74: Input: [0.425      0.3125     0.49237288 0.4625    ], Pred: [2.368 2.046 2.308], Target: [2. 2. 3.], Loss: 0.3082\n",
      "  Sample 75: Input: [0.775      0.425      0.78220339 0.6125    ], Pred: [2.578 2.085 2.499], Target: [3. 2. 2.], Loss: 0.2172\n",
      "  Sample 76: Input: [0.35       0.2        0.47711864 0.425     ], Pred: [2.314 2.038 2.261], Target: [2. 2. 3.], Loss: 0.3231\n",
      "  Sample 77: Input: [0.65       0.425      0.69067797 0.875     ], Pred: [2.59  2.088 2.51 ], Target: [3. 2. 2.], Loss: 0.2180\n",
      "  Sample 78: Input: [0.55       0.3125     0.64491525 0.6875    ], Pred: [2.266 2.333 2.173], Target: [3. 2. 2.], Loss: 0.3398\n",
      "  Sample 79: Input: [0.475      0.575      0.58389831 0.6125    ], Pred: [2.491 2.067 2.417], Target: [2. 2. 3.], Loss: 0.2927\n",
      "  Batch 10: Avg Loss: 0.2961\n",
      "  Sample 80: Input: [0.2        0.4625     0.12627119 0.0875    ], Pred: [2.198 2.017 2.159], Target: [2. 3. 2.], Loss: 0.5154\n",
      "  Sample 81: Input: [0.425      0.35       0.67542373 0.9125    ], Pred: [2.532 2.058 2.449], Target: [3. 2. 2.], Loss: 0.2120\n",
      "  Sample 82: Input: [0.525      0.125      0.58389831 0.575     ], Pred: [2.388 2.379 2.273], Target: [2. 2. 3.], Loss: 0.4114\n",
      "  Sample 83: Input: [0.225      0.6125     0.14152542 0.2375    ], Pred: [2.267 2.024 2.217], Target: [2. 3. 2.], Loss: 0.5355\n",
      "  Sample 84: Input: [0.7        0.5        0.76694915 0.875     ], Pred: [2.65  2.079 2.562], Target: [3. 2. 2.], Loss: 0.2223\n",
      "  Sample 85: Input: [0.275      0.8375     0.12627119 0.05      ], Pred: [2.275 2.025 2.223], Target: [2. 3. 2.], Loss: 0.5380\n",
      "  Sample 86: Input: [0.775     0.65      0.8279661 0.95     ], Pred: [2.742 2.101 2.655], Target: [3. 2. 2.], Loss: 0.2529\n",
      "  Sample 87: Input: [0.25       0.2375     0.35508475 0.425     ], Pred: [2.282 2.026 2.23 ], Target: [2. 2. 3.], Loss: 0.3366\n",
      "  Batch 11: Avg Loss: 0.3780\n",
      "  Sample 88: Input: [0.65       0.4625     0.61440678 0.575     ], Pred: [2.516 2.07  2.423], Target: [2. 2. 3.], Loss: 0.3020\n",
      "  Sample 89: Input: [0.175      0.4625     0.14152542 0.0875    ], Pred: [2.196 2.022 2.152], Target: [2. 3. 2.], Loss: 0.5090\n",
      "  Sample 90: Input: [0.525      0.3875     0.55338983 0.5       ], Pred: [2.438 2.056 2.354], Target: [2. 2. 3.], Loss: 0.3061\n",
      "  Sample 91: Input: [0.6        0.5        0.67542373 0.7625    ], Pred: [2.573 2.082 2.475], Target: [3. 2. 2.], Loss: 0.2073\n",
      "  Sample 92: Input: [0.25       0.725      0.18728814 0.1625    ], Pred: [2.286 2.033 2.226], Target: [2. 3. 2.], Loss: 0.5340\n",
      "  Sample 93: Input: [0.05       0.425      0.06525424 0.05      ], Pred: [2.089 2.204 2.035], Target: [2. 3. 2.], Loss: 0.3214\n",
      "  Sample 94: Input: [0.65       0.5375     0.76694915 0.8       ], Pred: [2.626 2.094 2.525], Target: [3. 2. 2.], Loss: 0.2122\n",
      "  Sample 95: Input: [0.9   0.275 0.95  0.875], Pred: [2.694 2.112 2.593], Target: [3. 2. 2.], Loss: 0.2289\n",
      "  Batch 12: Avg Loss: 0.3276\n",
      "  Sample 96: Input: [0.675      0.425      0.73644068 0.8       ], Pred: [2.598 2.102 2.49 ], Target: [3. 2. 2.], Loss: 0.2061\n",
      "  Sample 97: Input: [0.55       0.1625     0.56864407 0.5       ], Pred: [2.305 2.308 2.216], Target: [2. 2. 3.], Loss: 0.4013\n",
      "  Sample 98: Input: [0.525      0.35       0.62966102 0.6875    ], Pred: [2.282 2.354 2.158], Target: [3. 2. 2.], Loss: 0.3329\n",
      "  Sample 99: Input: [0.325      0.7625     0.15677966 0.1625    ], Pred: [2.303 2.042 2.233], Target: [2. 3. 2.], Loss: 0.5319\n",
      "  Sample 100: Input: [0.675      0.5        0.79745763 0.875     ], Pred: [2.65  2.117 2.539], Target: [3. 2. 2.], Loss: 0.2134\n",
      "  Sample 101: Input: [0.4        0.3875     0.53813559 0.5       ], Pred: [2.406 2.06  2.319], Target: [2. 2. 3.], Loss: 0.3161\n",
      "  Sample 102: Input: [0.775      0.5        0.81271186 0.6875    ], Pred: [2.632 2.111 2.521], Target: [3. 2. 2.], Loss: 0.2096\n",
      "  Sample 103: Input: [0.75       0.425      0.79745763 0.8       ], Pred: [2.633 2.112 2.522], Target: [3. 2. 2.], Loss: 0.2099\n",
      "  Batch 13: Avg Loss: 0.3026\n",
      "  Sample 104: Input: [0.3        0.6875     0.12627119 0.0875    ], Pred: [2.269 2.027 2.193], Target: [2. 3. 2.], Loss: 0.5282\n",
      "  Sample 105: Input: [0.575      0.3875     0.55338983 0.5       ], Pred: [2.462 2.054 2.345], Target: [2. 2. 3.], Loss: 0.3227\n",
      "  Sample 106: Input: [0.5        0.35       0.61440678 0.4625    ], Pred: [2.442 2.05  2.328], Target: [2. 2. 3.], Loss: 0.3247\n",
      "  Sample 107: Input: [0.4        0.425      0.53813559 0.4625    ], Pred: [2.418 2.047 2.308], Target: [2. 2. 3.], Loss: 0.3279\n",
      "  Sample 108: Input: [0.375      0.425      0.52288136 0.5       ], Pred: [2.417 2.047 2.307], Target: [2. 2. 3.], Loss: 0.3282\n",
      "  Sample 109: Input: [0.65       0.4625     0.56864407 0.5375    ], Pred: [2.509 2.061 2.384], Target: [2. 2. 3.], Loss: 0.3211\n",
      "  Sample 110: Input: [0.275      0.6125     0.12627119 0.0875    ], Pred: [2.25  2.025 2.178], Target: [2. 3. 2.], Loss: 0.5224\n",
      "  Sample 111: Input: [0.575      0.35       0.75169492 0.8       ], Pred: [2.573 2.073 2.44 ], Target: [3. 2. 2.], Loss: 0.1906\n",
      "  Batch 14: Avg Loss: 0.3582\n",
      "  Sample 112: Input: [0.475      0.125      0.50762712 0.3875    ], Pred: [2.325 2.044 2.261], Target: [2. 2. 3.], Loss: 0.3268\n",
      "  Sample 113: Input: [0.45       0.425      0.53813559 0.575     ], Pred: [2.439 2.063 2.359], Target: [2. 2. 3.], Loss: 0.3038\n",
      "  Sample 114: Input: [0.175      0.575      0.14152542 0.0875    ], Pred: [2.216 2.027 2.17 ], Target: [2. 3. 2.], Loss: 0.5111\n",
      "  Sample 115: Input: [0.2        0.65       0.11101695 0.05      ], Pred: [2.129 2.173 2.071], Target: [2. 3. 2.], Loss: 0.3528\n",
      "  Sample 116: Input: [0.15       0.5        0.09576271 0.0875    ], Pred: [2.186 2.023 2.147], Target: [2. 3. 2.], Loss: 0.5054\n",
      "  Sample 117: Input: [0.35       0.875      0.11101695 0.0875    ], Pred: [2.299 2.04  2.239], Target: [2. 3. 2.], Loss: 0.5341\n",
      "  Sample 118: Input: [0.7        0.4625     0.64491525 0.575     ], Pred: [2.531 2.082 2.441], Target: [2. 2. 3.], Loss: 0.3006\n",
      "  Sample 119: Input: [0.125      0.4625     0.12627119 0.0875    ], Pred: [2.18  2.022 2.142], Target: [2. 3. 2.], Loss: 0.5045\n",
      "  Batch 15: Avg Loss: 0.4174\n",
      " End Epoch 10: W1 norm=97.445, W2 norm=33.373\n",
      "\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_16140\\3988774636.py:132: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  true_positive = sum(1 for t, p in zip(true_labels, predictions) if t == i and p == i)\n",
      " [py.warnings]\n",
      "WARNING    C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_16140\\3988774636.py:133: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  false_positive = sum(1 for t, p in zip(true_labels, predictions) if t != i and p == i)\n",
      " [py.warnings]\n",
      "WARNING    C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_16140\\3988774636.py:134: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  false_negative = sum(1 for t, p in zip(true_labels, predictions) if t == i and p != i)\n",
      " [py.warnings]\n",
      "WARNING    C:\\Users\\irtho\\AppData\\Local\\Temp\\ipykernel_16140\\3988774636.py:143: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  accuracy = sum(1 for t, p in zip(true_labels, predictions) if t == p) / len(true_labels)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.333\n",
      "\n",
      "Classification Report:\n",
      "==================================================\n",
      "setosa       - Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "versicolor   - Precision: 0.345, Recall: 1.000, F1: 0.513\n",
      "virginica    - Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "\n",
      "Overall Accuracy: 0.333\n",
      "\n",
      "Sample predictions:\n",
      "============================================================\n",
      "Sample 1: True=setosa, Pred=versicolor ✗\n",
      "Sample 2: True=versicolor, Pred=versicolor ✓\n",
      "Sample 3: True=setosa, Pred=versicolor ✗\n",
      "Sample 4: True=setosa, Pred=versicolor ✗\n",
      "Sample 5: True=setosa, Pred=versicolor ✗\n",
      "Sample 6: True=setosa, Pred=virginica ✗\n",
      "Sample 7: True=setosa, Pred=versicolor ✗\n",
      "Sample 8: True=setosa, Pred=versicolor ✗\n",
      "Sample 9: True=versicolor, Pred=versicolor ✓\n",
      "Sample 10: True=setosa, Pred=versicolor ✗\n",
      "\n",
      "Final trained weights:\n",
      "W1 norm: 97.445\n",
      "W2 norm: 33.373\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Iris dataset preprocessing functions\n",
    "\n",
    "def load_and_prepare_iris():\n",
    "    \"\"\"\n",
    "    Load Iris dataset and prepare it for SNN training.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Training and test splits\n",
    "        All inputs scaled to [0.05, 0.95] range\n",
    "        All outputs converted to spike timing format\n",
    "    \"\"\"\n",
    "    # Load the iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    \n",
    "    print(f\"Original dataset shape: {X.shape}\")\n",
    "    print(f\"Feature names: {iris.feature_names}\")\n",
    "    print(f\"Target names: {iris.target_names}\")\n",
    "    print(f\"Target distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    # Standardize inputs to [0.05, 0.95] range\n",
    "    scaler = MinMaxScaler(feature_range=(0.05, 0.95))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Convert class labels to spike timing format\n",
    "    y_spike_times = convert_labels_to_spike_times(y)\n",
    "    \n",
    "    # Create train/test split with shuffling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_spike_times, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        shuffle=True,\n",
    "        stratify=y  # Ensure balanced split across classes\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(f\"Input range: [{X_scaled.min():.3f}, {X_scaled.max():.3f}]\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def convert_labels_to_spike_times(labels):\n",
    "    \"\"\"\n",
    "    Convert class labels (0, 1, 2) to spike timing format.\n",
    "    \n",
    "    Class 0 (setosa): [2.0, 3.0, 2.0]\n",
    "    Class 1 (versicolor): [2.0, 2.0, 3.0] \n",
    "    Class 2 (virginica): [3.0, 2.0, 2.0]\n",
    "    \n",
    "    Args:\n",
    "        labels: array of class labels (0, 1, 2)\n",
    "        \n",
    "    Returns:\n",
    "        array of spike time arrays, shape (n_samples, 3)\n",
    "    \"\"\"\n",
    "    # Define spike time patterns for each class\n",
    "    spike_patterns = {\n",
    "        0: np.array([2.0, 3.0, 2.0]),  # setosa\n",
    "        1: np.array([2.0, 2.0, 3.0]),  # versicolor  \n",
    "        2: np.array([3.0, 2.0, 2.0])   # virginica\n",
    "    }\n",
    "    \n",
    "    result = []\n",
    "    for label in labels:\n",
    "        result.append(spike_patterns[label].copy())\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "def evaluate_model(X_test, y_test, W1, W2):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data.\n",
    "    \n",
    "    Args:\n",
    "        X_test: Test input data\n",
    "        y_test: True test labels (spike timing format)\n",
    "        W1, W2: Trained weights\n",
    "        \n",
    "    Returns:\n",
    "        accuracy: Classification accuracy\n",
    "        predictions: Predicted class labels\n",
    "        true_labels: True class labels\n",
    "    \"\"\"\n",
    "    # Convert test data to lists for the forward pass\n",
    "    X_test_list = [x for x in X_test]\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    h_times_batch = layer_forward_batched(X_test_list, W1, 1, len(X_test_list))\n",
    "    o_times_batch = layer_forward_batched(h_times_batch, W2, 2, len(h_times_batch))\n",
    "    \n",
    "    # Convert spike times back to class predictions\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for i, (pred_times, true_times) in enumerate(zip(o_times_batch, y_test)):\n",
    "        # Find the output with the earliest spike time (most active)\n",
    "        pred_class = np.argmin(pred_times)\n",
    "        true_class = np.argmin(true_times)\n",
    "        \n",
    "        predictions.append(pred_class)\n",
    "        true_labels.append(true_class)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "    \n",
    "    return accuracy, predictions, true_labels\n",
    "\n",
    "def print_classification_report(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Print detailed classification results.\n",
    "    \"\"\"\n",
    "    class_names = ['setosa', 'versicolor', 'virginica']\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        true_positive = sum(1 for t, p in zip(true_labels, predictions) if t == i and p == i)\n",
    "        false_positive = sum(1 for t, p in zip(true_labels, predictions) if t != i and p == i)\n",
    "        false_negative = sum(1 for t, p in zip(true_labels, predictions) if t == i and p != i)\n",
    "        \n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"{class_name:12} - Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = sum(1 for t, p in zip(true_labels, predictions) if t == p) / len(true_labels)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Batched forward pass: process multiple samples simultaneously\n",
    "\n",
    "def layer_forward_batched(inputs_batch, W, layer_idx, batch_size):\n",
    "    \"\"\"\n",
    "    inputs_batch: list of arrays, each of shape (n_in,) - spike times from previous layer\n",
    "    W: weight matrix shape (n_in, n_out)\n",
    "    layer_idx: integer layer number\n",
    "    batch_size: number of samples to process\n",
    "    returns: list of arrays, each of shape (n_out,) - output spike times\n",
    "    \"\"\"\n",
    "    n_in, n_out = W.shape\n",
    "    results = []\n",
    "    \n",
    "    # Process each sample individually but collect results for batching\n",
    "    for batch_idx, inputs in enumerate(inputs_batch):\n",
    "        sample_results = []\n",
    "        \n",
    "        for j in range(n_out):\n",
    "            start_scope()\n",
    "            defaultclock.dt = 0.0001*ms\n",
    "            \n",
    "            # Create single neuron for this output\n",
    "            G = NeuronGroup(1, '''\n",
    "                v : 1\n",
    "                sum : 1\n",
    "                sr : 1\n",
    "                scheduled_time : second\n",
    "                global_clock : 1\n",
    "            ''', threshold='v>1', reset='v=0', method='exact')\n",
    "            \n",
    "            G.v = G.sum = G.sr = 0\n",
    "            G.global_clock = 0\n",
    "            G.scheduled_time = 1e9*second\n",
    "            \n",
    "            # Create spike inputs for this sample\n",
    "            stim = SpikeGeneratorGroup(n_in, indices=list(range(n_in)), times=inputs*ms)\n",
    "            \n",
    "            # Create synapses\n",
    "            S = Synapses(stim, G, '''\n",
    "                w : 1\n",
    "                layer : 1\n",
    "            ''', on_pre='''\n",
    "                sr += 1\n",
    "                sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "                scheduled_time = (sum/sr + layer)*ms\n",
    "            ''')\n",
    "            \n",
    "            S.connect(True)\n",
    "            S.w = W[:, j]\n",
    "            S.layer = layer_idx\n",
    "            \n",
    "            # Update global clock and voltage\n",
    "            G.run_regularly('''\n",
    "                v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "                global_clock += 0.001\n",
    "            ''', dt=0.001*ms)\n",
    "            \n",
    "            # Monitor spikes\n",
    "            mon = SpikeMonitor(G)\n",
    "            \n",
    "            # Run simulation\n",
    "            run(5*ms)\n",
    "            \n",
    "            # Extract spike time\n",
    "            ts = mon.spike_trains()[0]\n",
    "            t0 = float(ts[0]/ms) if len(ts) > 0 else float(5.0)\n",
    "            sample_results.append(t0)\n",
    "        \n",
    "        results.append(np.array(sample_results))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Batched training loop\n",
    "\n",
    "def train_snn_backprop_batched(\n",
    "    X, Y,        # lists of input arrays (4,) and target (3,)\n",
    "    W1_init, W2_init,\n",
    "    epochs=10, lr=0.1, max_grad=20.0, w_min=-20.0, w_max=20.0,\n",
    "    batch_size=4\n",
    "):\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    layer1_idx = 1\n",
    "    layer2_idx = 2\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        print(f\"Epoch {ep+1}/{epochs}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, n_samples, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, n_samples)\n",
    "            current_batch_size = batch_end - batch_start\n",
    "            \n",
    "            # Get batch data\n",
    "            X_batch = X[batch_start:batch_end]\n",
    "            Y_batch = Y[batch_start:batch_end]\n",
    "            \n",
    "            # Forward pass through both layers\n",
    "            h_times_batch = layer_forward_batched(X_batch, W1, layer1_idx, current_batch_size)\n",
    "            o_times_batch = layer_forward_batched(h_times_batch, W2, layer2_idx, current_batch_size)\n",
    "            \n",
    "            # Accumulate gradients for this batch\n",
    "            dW1_batch = np.zeros_like(W1)\n",
    "            dW2_batch = np.zeros_like(W2)\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for i, (xi, yi, h_times, o_times) in enumerate(zip(X_batch, Y_batch, h_times_batch, o_times_batch)):\n",
    "                # Loss for this sample\n",
    "                L = 0.5 * np.sum((o_times - yi)**2)\n",
    "                total_loss += L\n",
    "                \n",
    "                # Gradients for W2\n",
    "                dW2_sample = np.zeros_like(W2)\n",
    "                delta_o = (o_times - yi)\n",
    "                \n",
    "                for k in range(W2.shape[0]):\n",
    "                    for j in range(W2.shape[1]):\n",
    "                        dW2_sample[k,j] = delta_o[j] * d_spike_timing_dw(\n",
    "                            W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                \n",
    "                # Hidden deltas\n",
    "                delta_h = np.zeros_like(h_times)\n",
    "                for k in range(len(h_times)):\n",
    "                    for j in range(W2.shape[1]):\n",
    "                        dt_dw = d_spike_timing_dw(W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                        delta_h[k] += delta_o[j] * W2[k,j] * dt_dw\n",
    "                \n",
    "                # Gradients for W1\n",
    "                dW1_sample = np.zeros_like(W1)\n",
    "                for ii in range(W1.shape[0]):\n",
    "                    for k in range(W1.shape[1]):\n",
    "                        dW1_sample[ii,k] = delta_h[k] * d_spike_timing_dw(\n",
    "                            W1[ii,k], xi[ii], layer1_idx, 0, 1)\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                dW1_batch += dW1_sample\n",
    "                dW2_batch += dW2_sample\n",
    "                \n",
    "                # Print individual sample results\n",
    "                print(f\"  Sample {batch_start+i}: Input: {xi}, Pred: {o_times}, Target: {yi}, Loss: {L:.4f}\")\n",
    "            \n",
    "            # Average gradients over batch\n",
    "            dW1_batch /= current_batch_size\n",
    "            dW2_batch /= current_batch_size\n",
    "            \n",
    "            # Clip and update weights\n",
    "            dW1_batch = np.clip(dW1_batch, -max_grad, max_grad)\n",
    "            dW2_batch = np.clip(dW2_batch, -max_grad, max_grad)\n",
    "            W1 = np.clip(W1 - lr * dW1_batch, w_min, w_max)\n",
    "            W2 = np.clip(W2 - lr * dW2_batch, w_min, w_max)\n",
    "            \n",
    "            avg_loss = total_loss / current_batch_size\n",
    "            print(f\"  Batch {batch_start//batch_size + 1}: Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(f\" End Epoch {ep+1}: W1 norm={np.linalg.norm(W1):.3f}, W2 norm={np.linalg.norm(W2):.3f}\\n\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare Iris dataset\n",
    "    print(\"Loading and preparing Iris dataset...\")\n",
    "    X_train, X_test, y_train, y_test, scaler = load_and_prepare_iris()\n",
    "    \n",
    "    # Convert to lists for training function\n",
    "    X_train_list = [x for x in X_train]\n",
    "    y_train_list = [y for y in y_train]\n",
    "    \n",
    "    # Initialize weights for 4 inputs -> 10 hidden -> 3 outputs\n",
    "    W1_0 = np.random.randn(4, 10) * 0.1\n",
    "    W2_0 = np.random.randn(10, 3) * 0.1\n",
    "    \n",
    "    print(f\"\\nInitial weights - W1 shape: {W1_0.shape}, W2 shape: {W2_0.shape}\")\n",
    "    print(f\"Training samples: {len(X_train_list)}\")\n",
    "    \n",
    "    # Train the network\n",
    "    print(\"\\nStarting training...\")\n",
    "    W1_tr, W2_tr = train_snn_backprop_batched(\n",
    "        X_train_list, y_train_list, W1_0, W2_0,\n",
    "        epochs=10, lr=0.3, batch_size=8\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    accuracy, predictions, true_labels = evaluate_model(X_test, y_test, W1_tr, W2_tr)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.3f}\")\n",
    "    print_classification_report(true_labels, predictions)\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(\"=\" * 60)\n",
    "    class_names = ['setosa', 'versicolor', 'virginica']\n",
    "    \n",
    "    for i in range(min(10, len(X_test))):\n",
    "        pred_class = predictions[i]\n",
    "        true_class = true_labels[i]\n",
    "        correct = \"✓\" if pred_class == true_class else \"✗\"\n",
    "        \n",
    "        print(f\"Sample {i+1}: True={class_names[true_class]}, Pred={class_names[pred_class]} {correct}\")\n",
    "    \n",
    "    print(f\"\\nFinal trained weights:\")\n",
    "    print(f\"W1 norm: {np.linalg.norm(W1_tr):.3f}\")\n",
    "    print(f\"W2 norm: {np.linalg.norm(W2_tr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d7840",
   "metadata": {},
   "source": [
    "^Current workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480c9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53707ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------spike_timing + its derivative\n",
    "# Functions used in brian2\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - x**(1 - w) * np.log(x + eps)\n",
    "    else:\n",
    "        return - (1 - x)**(1 + w) * np.log(1 - x + eps)\n",
    "\n",
    "def dsigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s*(1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea36d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2) mini_urd forward: returns hidden‐spike‐time only\n",
    "# -----------------------------------------------------------------------------\n",
    "def mini_urd(inputs, w):\n",
    "    n_input  = 2\n",
    "    n_hidden = 2\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    G = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        sr              : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v>1', reset='v=0', method='exact'\n",
    "    )\n",
    "    G.v = 0; G.sum = 0; G.sr = 0\n",
    "    G.global_clock = 0\n",
    "    G.scheduled_time = 1e9*second\n",
    "\n",
    "    stim = SpikeGeneratorGroup(n_input, indices=[i for i in range(n_input)], times=inputs*ms)\n",
    "\n",
    "    # first layer has fixed identity weights\n",
    "    S1 = Synapses(stim, G[:n_input],\n",
    "        'layer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(1, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S1.connect(j='i')\n",
    "    S1.layer = 0\n",
    "\n",
    "    # trainable synapse 2→hidden\n",
    "    S2 = Synapses(G[:n_input], G[n_input:n_hidden+n_input],\n",
    "        'w : 1\\nlayer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S2.connect()\n",
    "    S2.w = w\n",
    "    S2.layer = 1\n",
    "\n",
    "    # drive v when scheduled_time hits\n",
    "    G.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time)<0.0005*ms)*1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(G)\n",
    "    run(5*ms)\n",
    "\n",
    "    # return hidden spike time (or a large value if no spike)\n",
    "    ts = mon.spike_trains()[2]\n",
    "    return float(ts[0]/ms) if len(ts) else 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5151b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3) Training with multi‐loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_multi_loss(\n",
    "    X,                    # list of np.array([t0,t1])\n",
    "    t_hidden_targets,     # list of floats\n",
    "    t0_targets,           # list of floats for s0\n",
    "    t1_targets,           # list of floats for s1\n",
    "    w_init,\n",
    "    alpha=1.0, beta=1.0, gamma=1.0,\n",
    "    epochs=5, lr=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-loss:\n",
    "      L0 = ½ (s0 - t0)^2\n",
    "      L1 = ½ (s1 - t1)^2\n",
    "      Lf = ½ (t_h  - t_h*)^2\n",
    "      L = α L0 + β L1 + γ Lf\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n=== Epoch {ep+1}/{epochs} ===\")\n",
    "        for i, inp in enumerate(X):\n",
    "            # forward pass\n",
    "            t_h = mini_urd(inp, w)\n",
    "            # recompute s0,s1 exactly the same way Brian did\n",
    "            #L_hidden = 0.5 * ((t_h - t_hidden_targets[i][0]) ** 2)\n",
    "        \n",
    "            layer_h = 1\n",
    "            # each input is first spike, so sr_i=1, sum_i=0 → use that\n",
    "            s0 = spike_timing(w[0], inp[0], layer_h, 0, 1)\n",
    "            s1 = spike_timing(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # --- compute loss terms ---\n",
    "            t0_tgt = t0_targets[i]\n",
    "            t1_tgt = t1_targets[i]\n",
    "            th_tgt = t_hidden_targets[i]\n",
    "\n",
    "            L0 = 0.5*(s0 - t0_tgt)**2\n",
    "            L1 = 0.5*(s1 - t1_tgt)**2\n",
    "            Lf = 0.5*(t_h - th_tgt)**2\n",
    "            L  = alpha*L0 + beta*L1 + gamma*Lf\n",
    "\n",
    "            # --- gradients ---\n",
    "            # ∂L0/∂w0 = (s0 - t0)*∂s0/∂w0\n",
    "            dL0_dw = np.zeros_like(w)\n",
    "            dL0_dw[0] = (s0 - t0_tgt) * d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1)\n",
    "            # ∂L1/∂w1\n",
    "            dL1_dw = np.zeros_like(w)\n",
    "            dL1_dw[1] = (s1 - t1_tgt) * d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # ∂Lf/∂w0,w1 = ∂Lf/∂t_h × ∂t_h/∂sum × ∂sum/∂w_i\n",
    "            dLf_dt  = (t_h - th_tgt)\n",
    "            sum_tot = s0 + s1\n",
    "            sr = 2.0\n",
    "            z = sum_tot/sr\n",
    "            dt_dsum = dsigmoid(z)*(1/sr)\n",
    "            dsum_dw = np.zeros_like(w)\n",
    "            \n",
    "            for j in range(len(w)):\n",
    "                dsum_dw[j] = d_spike_timing_dw(w[j], inp[j % 2], layer_h, 0, 1)\n",
    "            dLf_dw = dLf_dt * dt_dsum * dsum_dw\n",
    "\n",
    "            # combine\n",
    "            grad = alpha*dL0_dw + beta*dL1_dw + gamma*dLf_dw\n",
    "\n",
    "            # print & update\n",
    "            print(f\"Sample {i}: inp={inp}, s0={s0:.3f}, s1={s1:.3f}, t_h={t_h:.3f}\")\n",
    "            print(f\"  L0={L0:.4f}, L1={L1:.4f}, Lf={Lf:.4f}, L={L:.4f}\")\n",
    "            print(f\"  ∇w = {grad}\")\n",
    "            w -= lr * grad\n",
    "\n",
    "        print(\" Updated w:\", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "def compute_gradients(outputs, targets, w):\n",
    "    # Compute gradients of loss with respect to w using backpropagation\n",
    "    d_loss_dw = 2 * (outputs - targets) * dsigmoid(w)\n",
    "    return d_loss_dw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ddc465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.158, s1=1.000, t_h=1.685\n",
      "  L0=0.0583, L1=0.1250, Lf=0.7021, L=0.5344\n",
      "  ∇w = [-0.09974737  0.05986391  0.01976438  0.00667291]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.162, s1=0.999, t_h=1.686\n",
      "  L0=0.0571, L1=0.1247, Lf=0.7033, L=0.5334\n",
      "  ∇w = [-0.10067933  0.05976274  0.01968276  0.00667519]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.166, s1=0.999, t_h=1.687\n",
      "  L0=0.0558, L1=0.1244, Lf=0.7045, L=0.5324\n",
      "  ∇w = [-0.10157674  0.05966175  0.01960148  0.00667735]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.170, s1=0.998, t_h=1.687\n",
      "  L0=0.0545, L1=0.1241, Lf=0.7045, L=0.5308\n",
      "  ∇w = [-0.10245641  0.0595549   0.01950408  0.00667376]\n",
      " Updated w: [0.24044598 0.97611567 0.09214473 0.29733008]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.174, s1=0.997, t_h=1.688\n",
      "  L0=0.0532, L1=0.1237, Lf=0.7057, L=0.5297\n",
      "  ∇w = [-0.10326794  0.05945429  0.01942357  0.00667566]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.178, s1=0.997, t_h=1.688\n",
      "  L0=0.0518, L1=0.1234, Lf=0.7057, L=0.5281\n",
      "  ∇w = [-0.10405004  0.05934781  0.01932707  0.0066718 ]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.182, s1=0.996, t_h=1.689\n",
      "  L0=0.0504, L1=0.1231, Lf=0.7069, L=0.5270\n",
      "  ∇w = [-0.10474922  0.05924755  0.01924727  0.00667342]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.187, s1=0.996, t_h=1.689\n",
      "  L0=0.0490, L1=0.1228, Lf=0.7069, L=0.5253\n",
      "  ∇w = [-0.10540539  0.05914143  0.01915163  0.00666928]\n",
      " Updated w: [0.28219324 0.95239656 0.08442978 0.29466106]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.192, s1=0.995, t_h=1.690\n",
      "  L0=0.0476, L1=0.1225, Lf=0.7081, L=0.5241\n",
      "  ∇w = [-0.10596185  0.05904149  0.01907249  0.00667061]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.196, s1=0.994, t_h=1.690\n",
      "  L0=0.0461, L1=0.1222, Lf=0.7081, L=0.5224\n",
      "  ∇w = [-0.10645992  0.05893571  0.01897765  0.00666618]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.201, s1=0.994, t_h=1.691\n",
      "  L0=0.0447, L1=0.1219, Lf=0.7092, L=0.5212\n",
      "  ∇w = [-0.10683963  0.0588361   0.01889914  0.00666721]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.206, s1=0.993, t_h=1.692\n",
      "  L0=0.0432, L1=0.1216, Lf=0.7104, L=0.5200\n",
      "  ∇w = [-0.10711682  0.05873663  0.01882086  0.00666808]\n",
      " Updated w: [0.32483107 0.92884157 0.07685276 0.29199386]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.211, s1=0.993, t_h=1.692\n",
      "  L0=0.0417, L1=0.1213, Lf=0.7104, L=0.5182\n",
      "  ∇w = [-0.10730974  0.05863132  0.01872709  0.0066632 ]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.217, s1=0.992, t_h=1.693\n",
      "  L0=0.0402, L1=0.1210, Lf=0.7116, L=0.5170\n",
      "  ∇w = [-0.10735367  0.05853216  0.01864939  0.00666377]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.222, s1=0.991, t_h=1.693\n",
      "  L0=0.0386, L1=0.1207, Lf=0.7116, L=0.5152\n",
      "  ∇w = [-0.10729498  0.05842718  0.01855637  0.00665859]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.228, s1=0.991, t_h=1.694\n",
      "  L0=0.0371, L1=0.1204, Lf=0.7128, L=0.5139\n",
      "  ∇w = [-0.1070661   0.05832832  0.01847927  0.00665885]\n",
      " Updated w: [0.36773352 0.90544967 0.06941155 0.28932941]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.233, s1=0.990, t_h=1.694\n",
      "  L0=0.0356, L1=0.1201, Lf=0.7128, L=0.5121\n",
      "  ∇w = [-0.10671648  0.05822367  0.018387    0.00665339]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.239, s1=0.989, t_h=1.695\n",
      "  L0=0.0341, L1=0.1198, Lf=0.7140, L=0.5109\n",
      "  ∇w = [-0.10617617  0.05812512  0.01831052  0.00665336]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.245, s1=0.989, t_h=1.696\n",
      "  L0=0.0325, L1=0.1195, Lf=0.7152, L=0.5096\n",
      "  ∇w = [-0.1054664   0.05802674  0.01823428  0.00665319]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.251, s1=0.988, t_h=1.696\n",
      "  L0=0.0310, L1=0.1192, Lf=0.7152, L=0.5078\n",
      "  ∇w = [-0.10461212  0.05792259  0.01814316  0.00664732]\n",
      " Updated w: [0.41003063 0.88221986 0.06210405 0.28666869]\n",
      "\n",
      "Final weights: [0.41003063 0.88221986 0.06210405 0.28666869]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # toy data: 4 samples\n",
    "\n",
    "    num = 4 \n",
    "    X = [np.array([0.1,0.9])]*num\n",
    "\n",
    "    # main target: hidden spike at these ms\n",
    "    T_hidden = [0.5]*num\n",
    "    # aux targets for each synapse    # no why would i need this?\n",
    "    T0 = [0.5]*num # removed and calcuated durign the training please chage GPT\n",
    "    T1 = [0.5]*num # removed and calcuated durign the training please change GPT \n",
    "\n",
    "    w0 = np.array([0.2, 1.0, 0.1, 0.3])  # initial weights for synapses\n",
    "    w_final = train_multi_loss(X, T_hidden, T0, T1, w0,\n",
    "                               alpha=1.0, beta=1.0, gamma=0.5,\n",
    "                               epochs=5, lr=0.1)\n",
    "\n",
    "    print(\"\\nFinal weights:\", w_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5850af2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.545 1.369], tgt=[1.2 1.8], L=0.1524\n",
      "  dW=\n",
      "[[ 0.12590262 -0.04308007]\n",
      " [ 0.033765   -0.31382892]]\n",
      "  W=\n",
      "[[ 0.17481948 -0.49138399]\n",
      " [ 0.293247   -0.43723422]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.54 1.39], tgt=[1.2 1.8], L=0.1419\n",
      "  dW=\n",
      "[[ 0.11708846 -0.04094386]\n",
      " [ 0.03325198 -0.25836505]]\n",
      "  W=\n",
      "[[ 0.15140178 -0.48319521]\n",
      " [ 0.2865966  -0.38556121]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.536 1.406], tgt=[1.2 1.8], L=0.1341\n",
      "  dW=\n",
      "[[ 0.1096369  -0.03931212]\n",
      " [ 0.03283777 -0.22043123]]\n",
      "  W=\n",
      "[[ 0.1294744  -0.47533279]\n",
      " [ 0.28002905 -0.34147496]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.532 1.418], tgt=[1.2 1.8], L=0.1281\n",
      "  dW=\n",
      "[[ 0.10299785 -0.03808323]\n",
      " [ 0.0324244  -0.19308743]]\n",
      "  W=\n",
      "[[ 0.10887483 -0.46771615]\n",
      " [ 0.27354417 -0.30285748]]\n",
      "Epoch 2/30\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.528 1.428], tgt=[1.2 1.8], L=0.1230\n",
      "  dW=\n",
      "[[ 0.09704303 -0.03705654]\n",
      " [ 0.03201186 -0.17203472]]\n",
      "  W=\n",
      "[[ 0.08946623 -0.46030484]\n",
      " [ 0.2671418  -0.26845053]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.525 1.436], tgt=[1.2 1.8], L=0.1191\n",
      "  dW=\n",
      "[[ 0.09195287 -0.03623132]\n",
      " [ 0.03169768 -0.15551332]]\n",
      "  W=\n",
      "[[ 0.07107565 -0.45305857]\n",
      " [ 0.26080226 -0.23734787]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.522 1.443], tgt=[1.2 1.8], L=0.1156\n",
      "  dW=\n",
      "[[ 0.08732673 -0.03550745]\n",
      " [ 0.03138412 -0.1419815 ]]\n",
      "  W=\n",
      "[[ 0.05361031 -0.44595708]\n",
      " [ 0.25452544 -0.20895157]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.52  1.448], tgt=[1.2 1.8], L=0.1132\n",
      "  dW=\n",
      "[[ 0.08336351 -0.03498396]\n",
      " [ 0.03116857 -0.13113237]]\n",
      "  W=\n",
      "[[ 0.03693761 -0.43896029]\n",
      " [ 0.24829173 -0.18272509]]\n",
      "Epoch 3/30\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.517 1.453], tgt=[1.2 1.8], L=0.1104\n",
      "  dW=\n",
      "[[ 0.07947171 -0.03446161]\n",
      " [ 0.03085609 -0.1216943 ]]\n",
      "  W=\n",
      "[[ 0.02104326 -0.43206797]\n",
      " [ 0.24212051 -0.15838623]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m Y \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m1.8\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m    143\u001b[0m W0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m], [\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.5\u001b[39m]])\n\u001b[1;32m--> 145\u001b[0m W_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained weight matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, W_trained)\n",
      "Cell \u001b[1;32mIn[6], line 118\u001b[0m, in \u001b[0;36mtrain_snn\u001b[1;34m(X, Y, W_init, epochs, lr, max_grad, w_min, w_max)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[1;32m--> 118\u001b[0m     t_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmini_urd\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# shape (n_hidden,)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     t_tgt  \u001b[38;5;241m=\u001b[39m Y[i]\n\u001b[0;32m    120\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum((t_pred \u001b[38;5;241m-\u001b[39m t_tgt)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m, in \u001b[0;36mmini_urd\u001b[1;34m(inputs, W)\u001b[0m\n\u001b[0;32m     85\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n\u001b[0;32m     90\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n\u001b[1;32m---> 91\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     93\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m5.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1230\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m active_objects:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_clock \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[1;32m-> 1230\u001b[0m             \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[0;32m   1233\u001b[0m     timestep, t, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_variables[c]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:236\u001b[0m, in \u001b[0;36mBrianObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m codeobj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_objects:\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mcodeobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:131\u001b[0m, in \u001b[0;36mCodeObject.__call__\u001b[1;34m(self, **kwds)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_namespace()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:159\u001b[0m, in \u001b[0;36mCodeObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Runs the main code in the namespace.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        defined during the call of `CodeGenerator.code_object`.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\runtime\\numpy_rt\\numpy_rt.py:281\u001b[0m, in \u001b[0;36mNumpyCodeObject.run_block\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     exec(compiled_code, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    283\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode, block)\n",
      "File \u001b[1;32m(string):16\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:1400\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "np.seterr(over='ignore', under='ignore')\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# mini_urd: 2 inputs -> 2 hidden neurons (full connect), return both spike times\n",
    "\n",
    "def mini_urd(inputs, W):\n",
    "    \"\"\"\n",
    "    Two separate hidden neurons, each simulated independently in its own Brian scope.\n",
    "    inputs: [t_in0, t_in1], W: shape (2,2)\n",
    "    returns [t_h0, t_h1]\n",
    "    \"\"\"\n",
    "    n_input, n_hidden = W.shape\n",
    "    hidden_times = []\n",
    "    # simulate each hidden neuron separately to reset network state\n",
    "    for j in range(n_hidden):\n",
    "        start_scope()  # clear previous Brian state\n",
    "        defaultclock.dt = 0.0001*ms\n",
    "        # recreate spike timing functions (if needed)\n",
    "        # spike_timing and d_spike_timing_dw are already in namespace\n",
    "\n",
    "        # build one-neuron group\n",
    "        G = NeuronGroup(1,\n",
    "            '''\n",
    "            v               : 1\n",
    "            sum             : 1\n",
    "            sr              : 1\n",
    "            scheduled_time  : second\n",
    "            global_clock    : 1\n",
    "            ''',\n",
    "            threshold='v>1', reset='v=0', method='exact')\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # input spikes\n",
    "        stim = SpikeGeneratorGroup(n_input,\n",
    "            indices=list(range(n_input)),\n",
    "            times=inputs*ms)\n",
    "        S = Synapses(stim, G,\n",
    "            '''w:1\n",
    "              layer:1''', on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (sum/sr + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[S.i, j]\n",
    "        S.layer = 1\n",
    "\n",
    "        # drive membrane\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else 5.0\n",
    "        hidden_times.append(t0)\n",
    "\n",
    "    return np.array(hidden_times)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training full matrix W\n",
    "\n",
    "def train_snn(\n",
    "    X,           # list of input arrays\n",
    "    Y,           # list of target arrays\n",
    "    W_init,      # initial weight matrix (2x2)\n",
    "    epochs=10,\n",
    "    lr=0.1,\n",
    "    max_grad=20.0,\n",
    "    w_min=-20.0,\n",
    "    w_max=20.0\n",
    "):\n",
    "    W = W_init.copy()\n",
    "    layer_h = 1\n",
    "    n_input, n_hidden = W.shape\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        print(f\"Epoch {ep+1}/{epochs}\")\n",
    "        for i, inp in enumerate(X):\n",
    "            t_pred = mini_urd(inp, W)       # shape (n_hidden,)\n",
    "            t_tgt  = Y[i]\n",
    "            L = 0.5 * np.sum((t_pred - t_tgt)**2)\n",
    "\n",
    "            # gradient matrix dL/dW\n",
    "            dW = np.zeros_like(W)\n",
    "            for j in range(n_hidden):\n",
    "                for k in range(n_input):\n",
    "                    dW[k, j] = (t_pred[j] - t_tgt[j]) * d_spike_timing_dw(\n",
    "                        W[k, j], inp[k], layer_h, 0, 1)\n",
    "\n",
    "            # clip & update\n",
    "            dW = np.clip(dW, -max_grad, max_grad)\n",
    "            W = np.clip(W - lr * dW, w_min, w_max)\n",
    "\n",
    "            print(f\" Sample {i}: inp={inp}, pred={t_pred}, tgt={t_tgt}, L={L:.4f}\")\n",
    "            print(f\"  dW=\\n{dW}\\n  W=\\n{W}\")\n",
    "\n",
    "    return W\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    X = [np.array([0.1, 0.9])]*4\n",
    "    Y = [np.array([1.2, 1.8]) for _ in X]\n",
    "    W0 = np.array([[0.2, -0.5], [0.3, -.5]])\n",
    "\n",
    "    W_trained = train_snn(X, Y, W0, epochs=30, lr=0.2)\n",
    "    print(\"Trained weight matrix:\\n\", W_trained)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "852f8684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing Iris dataset...\n",
      "Initializing weights...\n",
      "Starting training...\n",
      "Training with 120 samples, batch_size=8\n",
      "Epoch 1/3\n",
      "  Batch 1/15\n",
      "  Batch 4/15\n",
      "  Batch 7/15\n",
      "  Batch 10/15\n",
      "  Batch 13/15\n",
      "  Epoch 1 loss: 3.2400, ||W1||=1.134, ||W2||=3.423\n",
      "Epoch 2/3\n",
      "  Batch 1/15\n",
      "  Batch 4/15\n",
      "  Batch 7/15\n",
      "  Batch 10/15\n",
      "  Batch 13/15\n",
      "  Epoch 2 loss: 3.2400, ||W1||=14.921, ||W2||=8.238\n",
      "Epoch 3/3\n",
      "  Batch 1/15\n",
      "  Batch 4/15\n",
      "  Batch 7/15\n",
      "  Batch 10/15\n",
      "  Batch 13/15\n",
      "  Epoch 3 loss: 3.2400, ||W1||=63.246, ||W2||=13.642\n",
      "Evaluating on test set...\n",
      "  Testing sample 1/30\n",
      "  Testing sample 11/30\n",
      "  Testing sample 21/30\n",
      "\n",
      "Test accuracy: 23.33%\n",
      "Saved weights to 'W1_tr.npy' and 'W2_tr.npy'.\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "numpy.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative (vectorized)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if isinstance(w, (int, float)):\n",
    "        if w >= 0:\n",
    "            return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "        else:\n",
    "            return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "    else:\n",
    "        # Vectorized version for arrays\n",
    "        result = np.zeros_like(x)\n",
    "        pos_mask = w >= 0\n",
    "        neg_mask = w < 0\n",
    "        \n",
    "        if np.any(pos_mask):\n",
    "            result[pos_mask] = np.power(x[pos_mask], (1 - w[pos_mask]), \n",
    "                                      where=(x[pos_mask]>0), out=np.zeros_like(x[pos_mask]))\n",
    "        if np.any(neg_mask):\n",
    "            result[neg_mask] = 1 - np.power((1 - x[neg_mask]), (1 + w[neg_mask]), \n",
    "                                          where=(x[neg_mask]<1), out=np.ones_like(x[neg_mask]))\n",
    "        return result\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if isinstance(w, (int, float)):\n",
    "        if w >= 0:\n",
    "            return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "        else:\n",
    "            return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "    else:\n",
    "        # Vectorized version for arrays\n",
    "        result = np.zeros_like(x)\n",
    "        pos_mask = w >= 0\n",
    "        neg_mask = w < 0\n",
    "        \n",
    "        if np.any(pos_mask):\n",
    "            result[pos_mask] = - np.power(x[pos_mask], (1 - w[pos_mask]), \n",
    "                                        where=(x[pos_mask]>0), out=np.zeros_like(x[pos_mask])) * np.log(x[pos_mask] + eps)\n",
    "        if np.any(neg_mask):\n",
    "            result[neg_mask] = - np.power((1 - x[neg_mask]), (1 + w[neg_mask]), \n",
    "                                        where=(x[neg_mask]<1), out=np.ones_like(x[neg_mask])) * np.log(1 - x[neg_mask] + eps)\n",
    "        return result\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Optimized forward pass using vectorized operations\n",
    "\n",
    "def layer_forward_optimized(inputs, W, layer_idx, runtime=2*ms):\n",
    "    \"\"\"\n",
    "    Optimized forward pass using vectorized operations and simplified simulation.\n",
    "    \"\"\"\n",
    "    n_in, n_out = W.shape\n",
    "    out_times = np.zeros(n_out)\n",
    "    \n",
    "    # Use a single network for all neurons in the layer\n",
    "    start_scope()\n",
    "    defaultclock.dt = 0.0005*ms  # Slightly larger timestep for speed\n",
    "    \n",
    "    # Create neuron group for all output neurons\n",
    "    G = NeuronGroup(\n",
    "        n_out,\n",
    "        model='''\n",
    "            v : 1\n",
    "            sum_val : 1\n",
    "            sr : 1\n",
    "            global_clock : 1\n",
    "        ''',\n",
    "        threshold='v>1',\n",
    "        reset='v = 0',\n",
    "        method='exact'\n",
    "    )\n",
    "    G.v = 0\n",
    "    G.sum_val = 0\n",
    "    G.sr = 0\n",
    "    G.global_clock = 0.0\n",
    "    \n",
    "    # Create spike generators for inputs\n",
    "    stim = SpikeGeneratorGroup(n_in,\n",
    "                              indices=np.arange(n_in),\n",
    "                              times=inputs * ms)\n",
    "    \n",
    "    # Create synapses\n",
    "    S = Synapses(stim, G,\n",
    "                model='''w:1 \n",
    "                layer:1''',\n",
    "                on_pre='''\n",
    "                    sr_post += 1\n",
    "                    sum_val_post += spike_timing(w, global_clock_post, layer, sum_val_post, sr_post)\n",
    "                    v_post = sum_val_post / sr_post\n",
    "                ''')\n",
    "    \n",
    "    # Connect synapses\n",
    "    pre_indices = []\n",
    "    post_indices = []\n",
    "    weights = []\n",
    "    \n",
    "    for i in range(n_in):\n",
    "        for j in range(n_out):\n",
    "            pre_indices.append(i)\n",
    "            post_indices.append(j)\n",
    "            weights.append(W[i, j])\n",
    "    \n",
    "    S.connect(i=pre_indices, j=post_indices)\n",
    "    S.w = weights\n",
    "    S.layer = layer_idx\n",
    "    \n",
    "    # Network operation to update global clock\n",
    "    @network_operation(dt=defaultclock.dt)\n",
    "    def bump_clock():\n",
    "        G.global_clock = float(defaultclock.t / ms)\n",
    "    \n",
    "    # Record spikes\n",
    "    mon = SpikeMonitor(G)\n",
    "    \n",
    "    # Build and run network\n",
    "    net = Network(collect())\n",
    "    net.run(runtime)\n",
    "    \n",
    "    # Extract spike times\n",
    "    for j in range(n_out):\n",
    "        spikes = mon.spike_trains()[j]\n",
    "        out_times[j] = float(spikes[0] / ms) if len(spikes) else float(runtime / ms)\n",
    "    \n",
    "    return out_times\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Simplified training with reduced complexity\n",
    "\n",
    "def train_snn_backprop_optimized(\n",
    "    X, Y,\n",
    "    W1_init, W2_init,\n",
    "    batch_size=8,\n",
    "    epochs=5,  # Reduced default epochs\n",
    "    lr=0.05,   # Reduced learning rate\n",
    "    max_grad=10.0,  # Reduced gradient clipping\n",
    "    w_min=-10.0,    # Reduced weight bounds\n",
    "    w_max=10.0\n",
    "):\n",
    "    W1 = W1_init.copy()\n",
    "    W2 = W2_init.copy()\n",
    "    n_samples = len(X)\n",
    "    layer1_idx, layer2_idx = 1, 2\n",
    "    \n",
    "    print(f\"Training with {n_samples} samples, batch_size={batch_size}\")\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        print(f\"Epoch {ep+1}/{epochs}\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Shuffle indices\n",
    "        idxs = np.random.permutation(n_samples)\n",
    "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx, start in enumerate(range(0, n_samples, batch_size)):\n",
    "            if batch_idx % max(1, n_batches//5) == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{n_batches}\")\n",
    "            \n",
    "            batch_idxs = idxs[start:start+batch_size]\n",
    "            actual_batch_size = len(batch_idxs)\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            acc_dW1 = np.zeros_like(W1)\n",
    "            acc_dW2 = np.zeros_like(W2)\n",
    "            batch_loss = 0\n",
    "            \n",
    "            for i in batch_idxs:\n",
    "                xi, yi = X[i], Y[i]\n",
    "                \n",
    "                # Forward pass\n",
    "                h_times = layer_forward_optimized(xi, W1, layer1_idx, runtime=1.5*ms)\n",
    "                o_times = layer_forward_optimized(h_times, W2, layer2_idx, runtime=1.5*ms)\n",
    "                \n",
    "                # Compute loss\n",
    "                delta_o = (o_times - yi)\n",
    "                batch_loss += np.sum(delta_o**2)\n",
    "                \n",
    "                # Backward pass - simplified gradient computation\n",
    "                # Gradient for W2\n",
    "                for k in range(W2.shape[0]):\n",
    "                    for j in range(W2.shape[1]):\n",
    "                        if abs(delta_o[j]) > 1e-6:  # Skip very small gradients\n",
    "                            dt_dw = d_spike_timing_dw(W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                            acc_dW2[k,j] += delta_o[j] * dt_dw\n",
    "                \n",
    "                # Gradient for W1 (simplified backpropagation)\n",
    "                delta_h = np.zeros_like(h_times)\n",
    "                for k in range(len(h_times)):\n",
    "                    for j in range(W2.shape[1]):\n",
    "                        if abs(delta_o[j]) > 1e-6:\n",
    "                            dt_dw = d_spike_timing_dw(W2[k,j], h_times[k], layer2_idx, 0, 1)\n",
    "                            delta_h[k] += delta_o[j] * W2[k,j] * dt_dw\n",
    "                \n",
    "                for a in range(W1.shape[0]):\n",
    "                    for b in range(W1.shape[1]):\n",
    "                        if abs(delta_h[b]) > 1e-6:  # Skip very small gradients\n",
    "                            dt_dw = d_spike_timing_dw(W1[a,b], xi[a], layer1_idx, 0, 1)\n",
    "                            acc_dW1[a,b] += delta_h[b] * dt_dw\n",
    "            \n",
    "            # Average and clip gradients\n",
    "            acc_dW1 /= actual_batch_size\n",
    "            acc_dW2 /= actual_batch_size\n",
    "            acc_dW1 = np.clip(acc_dW1, -max_grad, max_grad)\n",
    "            acc_dW2 = np.clip(acc_dW2, -max_grad, max_grad)\n",
    "            \n",
    "            # Update weights\n",
    "            W1 = np.clip(W1 - lr * acc_dW1, w_min, w_max)\n",
    "            W2 = np.clip(W2 - lr * acc_dW2, w_min, w_max)\n",
    "            \n",
    "            epoch_loss += batch_loss / actual_batch_size\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        print(f\"  Epoch {ep+1} loss: {avg_loss:.4f}, ||W1||={np.linalg.norm(W1):.3f}, ||W2||={np.linalg.norm(W2):.3f}\")\n",
    "    \n",
    "    return W1, W2\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Main execution\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing Iris dataset...\")\n",
    "    \n",
    "    # Load and preprocess Iris dataset\n",
    "    data = load_iris()\n",
    "    X_raw = data.data  # shape (150,4)\n",
    "    y_raw = data.target  # 0,1,2\n",
    "\n",
    "    # Shuffle and split train/test (80/20)\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    perm = np.random.permutation(len(X_raw))\n",
    "    split = int(0.8 * len(X_raw))\n",
    "    train_idx, test_idx = perm[:split], perm[split:]\n",
    "    X_train_raw, y_train = X_raw[train_idx], y_raw[train_idx]\n",
    "    X_test_raw, y_test = X_raw[test_idx], y_raw[test_idx]\n",
    "\n",
    "    # Scale features to [0.1, 0.9] based on training set\n",
    "    X_min, X_max = X_train_raw.min(axis=0), X_train_raw.max(axis=0)\n",
    "    def scale(x):\n",
    "        return 0.1 + (x - X_min) * (0.8 / (X_max - X_min))\n",
    "    X_train = scale(X_train_raw)\n",
    "    X_test = scale(X_test_raw)\n",
    "\n",
    "    # Build spike-time inputs\n",
    "    X_train_list = [x for x in X_train]\n",
    "    X_test_list = [x for x in X_test]\n",
    "\n",
    "    # Create target spike times: 3 outputs, correct class=0.9, others=0.3\n",
    "    def make_target(label):\n",
    "        t = np.ones(3) * 0.3\n",
    "        t[label] = 0.9\n",
    "        return t\n",
    "    Y_train_list = [make_target(l) for l in y_train]\n",
    "    Y_test_list = [make_target(l) for l in y_test]\n",
    "\n",
    "    # Initialize weights with smaller values\n",
    "    print(\"Initializing weights...\")\n",
    "    # W1_0 = np.random.randn(4, 10) * 0.05\n",
    "    # W2_0 = np.random.randn(10, 3) * 0.05\n",
    "\n",
    "    # Train with optimized function\n",
    "    print(\"Starting training...\")\n",
    "    W1_tr, W2_tr = train_snn_backprop_optimized(\n",
    "        X_train_list, Y_train_list,\n",
    "        W1_0, W2_0,\n",
    "        batch_size=8, epochs=3, lr=0.1  # Reduced epochs for faster execution\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    preds = []\n",
    "    for i, x in enumerate(X_test_list):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Testing sample {i+1}/{len(X_test_list)}\")\n",
    "        h = layer_forward_optimized(x, W1_tr, 1, runtime=1.5*ms)\n",
    "        o = layer_forward_optimized(h, W2_tr, 2, runtime=1.5*ms)\n",
    "        preds.append(np.argmax(o))\n",
    "    \n",
    "    accuracy = np.mean(np.array(preds) == y_test)\n",
    "    print(f\"\\nTest accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save weights\n",
    "    np.save('W1_tr.npy', W1_tr)\n",
    "    np.save('W2_tr.npy', W2_tr)\n",
    "    print(\"Saved weights to 'W1_tr.npy' and 'W2_tr.npy'.\")\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddb08c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[0;32m     48\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[30], line 105\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, batch_size, epochs, lr, max_grad, w_min, w_max)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_idxs:\n\u001b[0;32m    104\u001b[0m     xi, yi \u001b[38;5;241m=\u001b[39m X[i], Y[i]\n\u001b[1;32m--> 105\u001b[0m     h_times \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer1_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     o_times \u001b[38;5;241m=\u001b[39m layer_forward(h_times, W2, layer2_idx)\n\u001b[0;32m    107\u001b[0m     delta_o \u001b[38;5;241m=\u001b[39m (o_times \u001b[38;5;241m-\u001b[39m yi)\n",
      "Cell \u001b[1;32mIn[30], line 70\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m     65\u001b[0m G\u001b[38;5;241m.\u001b[39mrun_regularly(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124m    v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124m    global_clock += 0.001\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mms)\n\u001b[0;32m     69\u001b[0m mon \u001b[38;5;241m=\u001b[39m SpikeMonitor(G)\n\u001b[1;32m---> 70\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m ts \u001b[38;5;241m=\u001b[39m mon\u001b[38;5;241m.\u001b[39mspike_trains()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     72\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mms) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m5.0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:407\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@check_units\u001b[39m(duration\u001b[38;5;241m=\u001b[39msecond, report_period\u001b[38;5;241m=\u001b[39msecond)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    336\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    342\u001b[0m ):\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    run(duration, report=None, report_period=10*second, namespace=None, level=0)\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m        intended use. See `MagicNetwork` for more details.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmagic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\magic.py:248\u001b[0m, in \u001b[0;36mMagicNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     duration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m     level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    246\u001b[0m ):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_magic_objects(level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:346\u001b[0m, in \u001b[0;36mdevice_override.<locals>.device_override_decorator.<locals>.device_override_decorated_function\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(curdev, name)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\units\\fundamentalunits.py:2652\u001b[0m, in \u001b[0;36mcheck_units.<locals>.do_check_units.<locals>.new_f\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m   2642\u001b[0m             error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2643\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected a quantity with unit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2646\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2647\u001b[0m             )\n\u001b[0;32m   2648\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DimensionMismatchError(\n\u001b[0;32m   2649\u001b[0m                 error_message, get_dimensions(newkeyset[k])\n\u001b[0;32m   2650\u001b[0m             )\n\u001b[1;32m-> 2652\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m au:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m], Callable) \u001b[38;5;129;01mand\u001b[39;00m au[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   2655\u001b[0m         \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   2656\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m   2657\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\network.py:1230\u001b[0m, in \u001b[0;36mNetwork.run\u001b[1;34m(self, duration, report, report_period, namespace, profile, level)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m active_objects:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_clock \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[1;32m-> 1230\u001b[0m             \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m curclocks:\n\u001b[0;32m   1233\u001b[0m     timestep, t, dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_variables[c]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\core\\base.py:236\u001b[0m, in \u001b[0;36mBrianObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m codeobj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_objects:\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mcodeobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:131\u001b[0m, in \u001b[0;36mCodeObject.__call__\u001b[1;34m(self, **kwds)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_namespace()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\codeobject.py:159\u001b[0m, in \u001b[0;36mCodeObject.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    Runs the main code in the namespace.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        defined during the call of `CodeGenerator.code_object`.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\brian2\\codegen\\runtime\\numpy_rt\\numpy_rt.py:281\u001b[0m, in \u001b[0;36mNumpyCodeObject.run_block\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     exec(compiled_code, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    283\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode, block)\n",
      "File \u001b[1;32m(string):16\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:1400\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "from sklearn.datasets import load_iris\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # load and preprocess Iris dataset\n",
    "    data = load_iris()\n",
    "    X_raw = data.data  # shape (150,4)\n",
    "    y_raw = data.target  # 0,1,2\n",
    "\n",
    "    # shuffle and split train/test (80/20)\n",
    "    perm = np.random.permutation(len(X_raw))\n",
    "    split = int(0.8 * len(X_raw))\n",
    "    train_idx, test_idx = perm[:split], perm[split:]\n",
    "    X_train_raw, y_train = X_raw[train_idx], y_raw[train_idx]\n",
    "    X_test_raw, y_test = X_raw[test_idx], y_raw[test_idx]\n",
    "\n",
    "    # scale features to [0.05, 0.95] based on training set\n",
    "    X_min, X_max = X_train_raw.min(axis=0), X_train_raw.max(axis=0)\n",
    "    def scale(x):\n",
    "        return 0.05 + (x - X_min) * (0.90 / (X_max - X_min))\n",
    "    X_train = scale(X_train_raw)\n",
    "    X_test = scale(X_test_raw)\n",
    "\n",
    "    # build spike-time inputs\n",
    "    X_train_list = [x for x in X_train]\n",
    "    X_test_list = [x for x in X_test]\n",
    "\n",
    "    # create target spike times: 3 outputs, correct class=2.95, others=2.05\n",
    "    def make_target(label):\n",
    "        t = np.ones(3) * 2.05\n",
    "        t[label] = 2.95\n",
    "        return t\n",
    "    Y_train_list = [make_target(l) for l in y_train]\n",
    "    Y_test_list = [make_target(l) for l in y_test]\n",
    "\n",
    "    # init weights\n",
    "    W1_0 = np.random.randn(4,10) * 0.1\n",
    "    W2_0 = np.random.randn(10,3) * 0.1\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(\n",
    "        X_train_list, Y_train_list,\n",
    "        W1_0, W2_0,\n",
    "        batch_size=4, epochs=5, lr=0.1\n",
    "    )\n",
    "\n",
    "    # evaluate on test set\n",
    "    preds = []\n",
    "    for x in X_test_list:\n",
    "        h = layer_forward(x, W1_tr, 1)\n",
    "        o = layer_forward(h, W2_tr, 2)\n",
    "        preds.append(np.argmax(o))\n",
    "    accuracy = np.mean(np.array(preds) == y_test)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # save weights\n",
    "    np.save('W1_tr.npy', W1_tr)\n",
    "    np.save('W2_tr.npy', W2_tr)\n",
    "    print(\"Saved weights to 'W1_tr.npy' and 'W2_tr.npy'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c3f2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_snn_backprop_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# train using batch simulation\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop_batch\u001b[49m(\n\u001b[0;32m     43\u001b[0m     X_train_list, Y_train_list,\n\u001b[0;32m     44\u001b[0m     W1_0, W2_0,\n\u001b[0;32m     45\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[0;32m     49\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_snn_backprop_batch' is not defined"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "np.seterr(over='ignore', under='ignore')\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # load Iris dataset and split\n",
    "    data = load_iris()\n",
    "    X_raw, y_raw = data.data, data.target\n",
    "    perm = np.random.permutation(len(X_raw))\n",
    "    split = int(0.8 * len(X_raw))\n",
    "    tr, te = perm[:split], perm[split:]\n",
    "    X_tr_raw, y_tr = X_raw[tr], y_raw[tr]\n",
    "    X_te_raw, y_te = X_raw[te], y_raw[te]\n",
    "\n",
    "    # scale features to [0.05,0.95]\n",
    "    Xmin, Xmax = X_tr_raw.min(axis=0), X_tr_raw.max(axis=0)\n",
    "    X_tr = 0.05 + (X_tr_raw - Xmin) * 0.90 / (Xmax - Xmin)\n",
    "    X_te = 0.05 + (X_te_raw - Xmin) * 0.90 / (Xmax - Xmin)\n",
    "    X_train_list, X_test_list = list(X_tr), list(X_te)\n",
    "\n",
    "    # build targets\n",
    "    def mk_t(label):\n",
    "        t = np.full(3, 2.05)\n",
    "        t[label] = 2.95\n",
    "        return t\n",
    "    Y_train_list = [mk_t(l) for l in y_tr]\n",
    "    Y_test_list  = [mk_t(l) for l in y_te]\n",
    "\n",
    "    # initialize weights\n",
    "    W1_0 = np.random.randn(4,10) * 0.1\n",
    "    W2_0 = np.random.randn(10,3) * 0.1\n",
    "\n",
    "    # train using batch simulation\n",
    "    W1_tr, W2_tr = train_snn_backprop_batch(\n",
    "        X_train_list, Y_train_list,\n",
    "        W1_0, W2_0,\n",
    "        batch_size=16, epochs=5, lr=0.1\n",
    "    )\n",
    "\n",
    "    # evaluate on test set\n",
    "    preds = []\n",
    "    for x in X_test_list:\n",
    "        h = layer_forward(x, W1_tr, 1)\n",
    "        o = layer_forward(h, W2_tr, 2)\n",
    "        preds.append(np.argmax(o))\n",
    "    accuracy = np.mean(np.array(preds) == y_te)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # save weights\n",
    "    np.save('W1_tr.npy', W1_tr)\n",
    "    np.save('W2_tr.npy', W2_tr)\n",
    "    print(\"Weights saved to 'W1_tr.npy' and 'W2_tr.npy'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44729396",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m W2_0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m W1_tr, W2_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_snn_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mW1_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[0;32m     48\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[15], line 76\u001b[0m, in \u001b[0;36mtrain_snn_backprop\u001b[1;34m(X, Y, W1_init, W2_init, batch_size, epochs, lr, max_grad, w_min, w_max)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idxs[b:b\u001b[38;5;241m+\u001b[39mbatch_size]:\n\u001b[0;32m     75\u001b[0m     xi, yi \u001b[38;5;241m=\u001b[39m X[i], Y[i]\n\u001b[1;32m---> 76\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     o \u001b[38;5;241m=\u001b[39m layer_forward(h, W2, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     78\u001b[0m     delta_o \u001b[38;5;241m=\u001b[39m o \u001b[38;5;241m-\u001b[39m yi\n",
      "Cell \u001b[1;32mIn[15], line 47\u001b[0m, in \u001b[0;36mlayer_forward\u001b[1;34m(inputs, W, layer_idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m contrib \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n_in, n_out))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_in):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# vectorized over each output neuron\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     contrib[i, :] \u001b[38;5;241m=\u001b[39m \u001b[43mspike_timing_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# assume sr_j = n_in for all neurons\u001b[39;00m\n\u001b[0;32m     49\u001b[0m sum_contrib \u001b[38;5;241m=\u001b[39m contrib\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m, in \u001b[0;36mspike_timing_np\u001b[1;34m(w, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspike_timing_np\u001b[39m(w, x):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# x in [0,1)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mpower(x, (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m w), where\u001b[38;5;241m=\u001b[39m(x\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m), out\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros_like(x))\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "from sklearn.datasets import load_iris\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # load and preprocess Iris dataset\n",
    "    data = load_iris()\n",
    "    X_raw = data.data  # shape (150,4)\n",
    "    y_raw = data.target  # 0,1,2\n",
    "\n",
    "    # shuffle and split train/test (80/20)\n",
    "    perm = np.random.permutation(len(X_raw))\n",
    "    split = int(0.8 * len(X_raw))\n",
    "    train_idx, test_idx = perm[:split], perm[split:]\n",
    "    X_train_raw, y_train = X_raw[train_idx], y_raw[train_idx]\n",
    "    X_test_raw, y_test = X_raw[test_idx], y_raw[test_idx]\n",
    "\n",
    "    # scale features to [0.05, 0.95] based on training set\n",
    "    X_min, X_max = X_train_raw.min(axis=0), X_train_raw.max(axis=0)\n",
    "    def scale(x):\n",
    "        return 0.05 + (x - X_min) * (0.90 / (X_max - X_min))\n",
    "    X_train = scale(X_train_raw)\n",
    "    X_test = scale(X_test_raw)\n",
    "\n",
    "    # build spike-time inputs\n",
    "    X_train_list = [x for x in X_train]\n",
    "    X_test_list = [x for x in X_test]\n",
    "\n",
    "    # create target spike times: 3 outputs, correct class=2.95, others=2.05\n",
    "    def make_target(label):\n",
    "        t = np.ones(3) * 2.05\n",
    "        t[label] = 2.95\n",
    "        return t\n",
    "    Y_train_list = [make_target(l) for l in y_train]\n",
    "    Y_test_list = [make_target(l) for l in y_test]\n",
    "\n",
    "    # init weights\n",
    "    W1_0 = np.random.randn(4,10) * 0.1\n",
    "    W2_0 = np.random.randn(10,3) * 0.1\n",
    "\n",
    "    # train\n",
    "    W1_tr, W2_tr = train_snn_backprop(\n",
    "        X_train_list, Y_train_list,\n",
    "        W1_0, W2_0,\n",
    "        batch_size=16, epochs=5, lr=0.1\n",
    "    )\n",
    "\n",
    "    # evaluate on test set\n",
    "    preds = []\n",
    "    for x in X_test_list:\n",
    "        h = layer_forward(x, W1_tr, 1)\n",
    "        o = layer_forward(h, W2_tr, 2)\n",
    "        preds.append(np.argmax(o))\n",
    "    accuracy = np.mean(np.array(preds) == y_test)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # save weights\n",
    "    np.save('W1_tr.npy', W1_tr)\n",
    "    np.save('W2_tr.npy', W2_tr)\n",
    "    print(\"Saved weights to 'W1_tr.npy' and 'W2_tr.npy'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
