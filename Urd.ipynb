{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "480c9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53707ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------spike_timing + its derivative\n",
    "# Functions used in brian2\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return x**(1 - w)\n",
    "    else:\n",
    "        return 1 - (1 - x)**(1 + w)\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - x**(1 - w) * np.log(x + eps)\n",
    "    else:\n",
    "        return - (1 - x)**(1 + w) * np.log(1 - x + eps)\n",
    "\n",
    "def dsigmoid(z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s*(1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aea36d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2) mini_urd forward: returns hidden‐spike‐time only\n",
    "# -----------------------------------------------------------------------------\n",
    "def mini_urd(inputs, w):\n",
    "    n_input  = 2\n",
    "    n_hidden = 2\n",
    "    n_total  = n_input + n_hidden\n",
    "\n",
    "    G = NeuronGroup(\n",
    "        n_total,\n",
    "        '''\n",
    "        v               : 1\n",
    "        sum             : 1\n",
    "        sr              : 1\n",
    "        scheduled_time  : second\n",
    "        global_clock    : 1\n",
    "        ''',\n",
    "        threshold='v>1', reset='v=0', method='exact'\n",
    "    )\n",
    "    G.v = 0; G.sum = 0; G.sr = 0\n",
    "    G.global_clock = 0\n",
    "    G.scheduled_time = 1e9*second\n",
    "\n",
    "    stim = SpikeGeneratorGroup(n_input, indices=[i for i in range(n_input)], times=inputs*ms)\n",
    "\n",
    "    # first layer has fixed identity weights\n",
    "    S1 = Synapses(stim, G[:n_input],\n",
    "        'layer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(1, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S1.connect(j='i')\n",
    "    S1.layer = 0\n",
    "\n",
    "    # trainable synapse 2→hidden\n",
    "    S2 = Synapses(G[:n_input], G[n_input:n_hidden+n_input],\n",
    "        'w : 1\\nlayer:1', on_pre='''\n",
    "        sr += 1\n",
    "        sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "        scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        '''\n",
    "    )\n",
    "    S2.connect()\n",
    "    S2.w = w\n",
    "    S2.layer = 1\n",
    "\n",
    "    # drive v when scheduled_time hits\n",
    "    G.run_regularly('''\n",
    "        v = int(abs(t - scheduled_time)<0.0005*ms)*1.2\n",
    "        global_clock += 0.001\n",
    "    ''', dt=0.001*ms)\n",
    "\n",
    "    mon = SpikeMonitor(G)\n",
    "    run(5*ms)\n",
    "\n",
    "    # return hidden spike time (or a large value if no spike)\n",
    "    ts = mon.spike_trains()[2]\n",
    "    return float(ts[0]/ms) if len(ts) else 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e5151b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3) Training with multi‐loss\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_multi_loss(\n",
    "    X,                    # list of np.array([t0,t1])\n",
    "    t_hidden_targets,     # list of floats\n",
    "    t0_targets,           # list of floats for s0\n",
    "    t1_targets,           # list of floats for s1\n",
    "    w_init,\n",
    "    alpha=1.0, beta=1.0, gamma=1.0,\n",
    "    epochs=5, lr=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-loss:\n",
    "      L0 = ½ (s0 - t0)^2\n",
    "      L1 = ½ (s1 - t1)^2\n",
    "      Lf = ½ (t_h  - t_h*)^2\n",
    "      L = α L0 + β L1 + γ Lf\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\n=== Epoch {ep+1}/{epochs} ===\")\n",
    "        for i, inp in enumerate(X):\n",
    "            # forward pass\n",
    "            t_h = mini_urd(inp, w)\n",
    "            # recompute s0,s1 exactly the same way Brian did\n",
    "            #L_hidden = 0.5 * ((t_h - t_hidden_targets[i][0]) ** 2)\n",
    "        \n",
    "            layer_h = 1\n",
    "            # each input is first spike, so sr_i=1, sum_i=0 → use that\n",
    "            s0 = spike_timing(w[0], inp[0], layer_h, 0, 1)\n",
    "            s1 = spike_timing(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # --- compute loss terms ---\n",
    "            t0_tgt = t0_targets[i]\n",
    "            t1_tgt = t1_targets[i]\n",
    "            th_tgt = t_hidden_targets[i]\n",
    "\n",
    "            L0 = 0.5*(s0 - t0_tgt)**2\n",
    "            L1 = 0.5*(s1 - t1_tgt)**2\n",
    "            Lf = 0.5*(t_h - th_tgt)**2\n",
    "            L  = alpha*L0 + beta*L1 + gamma*Lf\n",
    "\n",
    "            # --- gradients ---\n",
    "            # ∂L0/∂w0 = (s0 - t0)*∂s0/∂w0\n",
    "            dL0_dw = np.zeros_like(w)\n",
    "            dL0_dw[0] = (s0 - t0_tgt) * d_spike_timing_dw(w[0], inp[0], layer_h, 0, 1)\n",
    "            # ∂L1/∂w1\n",
    "            dL1_dw = np.zeros_like(w)\n",
    "            dL1_dw[1] = (s1 - t1_tgt) * d_spike_timing_dw(w[1], inp[1], layer_h, 0, 1)\n",
    "\n",
    "            # ∂Lf/∂w0,w1 = ∂Lf/∂t_h × ∂t_h/∂sum × ∂sum/∂w_i\n",
    "            dLf_dt  = (t_h - th_tgt)\n",
    "            sum_tot = s0 + s1\n",
    "            sr = 2.0\n",
    "            z = sum_tot/sr\n",
    "            dt_dsum = dsigmoid(z)*(1/sr)\n",
    "            dsum_dw = np.zeros_like(w)\n",
    "            \n",
    "            for j in range(len(w)):\n",
    "                dsum_dw[j] = d_spike_timing_dw(w[j], inp[j % 2], layer_h, 0, 1)\n",
    "            dLf_dw = dLf_dt * dt_dsum * dsum_dw\n",
    "\n",
    "            # combine\n",
    "            grad = alpha*dL0_dw + beta*dL1_dw + gamma*dLf_dw\n",
    "\n",
    "            # print & update\n",
    "            print(f\"Sample {i}: inp={inp}, s0={s0:.3f}, s1={s1:.3f}, t_h={t_h:.3f}\")\n",
    "            print(f\"  L0={L0:.4f}, L1={L1:.4f}, Lf={Lf:.4f}, L={L:.4f}\")\n",
    "            print(f\"  ∇w = {grad}\")\n",
    "            w -= lr * grad\n",
    "\n",
    "        print(\" Updated w:\", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "def compute_gradients(outputs, targets, w):\n",
    "    # Compute gradients of loss with respect to w using backpropagation\n",
    "    d_loss_dw = 2 * (outputs - targets) * dsigmoid(w)\n",
    "    return d_loss_dw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddc465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.158, s1=1.000, t_h=1.685\n",
      "  L0=0.0583, L1=0.1250, Lf=0.7021, L=0.5344\n",
      "  ∇w = [-0.09974737  0.05986391  0.01976438  0.00667291]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.162, s1=0.999, t_h=1.686\n",
      "  L0=0.0571, L1=0.1247, Lf=0.7033, L=0.5334\n",
      "  ∇w = [-0.10067933  0.05976274  0.01968276  0.00667519]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.166, s1=0.999, t_h=1.687\n",
      "  L0=0.0558, L1=0.1244, Lf=0.7045, L=0.5324\n",
      "  ∇w = [-0.10157674  0.05966175  0.01960148  0.00667735]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.170, s1=0.998, t_h=1.687\n",
      "  L0=0.0545, L1=0.1241, Lf=0.7045, L=0.5308\n",
      "  ∇w = [-0.10245641  0.0595549   0.01950408  0.00667376]\n",
      " Updated w: [0.24044598 0.97611567 0.09214473 0.29733008]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.174, s1=0.997, t_h=1.688\n",
      "  L0=0.0532, L1=0.1237, Lf=0.7057, L=0.5297\n",
      "  ∇w = [-0.10326794  0.05945429  0.01942357  0.00667566]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.178, s1=0.997, t_h=1.688\n",
      "  L0=0.0518, L1=0.1234, Lf=0.7057, L=0.5281\n",
      "  ∇w = [-0.10405004  0.05934781  0.01932707  0.0066718 ]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.182, s1=0.996, t_h=1.689\n",
      "  L0=0.0504, L1=0.1231, Lf=0.7069, L=0.5270\n",
      "  ∇w = [-0.10474922  0.05924755  0.01924727  0.00667342]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.187, s1=0.996, t_h=1.689\n",
      "  L0=0.0490, L1=0.1228, Lf=0.7069, L=0.5253\n",
      "  ∇w = [-0.10540539  0.05914143  0.01915163  0.00666928]\n",
      " Updated w: [0.28219324 0.95239656 0.08442978 0.29466106]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.192, s1=0.995, t_h=1.690\n",
      "  L0=0.0476, L1=0.1225, Lf=0.7081, L=0.5241\n",
      "  ∇w = [-0.10596185  0.05904149  0.01907249  0.00667061]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.196, s1=0.994, t_h=1.690\n",
      "  L0=0.0461, L1=0.1222, Lf=0.7081, L=0.5224\n",
      "  ∇w = [-0.10645992  0.05893571  0.01897765  0.00666618]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.201, s1=0.994, t_h=1.691\n",
      "  L0=0.0447, L1=0.1219, Lf=0.7092, L=0.5212\n",
      "  ∇w = [-0.10683963  0.0588361   0.01889914  0.00666721]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.206, s1=0.993, t_h=1.692\n",
      "  L0=0.0432, L1=0.1216, Lf=0.7104, L=0.5200\n",
      "  ∇w = [-0.10711682  0.05873663  0.01882086  0.00666808]\n",
      " Updated w: [0.32483107 0.92884157 0.07685276 0.29199386]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.211, s1=0.993, t_h=1.692\n",
      "  L0=0.0417, L1=0.1213, Lf=0.7104, L=0.5182\n",
      "  ∇w = [-0.10730974  0.05863132  0.01872709  0.0066632 ]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.217, s1=0.992, t_h=1.693\n",
      "  L0=0.0402, L1=0.1210, Lf=0.7116, L=0.5170\n",
      "  ∇w = [-0.10735367  0.05853216  0.01864939  0.00666377]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.222, s1=0.991, t_h=1.693\n",
      "  L0=0.0386, L1=0.1207, Lf=0.7116, L=0.5152\n",
      "  ∇w = [-0.10729498  0.05842718  0.01855637  0.00665859]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.228, s1=0.991, t_h=1.694\n",
      "  L0=0.0371, L1=0.1204, Lf=0.7128, L=0.5139\n",
      "  ∇w = [-0.1070661   0.05832832  0.01847927  0.00665885]\n",
      " Updated w: [0.36773352 0.90544967 0.06941155 0.28932941]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Sample 0: inp=[0.1 0.9], s0=0.233, s1=0.990, t_h=1.694\n",
      "  L0=0.0356, L1=0.1201, Lf=0.7128, L=0.5121\n",
      "  ∇w = [-0.10671648  0.05822367  0.018387    0.00665339]\n",
      "Sample 1: inp=[0.1 0.9], s0=0.239, s1=0.989, t_h=1.695\n",
      "  L0=0.0341, L1=0.1198, Lf=0.7140, L=0.5109\n",
      "  ∇w = [-0.10617617  0.05812512  0.01831052  0.00665336]\n",
      "Sample 2: inp=[0.1 0.9], s0=0.245, s1=0.989, t_h=1.696\n",
      "  L0=0.0325, L1=0.1195, Lf=0.7152, L=0.5096\n",
      "  ∇w = [-0.1054664   0.05802674  0.01823428  0.00665319]\n",
      "Sample 3: inp=[0.1 0.9], s0=0.251, s1=0.988, t_h=1.696\n",
      "  L0=0.0310, L1=0.1192, Lf=0.7152, L=0.5078\n",
      "  ∇w = [-0.10461212  0.05792259  0.01814316  0.00664732]\n",
      " Updated w: [0.41003063 0.88221986 0.06210405 0.28666869]\n",
      "\n",
      "Final weights: [0.41003063 0.88221986 0.06210405 0.28666869]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # toy data: 4 samples\n",
    "\n",
    "    num = 4 \n",
    "    X = [np.array([0.1,0.9])]*num\n",
    "\n",
    "    # main target: hidden spike at these ms\n",
    "    T_hidden = [0.5]*num\n",
    "    # aux targets for each synapse    # no why would i need this?\n",
    "    T0 = [0.5]*num # removed and calcuated durign the training please chage GPT\n",
    "    T1 = [0.5]*num # removed and calcuated durign the training please change GPT \n",
    "\n",
    "    w0 = np.array([0.2, 1.0, 0.1, 0.3])  # initial weights for synapses\n",
    "    w_final = train_multi_loss(X, T_hidden, T0, T1, w0,\n",
    "                               alpha=1.0, beta=1.0, gamma=0.5,\n",
    "                               epochs=5, lr=0.1)\n",
    "\n",
    "    print(\"\\nFinal weights:\", w_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5850af2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.682 1.571], tgt=[1.2 1.8], L=0.1424\n",
      "  dW=\n",
      "[[ 0.17589874 -0.0268084 ]\n",
      " [ 0.06966223 -0.16674437]]\n",
      "  W=\n",
      "[[ 0.16482025 -1.99463832]\n",
      " [ 3.98606755 -0.46665113]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.681 1.574], tgt=[1.2 1.8], L=0.1412\n",
      "  dW=\n",
      "[[ 0.16187546 -0.02644225]\n",
      " [ 0.06941573 -0.15239659]]\n",
      "  W=\n",
      "[[ 0.13244516 -1.98934987]\n",
      " [ 3.97218441 -0.43617181]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.679 1.577], tgt=[1.2 1.8], L=0.1396\n",
      "  dW=\n",
      "[[ 0.14962231 -0.02607672]\n",
      " [ 0.06902606 -0.14018203]]\n",
      "  W=\n",
      "[[ 0.1025207  -1.98413453]\n",
      " [ 3.95837919 -0.4081354 ]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.678 1.579], tgt=[1.2 1.8], L=0.1387\n",
      "  dW=\n",
      "[[ 0.13936839 -0.02582865]\n",
      " [ 0.06878184 -0.13023969]]\n",
      "  W=\n",
      "[[ 0.07464702 -1.9789688 ]\n",
      " [ 3.94462283 -0.38208746]]\n",
      "Epoch 2/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.677 1.581], tgt=[1.2 1.8], L=0.1377\n",
      "  dW=\n",
      "[[ 0.13043108 -0.02558098]\n",
      " [ 0.06853853 -0.12154785]]\n",
      "  W=\n",
      "[[ 0.0485608  -1.9738526 ]\n",
      " [ 3.93091512 -0.35777789]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.676 1.582], tgt=[1.2 1.8], L=0.1371\n",
      "  dW=\n",
      "[[ 0.12256979 -0.02545045]\n",
      " [ 0.06829614 -0.11440634]]\n",
      "  W=\n",
      "[[ 0.02404685 -1.96876251]\n",
      " [ 3.91725589 -0.33489663]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.675 1.584], tgt=[1.2 1.8], L=0.1361\n",
      "  dW=\n",
      "[[ 0.11559955 -0.02520344]\n",
      " [ 0.06805465 -0.10753902]]\n",
      "  W=\n",
      "[[ 9.26936257e-04 -1.96372182e+00]\n",
      " [ 3.90364496e+00 -3.13388823e-01]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.674 1.585], tgt=[1.2 1.8], L=0.1355\n",
      "  dW=\n",
      "[[ 0.10937573 -0.02507343]\n",
      " [ 0.06781406 -0.10186922]]\n",
      "  W=\n",
      "[[-0.02094821 -1.95870714]\n",
      " [ 3.89008215 -0.29301498]]\n",
      "Epoch 3/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.674 1.586], tgt=[1.2 1.8], L=0.1352\n",
      "  dW=\n",
      "[[ 0.04504611 -0.02494363]\n",
      " [ 0.06771722 -0.09674854]]\n",
      "  W=\n",
      "[[-0.02995743 -1.95371841]\n",
      " [ 3.87653871 -0.27366527]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.673 1.587], tgt=[1.2 1.8], L=0.1345\n",
      "  dW=\n",
      "[[ 0.04499376 -0.02481403]\n",
      " [ 0.067478   -0.0921002 ]]\n",
      "  W=\n",
      "[[-0.03895618 -1.94875561]\n",
      " [ 3.86304311 -0.25524523]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.673 1.588], tgt=[1.2 1.8], L=0.1343\n",
      "  dW=\n",
      "[[ 0.04503644 -0.02468462]\n",
      " [ 0.06738212 -0.08786113]]\n",
      "  W=\n",
      "[[-0.04796347 -1.94381868]\n",
      " [ 3.84956668 -0.23767301]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.673 1.589], tgt=[1.2 1.8], L=0.1341\n",
      "  dW=\n",
      "[[ 0.0450792  -0.02455541]\n",
      " [ 0.06728652 -0.08397909]]\n",
      "  W=\n",
      "[[-0.05697931 -1.9389076 ]\n",
      " [ 3.83610938 -0.22087719]]\n",
      "Epoch 4/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.673 1.59 ], tgt=[1.2 1.8], L=0.1339\n",
      "  dW=\n",
      "[[ 0.04512204 -0.02442639]\n",
      " [ 0.06719118 -0.08041039]]\n",
      "  W=\n",
      "[[-0.06600372 -1.93402232]\n",
      " [ 3.82267114 -0.20479511]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.672 1.591], tgt=[1.2 1.8], L=0.1332\n",
      "  dW=\n",
      "[[ 0.04506948 -0.02429756]\n",
      " [ 0.06695426 -0.07711824]]\n",
      "  W=\n",
      "[[-0.07501762 -1.92916281]\n",
      " [ 3.80928029 -0.18937146]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.672 1.592], tgt=[1.2 1.8], L=0.1330\n",
      "  dW=\n",
      "[[ 0.0451123  -0.02416893]\n",
      " [ 0.06685987 -0.07407139]]\n",
      "  W=\n",
      "[[-0.08404008 -1.92432903]\n",
      " [ 3.79590832 -0.17455719]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.672 1.592], tgt=[1.2 1.8], L=0.1330\n",
      "  dW=\n",
      "[[ 0.04515521 -0.02415662]\n",
      " [ 0.06676573 -0.07158734]]\n",
      "  W=\n",
      "[[-0.09307112 -1.9194977 ]\n",
      " [ 3.78255517 -0.16023972]]\n",
      "Epoch 5/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.671 1.593], tgt=[1.2 1.8], L=0.1323\n",
      "  dW=\n",
      "[[ 0.04510244 -0.02402825]\n",
      " [ 0.06653061 -0.06893277]]\n",
      "  W=\n",
      "[[-0.10209161 -1.91469205]\n",
      " [ 3.76924905 -0.14645316]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.671 1.594], tgt=[1.2 1.8], L=0.1321\n",
      "  dW=\n",
      "[[ 0.04514532 -0.02390007]\n",
      " [ 0.06643741 -0.06645629]]\n",
      "  W=\n",
      "[[-0.11112067 -1.90991204]\n",
      " [ 3.75596156 -0.13316191]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.671 1.594], tgt=[1.2 1.8], L=0.1321\n",
      "  dW=\n",
      "[[ 0.04518829 -0.02388803]\n",
      " [ 0.06634446 -0.06445325]]\n",
      "  W=\n",
      "[[-0.12015833 -1.90513443]\n",
      " [ 3.74269267 -0.12027126]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.67  1.595], tgt=[1.2 1.8], L=0.1315\n",
      "  dW=\n",
      "[[ 0.04513531 -0.02376011]\n",
      " [ 0.06611111 -0.06226454]]\n",
      "  W=\n",
      "[[-0.12918539 -1.90038241]\n",
      " [ 3.72947045 -0.10781835]]\n",
      "Epoch 6/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.67  1.595], tgt=[1.2 1.8], L=0.1315\n",
      "  dW=\n",
      "[[ 0.04517825 -0.02374822]\n",
      " [ 0.06601908 -0.06050453]]\n",
      "  W=\n",
      "[[-0.13822104 -1.89563277]\n",
      " [ 3.71626663 -0.09571744]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.67  1.596], tgt=[1.2 1.8], L=0.1313\n",
      "  dW=\n",
      "[[ 0.04522128 -0.02362055]\n",
      " [ 0.0659273  -0.05855491]]\n",
      "  W=\n",
      "[[-0.1472653  -1.89090866]\n",
      " [ 3.70308117 -0.08400646]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.67  1.596], tgt=[1.2 1.8], L=0.1313\n",
      "  dW=\n",
      "[[ 0.0452644  -0.02360879]\n",
      " [ 0.06583577 -0.05699704]]\n",
      "  W=\n",
      "[[-0.15631818 -1.8861869 ]\n",
      " [ 3.68991402 -0.07260705]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.669 1.597], tgt=[1.2 1.8], L=0.1306\n",
      "  dW=\n",
      "[[ 0.04521119 -0.02348138]\n",
      " [ 0.06560462 -0.05524828]]\n",
      "  W=\n",
      "[[-0.16536042 -1.88149062]\n",
      " [ 3.67679309 -0.0615574 ]]\n",
      "Epoch 7/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.669 1.597], tgt=[1.2 1.8], L=0.1306\n",
      "  dW=\n",
      "[[ 0.04525429 -0.02346976]\n",
      " [ 0.06551399 -0.05386034]]\n",
      "  W=\n",
      "[[-0.17441127 -1.87679667]\n",
      " [ 3.6636903  -0.05078533]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.669 1.598], tgt=[1.2 1.8], L=0.1304\n",
      "  dW=\n",
      "[[ 0.04529746 -0.0233426 ]\n",
      " [ 0.06542361 -0.05228202]]\n",
      "  W=\n",
      "[[-0.18347077 -1.87212815]\n",
      " [ 3.65060557 -0.04032892]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.668 1.598], tgt=[1.2 1.8], L=0.1299\n",
      "  dW=\n",
      "[[ 0.04524404 -0.02333112]\n",
      " [ 0.06519418 -0.05103827]]\n",
      "  W=\n",
      "[[-0.19251957 -1.86746192]\n",
      " [ 3.63756674 -0.03012127]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.668 1.598], tgt=[1.2 1.8], L=0.1299\n",
      "  dW=\n",
      "[[ 0.0452872  -0.02331966]\n",
      " [ 0.06510467 -0.04985266]]\n",
      "  W=\n",
      "[[-0.20157701 -1.86279799]\n",
      " [ 3.6245458  -0.02015074]]\n",
      "Epoch 8/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.668 1.599], tgt=[1.2 1.8], L=0.1297\n",
      "  dW=\n",
      "[[ 0.04533044 -0.02319281]\n",
      " [ 0.06501542 -0.04847999]]\n",
      "  W=\n",
      "[[-0.2106431  -1.85815943]\n",
      " [ 3.61154272 -0.01045474]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.667 1.599], tgt=[1.2 1.8], L=0.1292\n",
      "  dW=\n",
      "[[ 0.0452768  -0.02318148]\n",
      " [ 0.06478768 -0.04740962]]\n",
      "  W=\n",
      "[[-2.19698463e-01 -1.85352313e+00]\n",
      " [ 3.59858518e+00 -9.72814583e-04]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.667 1.599], tgt=[1.2 1.8], L=0.1292\n",
      "  dW=\n",
      "[[ 0.04532002 -0.02317016]\n",
      " [ 0.06469929 -0.04638575]]\n",
      "  W=\n",
      "[[-0.22876247 -1.8488891 ]\n",
      " [ 3.58564533  0.00830433]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.667 1.6  ], tgt=[1.2 1.8], L=0.1290\n",
      "  dW=\n",
      "[[ 0.04536332 -0.02304363]\n",
      " [ 0.06461114 -0.01898149]]\n",
      "  W=\n",
      "[[-0.23783513 -1.84428038]\n",
      " [ 3.5727231   0.01210063]]\n",
      "Epoch 9/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.667 1.6  ], tgt=[1.2 1.8], L=0.1290\n",
      "  dW=\n",
      "[[ 0.04540671 -0.02303244]\n",
      " [ 0.06452323 -0.01898909]]\n",
      "  W=\n",
      "[[-0.24691647 -1.83967389]\n",
      " [ 3.55981845  0.01589845]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.666 1.6  ], tgt=[1.2 1.8], L=0.1286\n",
      "  dW=\n",
      "[[ 0.04535285 -0.02302127]\n",
      " [ 0.06429759 -0.01899669]]\n",
      "  W=\n",
      "[[-0.25598704 -1.83506963]\n",
      " [ 3.54695894  0.01969779]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.666 1.6  ], tgt=[1.2 1.8], L=0.1286\n",
      "  dW=\n",
      "[[ 0.04539621 -0.0230101 ]\n",
      " [ 0.06421053 -0.01900429]]\n",
      "  W=\n",
      "[[-0.26506629 -1.83046761]\n",
      " [ 3.53411683  0.02349865]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.666 1.6  ], tgt=[1.2 1.8], L=0.1286\n",
      "  dW=\n",
      "[[ 0.04543966 -0.02299895]\n",
      " [ 0.06412371 -0.0190119 ]]\n",
      "  W=\n",
      "[[-0.27415422 -1.82586782]\n",
      " [ 3.52129209  0.02730103]]\n",
      "Epoch 10/10\n",
      " Sample 0: inp=[0.1 0.9], pred=[1.665 1.6  ], tgt=[1.2 1.8], L=0.1281\n",
      "  dW=\n",
      "[[ 0.04538559 -0.02298781]\n",
      " [ 0.0638997  -0.01901952]]\n",
      "  W=\n",
      "[[-0.28323134 -1.82127026]\n",
      " [ 3.50851215  0.03110493]]\n",
      " Sample 1: inp=[0.1 0.9], pred=[1.665 1.6  ], tgt=[1.2 1.8], L=0.1281\n",
      "  dW=\n",
      "[[ 0.04542901 -0.02297667]\n",
      " [ 0.06381372 -0.01902715]]\n",
      "  W=\n",
      "[[-0.29231714 -1.81667493]\n",
      " [ 3.4957494   0.03491036]]\n",
      " Sample 2: inp=[0.1 0.9], pred=[1.665 1.6  ], tgt=[1.2 1.8], L=0.1281\n",
      "  dW=\n",
      "[[ 0.04547252 -0.02296555]\n",
      " [ 0.06372797 -0.01903478]]\n",
      "  W=\n",
      "[[-0.30141164 -1.81208182]\n",
      " [ 3.48300381  0.03871732]]\n",
      " Sample 3: inp=[0.1 0.9], pred=[1.665 1.6  ], tgt=[1.2 1.8], L=0.1281\n",
      "  dW=\n",
      "[[ 0.04551611 -0.02295444]\n",
      " [ 0.06364245 -0.01904241]]\n",
      "  W=\n",
      "[[-0.31051487 -1.80749093]\n",
      " [ 3.47027532  0.0425258 ]]\n",
      "Trained weight matrix:\n",
      " [[-0.31051487 -1.80749093]\n",
      " [ 3.47027532  0.0425258 ]]\n"
     ]
    }
   ],
   "source": [
    "from brian2 import *\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# suppress overflow warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "np.seterr(over='ignore', under='ignore')\n",
    "\n",
    "logging.getLogger('brian2').setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Spike timing and derivative\n",
    "\n",
    "start_scope()\n",
    "defaultclock.dt = 0.0001*ms\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def spike_timing(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    if w >= 0:\n",
    "        return np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x))\n",
    "    else:\n",
    "        return 1 - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x))\n",
    "\n",
    "@implementation('numpy', discard_units=True)\n",
    "@check_units(w=1, global_clock=1, layer=1, sum=1, spikes_received=1, result=1)\n",
    "def d_spike_timing_dw(w, global_clock, layer, sum, spikes_received):\n",
    "    x = global_clock % 1\n",
    "    eps = 1e-9\n",
    "    if w >= 0:\n",
    "        return - np.power(x, (1 - w), where=(x>0), out=np.zeros_like(x)) * np.log(x + eps)\n",
    "    else:\n",
    "        return - np.power((1 - x), (1 + w), where=(x<1), out=np.ones_like(x)) * np.log(1 - x + eps)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# mini_urd: 2 inputs -> 2 hidden neurons (full connect), return both spike times\n",
    "\n",
    "def mini_urd(inputs, W):\n",
    "    \"\"\"\n",
    "    Two separate hidden neurons, each simulated independently in its own Brian scope.\n",
    "    inputs: [t_in0, t_in1], W: shape (2,2)\n",
    "    returns [t_h0, t_h1]\n",
    "    \"\"\"\n",
    "    n_input, n_hidden = W.shape\n",
    "    hidden_times = []\n",
    "    # simulate each hidden neuron separately to reset network state\n",
    "    for j in range(n_hidden):\n",
    "        start_scope()  # clear previous Brian state\n",
    "        defaultclock.dt = 0.0001*ms\n",
    "        # recreate spike timing functions (if needed)\n",
    "        # spike_timing and d_spike_timing_dw are already in namespace\n",
    "\n",
    "        # build one-neuron group\n",
    "        G = NeuronGroup(1,\n",
    "            '''\n",
    "            v               : 1\n",
    "            sum             : 1\n",
    "            sr              : 1\n",
    "            scheduled_time  : second\n",
    "            global_clock    : 1\n",
    "            ''',\n",
    "            threshold='v>1', reset='v=0', method='exact')\n",
    "        G.v = G.sum = G.sr = 0\n",
    "        G.global_clock = 0\n",
    "        G.scheduled_time = 1e9*second\n",
    "\n",
    "        # input spikes\n",
    "        stim = SpikeGeneratorGroup(n_input,\n",
    "            indices=list(range(n_input)),\n",
    "            times=inputs*ms)\n",
    "        S = Synapses(stim, G,\n",
    "            '''w:1\n",
    "            layer:1''', on_pre='''\n",
    "            sr += 1\n",
    "            sum += spike_timing(w, global_clock, layer, sum, sr)\n",
    "            scheduled_time = (1/(1+exp(-(sum/sr))) + layer)*ms\n",
    "        ''')\n",
    "        S.connect(True)\n",
    "        S.w = W[S.i, j]\n",
    "        S.layer = 1\n",
    "\n",
    "        # drive membrane\n",
    "        G.run_regularly('''\n",
    "            v = int(abs(t - scheduled_time) < 0.0005*ms) * 1.2\n",
    "            global_clock += 0.001\n",
    "        ''', dt=0.001*ms)\n",
    "\n",
    "        mon = SpikeMonitor(G)\n",
    "        run(5*ms)\n",
    "        ts = mon.spike_trains()[0]\n",
    "        t0 = float(ts[0]/ms) if len(ts)>0 else 5.0\n",
    "        hidden_times.append(t0)\n",
    "\n",
    "    return np.array(hidden_times)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Training full matrix W\n",
    "\n",
    "def train_snn(\n",
    "    X,           # list of input arrays\n",
    "    Y,           # list of target arrays\n",
    "    W_init,      # initial weight matrix (2x2)\n",
    "    epochs=10,\n",
    "    lr=0.1,\n",
    "    max_grad=20.0,\n",
    "    w_min=-20.0,\n",
    "    w_max=20.0\n",
    "):\n",
    "    W = W_init.copy()\n",
    "    layer_h = 1\n",
    "    n_input, n_hidden = W.shape\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        print(f\"Epoch {ep+1}/{epochs}\")\n",
    "        for i, inp in enumerate(X):\n",
    "            t_pred = mini_urd(inp, W)       # shape (n_hidden,)\n",
    "            t_tgt  = Y[i]\n",
    "            L = 0.5 * np.sum((t_pred - t_tgt)**2)\n",
    "\n",
    "            # gradient matrix dL/dW\n",
    "            dW = np.zeros_like(W)\n",
    "            for j in range(n_hidden):\n",
    "                for k in range(n_input):\n",
    "                    dW[k, j] = (t_pred[j] - t_tgt[j]) * d_spike_timing_dw(\n",
    "                        W[k, j], inp[k], layer_h, 0, 1)\n",
    "\n",
    "            # clip & update\n",
    "            dW = np.clip(dW, -max_grad, max_grad)\n",
    "            W = np.clip(W - lr * dW, w_min, w_max)\n",
    "\n",
    "            print(f\" Sample {i}: inp={inp}, pred={t_pred}, tgt={t_tgt}, L={L:.4f}\")\n",
    "            print(f\"  dW=\\n{dW}\\n  W=\\n{W}\")\n",
    "\n",
    "    return W\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    X = [np.array([0.1, 0.9])]*4\n",
    "    Y = [np.array([1.2, 1.8]) for _ in X]\n",
    "    W0 = np.array([[0.2, -2], [4, -.5]])\n",
    "\n",
    "    W_trained = train_snn(X, Y, W0, epochs=10, lr=0.2)\n",
    "    print(\"Trained weight matrix:\\n\", W_trained)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
